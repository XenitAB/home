[{"title":"Introduction","type":0,"sectionRef":"#","url":"docs/","content":"Introduction Welcome to Xenit's open source projects! We at Xenit are strong believers in open source, we feel that we have gotten so much from open source and we try to give as much back as possible as part of our daily work. This page hosts our general documentation but we also store documentation close to our source code which can give you extra needed context. Here are a few of our projects. terraform-modules stores all our Terraform code and the base for XKF.azure-devops-templates contains both GitHub and Azure DevOps templates for our CI/CD solution.azad-kube-proxy is an auth proxy for kubernetes used in XKF.gitops-promotion is a small Go application that we have built to auto update our GitOps repositories, instead of having to write complex shell scripts.git-auth-proxy is a proxy to allow multi-tenant sharing of GitHub and Azure DevOps credentials in Kubernetes.github-actions is a Go application that automatically prepares Terraform state management and a container image to run in Terraform pipelines. We have many more open source projects so feel free to look around in https://github.com/XenitAB.","keywords":""},{"title":"Architecture and design","type":0,"sectionRef":"#","url":"docs/xks/architecture-and-design","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Architecture and design","url":"docs/xks/architecture-and-design#overview","content":"Xenit Kubernetes Framework (XKF) are the open source building blocks for a service Xenit AB provides to customers: Xenit Kubernetes Service (XKS) In the terminology of Microsoft Cloud Adoption Framework (CAF), Xenit Kubernetes Service is an enterprise-scale landing zone. Additionally, the workload supports multiple cloud providers and AWS is also supported at the moment (but still requires the governance part in Azure).  "},{"title":"Glossary​","type":1,"pageTitle":"Architecture and design","url":"docs/xks/architecture-and-design#glossary","content":"Platform team: the team managing the platform (XKF)Tenant: A group of people (team/project/product) at the company using XKS "},{"title":"Role-based access management​","type":1,"pageTitle":"Architecture and design","url":"docs/xks/architecture-and-design#role-based-access-management","content":"All role-based access control (RBAC) and identity &amp; access management (IAM) is handled with Azure AD. Azure AD groups are created and nested using the framework and framework admins as well as customer end users are granted access through these different groups. Where possible two different permissions are exposed through Azure AD groups: Reader and Contributor. These permissions are scoped in many different ways and start at the management group level, subscription level, resource group level and at last namespaces in Kubernetes. These are also split over the different environments (development, quality assurance and production) meaning you can have read/write in one environment but only read in the others. An owner role and group is also created for most resources, but the recommendation is not to use it as owners will be able to actually change the IAM which in most cases is undesirable. Usually the tenant is granted read/write to their resource groups and namespaces, meaning they will be able to add/remove whatever they want in their limited scope. This usually means creating deployments in Kubernetes as well as databases and other stateful resources in their Azure Resource Groups. When using AWS Elastic Kubernetes Service (EKS) the delegation is not as rigorous as in Azure and the default setup creates three accounts where all the customer tenants share resources. Each tenant namespace has the ability to use the cloud provider metadata service to access services in the cloud provider. This is enabled through tools like Azure AD POD Identity (aad-pod-identity) and IAM Roles for Service Accounts (IRSA). These tools enable the tenants to access resources in their respective resource groups or accounts without having to create manually shared secrets (that would also have to be rotated). "},{"title":"Security and access​","type":1,"pageTitle":"Architecture and design","url":"docs/xks/architecture-and-design#security-and-access","content":""},{"title":"Security​","type":1,"pageTitle":"Architecture and design","url":"docs/xks/architecture-and-design#security","content":"The platform is based on the principle of least privilege and in every layer we try to only delegate what is needed and no more. This is both true for the tenant and platform resources and their respective access. The second part of the work we do regarding security is focused around trying to keep the platform as lean and easy to understand as possible, making sure to remove complexity where needed. Adding security often means selecting a much more complex solution and it is much harder to understand and maintain something complex over something simple. Keeping the cognitive load of maintaining a platform like this as low as possible is always a priority. Additionally, we try to add products and services into the mix to make it easier for both the platform team and tenant teams to keep the environment secure. This is an ever-changing environment where new things are added as needed and the list below is not an exhaustive one: Tenant security​ GitOps for application delivery to clustersCI/CD templates for building containers and promoting applications with GitOpsTools for security scanning and linting inside the pipelinesAdmission controllers to make sure unsafe configuration is prohibitedMutating admission controllers to make it easier to have sane defaultsContinuous education from the platform teamOne account to protect instead of having multiple accounts to access different resourcesInfrastructure as Code with corresponding CI/CD templates for secure deployment of resourcesSSO from the tenant namespaces to the tenant resource groups to make sure no secrets have to be sharedAbility to use mTLS together with a service mesh (linkerd)Automated certificate and DNS management from the platform Platform​ Observability of the platform handled by the platform teamRuntime security in the platform using FalcoAutomated update management of Infrastructure as Code (with code review) "},{"title":"Access​","type":1,"pageTitle":"Architecture and design","url":"docs/xks/architecture-and-design#access","content":"The primary authentication method to access any resource is based on Azure AD. Most of the actions taken by a tenant engineer will require authentication to Azure AD either using the Azure Management Console or the Azure CLI. A tenant will be granted access to the clusters using a proxy built by Xenit and provided in the framework, making sure they do not have to reconfigure their computers when a blue/green deployment of the clusters are made and the Kubernetes API endpoint change. The proxy will move with the clusters and it will be seamless for the tenant engineers. The proxy also provides a CLI (kubectl plugin through krew) that makes it easier to both discover and configure access to the clusters. A valid session with Azure CLI is required to use it. Other than that, most of the access and work with the tenant resources are done through a GitOps repository for changes in regards to applications in Kubernetes and a Terraform repository in regards to resources in the cloud provider. "},{"title":"Network design​","type":1,"pageTitle":"Architecture and design","url":"docs/xks/architecture-and-design#network-design","content":"By default, the network setup is expected to be quite autonomous and usually considered to be an external service compared to everything else in the organization using it. It is possible to setup peering with internal networks, but usually it begins with a much simpler setup and then grows organically when required.  The cluster environments are completely separated from each other, but a hub in the production subscription has a peering with them to provide static IP-addresses for CI/CD like terraform to access resources. Inside an environment the cluster is using kubenet and Calico to keep the amount of IP-addresses to a minimum. Kubernetes Services can either be exposed internally or externally using either a service resource or an ingress resource, where most tenants exclusively use ingress (provided by NGINX Ingress Controller). Inside the clusters Calico is used to restrict traffic between namespaces. "},{"title":"Backup​","type":1,"pageTitle":"Architecture and design","url":"docs/xks/architecture-and-design#backup","content":"Xenit Kubernetes Framework is built to be ephemeral wich means any cluster can at any time be wiped and a new setup without any loss of data. This means that tenants are not allowed to store state inside of the clusters and are required to store it in the cloud provider (blob storage, databases, message queues etc.). Since both the platform team and the tenant teams are deploying resources using GitOps, the current state of the environment is stored in git. The content of stateful resources (including backups) are handled by the cloud provider and it is up to the tenants to configure and manage. "},{"title":"Cost optimization​","type":1,"pageTitle":"Architecture and design","url":"docs/xks/architecture-and-design#cost-optimization","content":"The platform team limits how much the clusters can auto scale and a service delivery manager together with the platform team helps the tenants understand their utilization and provides feedback to keep the cost at bay. "},{"title":"Container management​","type":1,"pageTitle":"Architecture and design","url":"docs/xks/architecture-and-design#container-management","content":"When a new tenant is being setup, the platform team provides onboarding for them and they then continously work together to assist in any questions. Monthly health checks are done to make sure that no obvious mistakes have been made by the tenants and monitoring is setup to warn the platform team if something is wrong with the platform. Most of the management of the workloads that the tenants deploy are handled through GitOps but they are also able to work with the clusters directly, with the knowledge that any cluster may at any time be rolled over (blue/green) and anything not in git will not be persisted. "},{"title":"Xenit Kubernetes Framework​","type":1,"pageTitle":"Architecture and design","url":"docs/xks/architecture-and-design#xenit-kubernetes-framework","content":"XKF is set up from a set of Terraform modules that when combined creates the full XKS service. There are multiple individual states that all fulfill their own purpose and build upon each other in a hierarchical manner. The first setup requires applying the Terraform in the correct order, but after that ordering should not matter. Separate states are used as it allows for a more flexible architecture that could be changed in parallel.  The AKS Terraform contains three modules that are used to setup a Kubernetes cluster. To allow for blue/green deployments of AKS clusters resources have to be split up into global resources that can be shared between the clusters, and cluster-specific resources. The aks-global module contains the global resources like ACR, DNS and Azure AD configuration. The aks and aks-core module creates an AKS cluster and configures it. This cluster will have a suffix, normally a number to allow for temporarily creating multiple clusters when performing a blue/green deployment of the clusters. Namespaces will be created in the cluster for each of the configured tenants. Each namespace is linked to a resource group in Azure where namespace resources are expected to be created.  "},{"title":"Best Practices","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/best-practices","content":"","keywords":""},{"title":"Container Resources​","type":1,"pageTitle":"Best Practices","url":"docs/xks/developer-guide/best-practices#container-resources","content":""},{"title":"Probes​","type":1,"pageTitle":"Best Practices","url":"docs/xks/developer-guide/best-practices#probes","content":""},{"title":"Pod Scaling​","type":1,"pageTitle":"Best Practices","url":"docs/xks/developer-guide/best-practices#pod-scaling","content":""},{"title":"Disruption Budgets​","type":1,"pageTitle":"Best Practices","url":"docs/xks/developer-guide/best-practices#disruption-budgets","content":""},{"title":"Resources​","type":1,"pageTitle":"Best Practices","url":"docs/xks/developer-guide/best-practices#resources","content":"Here are some good resources to also read on top of this page. https://srcco.de/posts/web-service-on-kubernetes-production-checklist-2019.html "},{"title":"Continuous Delivery","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/cd","content":"","keywords":""},{"title":"GitOps​","type":1,"pageTitle":"Continuous Delivery","url":"docs/xks/developer-guide/cd#gitops","content":""},{"title":"Azure AD Identity​","type":1,"pageTitle":"Continuous Delivery","url":"docs/xks/developer-guide/cd#azure-ad-identity","content":""},{"title":"Setup CI/CD pipeline​","type":1,"pageTitle":"Continuous Delivery","url":"docs/xks/developer-guide/cd#setup-cicd-pipeline","content":"At Xenit we have created a CI/CD template to make it easier to get started with GitOps in our case using FluxV2 and the GitOps toolkit. You can find the base for all our Azure DevOps pipelines in our Azure Devops Templates repo. Follow the example documentation on how to setup your base repo. Below we will explain how to do the manual steps that is needed to get Azure DevOps to enable some of the flows that we are creating. "},{"title":"Enable CI user to push to gitops repo​","type":1,"pageTitle":"Continuous Delivery","url":"docs/xks/developer-guide/cd#enable-ci-user-to-push-to-gitops-repo","content":"The core feature of the gitops repo is that one of the pipelines automatically updates the image tag in your repository so Flux will automatically update your deployment in Kubernetes. We have to grant it permissions to do this, sadly manually...  "},{"title":"Service connections​","type":1,"pageTitle":"Continuous Delivery","url":"docs/xks/developer-guide/cd#service-connections","content":"To be able to talk from Azure DevOps to AKS using our gitops pipelines we also need to configure service connections to tenant namespace. Sadly setting up the Service Connections is a manual step. Get the needed config. az keyvault secret show --vault-name &lt;vault-name&gt; --name &lt;secret-name&gt; -o tsv --query value # Example az keyvault secret show --vault-name kv-prod-we-core-1337 --name sp-rg-xks-prod-backend-contributor -o tsv --query value # The output will look something like this {&quot;clientId&quot;:&quot;12345&quot;,&quot;clientSecret&quot;:&quot;SoMuchSecret&quot;,&quot;subscriptionId&quot;:&quot;sub-id&quot;,&quot;tenantId&quot;:&quot;tenant-id&quot;} Copy In Azure DevOps: Project settings -&gt; Service connections -&gt; New service connection -&gt; Azure Resource Manager -&gt; Service principal (manual) Subscription Id = subscriptionIdService Principal Id = clientIdService principal key = clientSecretTenant ID = tenantIdService connection name = random-name "},{"title":"Continuous Integration","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/ci","content":"","keywords":""},{"title":"Hadolint​","type":1,"pageTitle":"Continuous Integration","url":"docs/xks/developer-guide/ci#hadolint","content":"Hadolint is a linter for container files and gives you suggestions on how to follow best practices. This check is enabled by default and you can disable it by adding: dockerLint: enabled: false ignoreRuleViolations: false Copy You can add a config file for hadolint where you canignore specific errors. "},{"title":"Trivy​","type":1,"pageTitle":"Continuous Integration","url":"docs/xks/developer-guide/ci#trivy","content":"Trivy is a container image scanner in a cli tool used to scan for CVE:s in both your code and base image. To scan the image: trivy &lt;image-name&gt; Copy After we have built the image we scan it and send you a report. If you want to ignore specific Trivy errors you can create a .trivyignore file. For example it can look like this: CVE-2020-29652 Copy If you want to disable Trivy you can do so by appending the following to your CI file. imageScan enable: true ignoreRuleViolations: true Copy "},{"title":"Horusec​","type":1,"pageTitle":"Continuous Integration","url":"docs/xks/developer-guide/ci#horusec","content":"A CLI tool to scan your application code for security issues that is disabled by default. "},{"title":"To enable Horusec in your CI pipeline​","type":1,"pageTitle":"Continuous Integration","url":"docs/xks/developer-guide/ci#to-enable-horusec-in-your-ci-pipeline","content":"Before enabling Horusec in your CI pipeline you should configure what issues your CI pipeline should catch: horusec generate Copy This will generate a file called horusec-config.json, in this file you can configure everything Horusec should scan and report. You can find more Horusec config flags here. Key config values: horusecCliSeveritiesToIgnore: What severities do you want to show?horusecCliFilesOrPathsToIgnore: What paths do you want to ignore? To save time it is easy to run Horusec locally: horusec start -p . Copy "},{"title":"Flux","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/flux","content":"","keywords":""},{"title":"Flux CLI​","type":1,"pageTitle":"Flux","url":"docs/xks/developer-guide/flux#flux-cli","content":"You do not have to use the Flux CLI but it can be very helpful especially if want to force a reconciliation of a Flux resource. In some of the commands and debugging we assume that you have the CLI installed. Here you can find more information on how to setup the Flux CLI. "},{"title":"XKF and Flux​","type":1,"pageTitle":"Flux","url":"docs/xks/developer-guide/flux#xkf-and-flux","content":"In the XKF framework we talk a lot about tenants, from a Kubernetes point of view a tenant is a namespace that has been generated using Terraform. If you want more information on how that works you can look at the AZDO moduleand the GitHub module. The module populates the tenant namespaces by creating a basic config with a Flux GitRepository and Kustomization pointing to a pre-defined repository and path. It stores this in a central repository that normally your platform team manages and it should only be updated using Terraform. At the time of writing these docs the files generated could look something like this if you are using Azure DevOps (AZDO) and AZDO-proxy. As a member of tenant1 you will be able to see these resources in your namespace, in this case tenant1. You should never modify these resources manually, Flux will overwrite any manual changes back to the config defined in the git repository. --- apiVersion: source.toolkit.fluxcd.io/v1beta1 kind: GitRepository metadata: name: tenant1 namespace: tenant1 spec: # If you are using github libgit2 will not be defined gitImplementation: libgit2 interval: 1m # This example url assumes that you are using AZDO-proxy https://github.com/XenitAB/azdo-proxy url: http://azdo-proxy.flux-system.svc.cluster.local/Org1/project1/_git/gitops secretRef: name: flux ref: branch: main --- apiVersion: kustomize.toolkit.fluxcd.io/v1beta1 kind: Kustomization metadata: name: tenant1 namespace: tenant1 spec: serviceAccountName: flux interval: 5m path: ./tenant/dev sourceRef: kind: GitRepository name: tenant1 prune: true validation: client Copy "},{"title":"Debugging​","type":1,"pageTitle":"Flux","url":"docs/xks/developer-guide/flux#debugging","content":"Below, you will find a few good base commands to debug why Flux has not applied your changes. "},{"title":"Normal error​","type":1,"pageTitle":"Flux","url":"docs/xks/developer-guide/flux#normal-error","content":"When adding a new file to your GitOps repository do not forget to update the kustomization.yaml file. It can easily happen that you create a file in your repository and you commit it and when you look in the cluster it has not been synced. This is most likely due to that you have missed to update the kustomization.yaml file. apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - deployment.yaml - ingress.yaml - networkpolicy.yaml Copy "},{"title":"git repositories​","type":1,"pageTitle":"Flux","url":"docs/xks/developer-guide/flux#git-repositories","content":"Shows you the status if your changes have been synced to the cluster. $ kubectl get gitrepositories NAME URL READY STATUS AGE wt http://azdo-proxy.flux-system.svc.cluster.local/Org1/project1/_git/gitops True Fetched revision: main/9baa401630894b78ecc5fa5ebdf72c978583dea8 2d2h Copy Flux should automatically pull the changes to the cluster but if you think they sync takes to long time or you want to sync it for some other reason you can. Remember to provide the --namespace flag, else Flux will assume the source is in the flux-system namespace. flux reconcile source git tenant1 --namespace tenant1 Copy "},{"title":"Kustomization​","type":1,"pageTitle":"Flux","url":"docs/xks/developer-guide/flux#kustomization","content":"It is always good to check if Flux has applied your changes and if your health checks have passed. Overall the checksum of your source and the kustomization resource should be the same. $ kubectl get kustomizations NAME READY STATUS AGE apps-dev True Applied revision: main/9baa401630894b78ecc5fa5ebdf72c978583dea8 47h tenant1 True Applied revision: main/9baa401630894b78ecc5fa5ebdf72c978583dea8 2d2h Copy "},{"title":"GitOps a la XKS","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/gitops","content":"","keywords":""},{"title":"What is GitOps?​","type":1,"pageTitle":"GitOps a la XKS","url":"docs/xks/developer-guide/gitops#what-is-gitops","content":"GitOps works by using Git as a single source of truth for declarative infrastructure and applications. With GitOps, the use of software agents can alert on any divergence between Git and what is running in [an environment]. If there is a difference, Kubernetes reconcilers automatically update or rollback the cluster depending on what is appropriate. ‐ Weave Works - Guide To GitOps XKS supports GitHub and Azure DevOps with almost identical workflows. XKF refers to these as Git providers. For simplicity, we refer to their CI/CD automation as &quot;pipelines&quot;. If you are using GitHub, whenever this text refers to &quot;pipeline&quot;, think &quot;GitHub Actions workflow&quot;. As you saw in the previous section, XKS comes with a set of pipelines that automatically detects app releases and promotes them through a series of environments. The allows both rapid iteration and strong validation of apps. XKS is built around trunk-based development. "},{"title":"User story: Emilia updates an app​","type":1,"pageTitle":"GitOps a la XKS","url":"docs/xks/developer-guide/gitops#user-story-emilia-updates-an-app","content":"In the previous section, we looked at deploying our first app to Kubernetes using a fully automatic flow. But what actually happened in that flow? This section tells the story about a developer called Emilia. She has updated an app packaged as a container image. A pipeline in the app's repository has just tagged the container image as a8b91c33 and uploaded it to a container registry. The GitOps repository for Emilia's app has a gitops-promotion.yaml that looks like this (for much more details, see the gitops-promotion readme): prflow: per-app environments: - env: dev auto: true - env: qa auto: true - env: prod auto: false Copy The dev and qa environments have auto: true which means that new releases will be automatically applied, while the prod environment is configured with auto: false. This means that pull requests must be merged by a human, presumably one that has verified that the update worked as expected in previous environments. The GitOps repository is configured to require checks on pull requests to pass in order to allow merge. "},{"title":"Applying to dev​","type":1,"pageTitle":"GitOps a la XKS","url":"docs/xks/developer-guide/gitops#applying-to-dev","content":"The flow is fully automatic and is triggered by the container image upload.  The / container image upload triggers a pipeline in the GitOps repository that runs the / gitops-promotion new command. It pushes a new branch and updates the dev environment manifest for the app with the new tag. It then opens an &quot;auto-merging&quot; pull request to integrate the new tag into the main branch.The pull request triggers another pipeline that runs / gitops-promotion status command. Since dev is the first environment in the list, it does nothing and reports success.The pull request check turns green and the pull request is automatically merged by the Git provider.The Flux Kustomization controller detects that there has been an update to the app's tag in the Git repository and applies this update to the Kubernetes resource for the app (typically a Deployment).The pods running the new container image came up healthy and so Flux sets a commit status on the main branch in the GitOps repository, reporting that the update was successfully applied. This will be significant in the next section. "},{"title":"Applying to qa​","type":1,"pageTitle":"GitOps a la XKS","url":"docs/xks/developer-guide/gitops#applying-to-qa","content":" Merging a promotion to the main branch triggers a pipeline in the GitOps repository that runs the gitops-promotion promote command. Like new, it creates a branch and updates the qa environment manifest for the app with the new tag. Because the configuration for this environment says auto: true it creates an auto-merging pull request.As before, this new pull request triggers another pipeline that runs the status command. This time there is a previous environment and the status command reads the Flux commit status for that environment. Since Flux managed to apply the change in dev the status command reports success.The pull request check turns green and the pull request is automatically merged by the Git provider.The Flux Kustomization controller detects that there has been an update to the app's tag in the Git repository and applies this update to the Kubernetes resource for the app (typically a Deployment).In this case, someone had applied manual changes to the app's database in the dev environment during development. These updates are not present in the qa environment, and when the pods running the new container image come up, they cannot read state from the database and so fail their health check. Flux consequently sets a commit status on the main branch of the GitHub repository, reporting that the update failed. Emilia's team has configured Flux to notify them when updates fail and so Emilia's chat client informs her that the update did not go through. "},{"title":"Application to prod is blocked​","type":1,"pageTitle":"GitOps a la XKS","url":"docs/xks/developer-guide/gitops#application-to-prod-is-blocked","content":" The workflow for applying to prod is similar to that of qa above, but since Flux reported failure when applying the update to qa, the pipeline running the status command will fail and the Git provider will block merging of the pull request. Seeing that the rollout failed, Emilia investigates and realizes that the release is missing a database migration script. She pushes an updated release tagged cc2b7e0a and so triggers the pipline running the new command. Because the configuration says prflow: per-app, the command &quot;resets&quot; the blocked pull request to apply to the updated release to the dev environment. "},{"title":"Second attempt applying to prod​","type":1,"pageTitle":"GitOps a la XKS","url":"docs/xks/developer-guide/gitops#second-attempt-applying-to-prod","content":" Emilia's updated app with database migration is successfully applied, first to the dev environment and then to the qa environment. The status check for the pull request against prod turns green and the pull request can be merged. Since the configuration says auto: false, the pull request is not automatically merged. Emilia can now verify the update in the qa environment and then merge the pull request through the Git provider's user interface. "},{"title":"Cloud IAM","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/cloud-iam","content":"","keywords":""},{"title":"Cloud Providers​","type":1,"pageTitle":"Cloud IAM","url":"docs/xks/developer-guide/cloud-iam#cloud-providers","content":""},{"title":"Azure​","type":1,"pageTitle":"Cloud IAM","url":"docs/xks/developer-guide/cloud-iam#azure","content":"The recommended way to authenticate towards Azure in XKS is to make use of AAD Pod Identity which runs inside the cluster. AAD Pod Identity allows Pods within the cluster to use managed identities to authenticate towards Azure. This removes the need for static credentials that have to be passed to the Pods. It works by intercepting API requests before they leave the cluster and will attach the correct credential based on the source Pod of the request. Each tenant namespace comes preconfigured with an AzureIdentity andAzureIdentityBinding. These have been setup so that the identity has access to the tenant's resource group. All that has to be done to enable the managed identity is to add the label foo to the Pod. The preconfigured AzureIdentity has a labelselector which expects the label to have the same value as the namespace name. This example will deploy a Pod with the Azure CLI so that you can test access to the Azure API. apiVersion: v1 kind: Pod metadata: name: msi-test namespace: tenant labels: aadpodidbinding: ${NAMESPACE_NAME} spec: containers: - name: msi-test image: mcr.microsoft.com/azure-cli tty: true volumeMounts: - name: az-cli mountPath: /root/.azure volumes: - name: az-cli emptyDir: {} Copy After the Pod has started you can execute a shell in the Pod and verify that the managed identity is working. kubectl -n tenant exec -it msi-test az login --identity az account show Copy Make sure your application supports retries when retrieving tokens. It should at least be able to retry for 60 seconds. Read more about it here. More good practices can be found in the aad-pod-identity docs. SDK​ A common scenario is that an application may need API access to Azure resources through the API. In these cases the best solution is to use the language specific SDKs which will most of the time support MSI credentials. Below are examples for how to create a client using MSI credentials which can interact with Azure storage account blobs.  Golang  package main import ( &quot;time&quot; &quot;github.com/Azure/go-autorest/autorest/azure/auth&quot; blob &quot;github.com/Azure/azure-storage-blob-go/azblob&quot; ) func main() { msiConfig := auth.NewMSIConfig() spt, err := msiConfig.ServicePrincipalToken() if err != nil { return nil, err } if err := spt.Refresh(); err != nil { return nil, err } tc, err := blob.NewTokenCredential(spt.Token().AccessToken, func(tc blob.TokenCredential) time.Duration { err := spt.Refresh() if err != nil { return 30 * time.Second } tc.SetToken(spt.Token().AccessToken) return spt.Token().Expires().Sub(time.Now().Add(2 * time.Minute)) }), nil } Copy  C# with ASP.NET  using Azure; using Azure.Identity; using Azure.Storage.Blobs; async static Task CreateBlockBlobAsync(string accountName, string containerName, string blobName) { string containerEndpoint = string.Format(&quot;https://{0}.blob.core.windows.net/{1}&quot;, accountName, containerName); BlobContainerClient containerClient = new BlobContainerClient(new Uri(containerEndpoint), new DefaultAzureCredential()); } Copy Limiting Permissions​ TBD "},{"title":"AWS​","type":1,"pageTitle":"Cloud IAM","url":"docs/xks/developer-guide/cloud-iam#aws","content":"When authenticating towards AWS in XKS we recommend using IAM Roles for Service Accounts (IRSA). IRSA works by intercepting AWS API calls before leaving the cluster and appending the correct authentication token to the request. This removes the need for static security credentials as it is handled outside the app. IRSA works by annotating a Service Account with a reference to a specfic AWS IAM role. When that Service Account is attached to a Pod, the Pod will be able to assume the IAM role. The reason IRSA works in a multi-tenant cluster is because the reference is multi-directional. The Service Account has to specify the full role ARN it wants to assume and the IAM role has to specify the name and namespace of the Service Account which is allowed to assume the role. So it is not enough to know the ARN of the role unless you have access to the correct namespace and Service Account. Start by defining a variable for the OIDC URLs that should be trusted. Currently this is a static definition that needs to be specified but work is planned to make this value discoverable in the future. variable &quot;oidc_urls&quot; { description = &quot;List of EKS OIDC URLs to trust.&quot; type = list(string) } Copy A new OIDC provider has to be created for each trusted URL. The simplest way to do this is to iterate the URL list. This should only be done once per tenant account, so try to define all roles in the same Terraform state. data &quot;tls_certificate&quot; &quot;this&quot; { for_each = { for v in var.oidc_urls : v =&gt; v } url = each.value } resource &quot;aws_iam_openid_connect_provider&quot; &quot;this&quot; { for_each = { for v in var.oidc_urls : v =&gt; v } client_id_list = [&quot;sts.amazonaws.com&quot;] thumbprint_list = [data.tls_certificate.this[each.value].certificates[0].sha1_fingerprint] url = each.value } Copy Define an AWS IAM policy document and an instance of the IRSA Terraform module. The policy document describes which permissions should be granted to a Pod and the IRSA module creates the IAM policy and role for a Service Account in a specific namespace. The example below will for example only work with a Service Account called irsa-test in the namespace tenant. Keep in mind that a policy document and module instance is required for each unique permission set. data &quot;aws_iam_policy_document&quot; &quot;get_login_profile&quot; { statement { effect = &quot;Allow&quot; actions = [ &quot;iam:GetLoginProfile&quot;, ] resources = [&quot;*&quot;] } } module &quot;irsa_test&quot; { source = &quot;github.com/xenitab/terraform-modules//modules/aws/irsa?ref=2021.08.9&quot; name = &quot;irsa-test&quot; oidc_providers = [ for v in var.oidc_urls : { url = v arn = aws_iam_openid_connect_provider.this[v].arn } ] kubernetes_namespace = &quot;tenant&quot; kubernetes_service_account = &quot;irsa-test&quot; policy_json = data.aws_iam_policy_document.get_login_profile.json } Copy It is a good idea to output the ARN of the created role, as it will be needed in the next step. output &quot;irsa_test_arn&quot; { value = module.irsa_test.role_arn } Copy The correct IAM roles and policies should be created after the Terraform has been applied. The next step is to create a Service Account with the same name as specified in the IRSA module and annotate it with the key eks.amazonaws.com/role-arn. The value should be the full ARN of the created IAM role. Note that the account id is part of the ARN as the IAM role is created in a different account than the one the cluster is located in. apiVersion: v1 kind: ServiceAccount metadata: name: irsa-test namespace: tenant annotations: eks.amazonaws.com/role-arn: arn:aws:iam::111111111111:role/irsa-test Copy Create a Pod using the newly created Service Account to test using the IAM role. apiVersion: v1 kind: Pod metadata: name: irsa-test namespace: tenant spec: serviceAccountName: irsa-test containers: - name: irsa-test image: amazon/aws-cli command: [&quot;sh&quot;] stdin: true tty: true Copy After the Pod has started you can execute a shell in the Pod and verify that the managed identity is working. kubectl -n tenant exec -it irsa-test aws sts get-caller-identity Copy SDK​ TBD "},{"title":"Introduction","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/introduction","content":"","keywords":""},{"title":"Getting Started​","type":1,"pageTitle":"Introduction","url":"docs/xks/developer-guide/introduction#getting-started","content":"A good starting point is to get CLI access to the clusters. It is useful to have manual access to the clusters for debugging purposes. Try to avoid doing any changes to cluster resources with the CLI as the changes will be difficult to track and update in the future. There are a couple of prerequisites that have to be met before getting access however. Start off by installing the Azure CLI. az login Copy "},{"title":"Kubectl Configuration​","type":1,"pageTitle":"Introduction","url":"docs/xks/developer-guide/introduction#kubectl-configuration","content":"You can run the following commands to add the AKS cluster to your kubeconfig, assuming that you have installed the Azure CLIand authenticated with the Azure portal. "},{"title":"Using AZAD Proxy​","type":1,"pageTitle":"Introduction","url":"docs/xks/developer-guide/introduction#using-azad-proxy","content":"If you use the AZAD proxy you can find documentation to help you set it up here: AZAD Documentation "},{"title":"Otherwise​","type":1,"pageTitle":"Introduction","url":"docs/xks/developer-guide/introduction#otherwise","content":"Once you have logged in you can list your subscriptions az account list -o table Copy In the case where you have more than one subscription, you might want to change the default subscription in order to target the correct environment. az account set -s &lt;SubscriptionId&gt; Copy To get information about cluster name and resource group for your current default subscription you can use: az aks list -o table Copy Once you know the resource group and name of the cluster, you can run the following to add the credentials to your kubekonfig: az aks get-credentials --resource-group &lt;ResourceGroup&gt; --name &lt;Name&gt; Copy "},{"title":"My First Application​","type":1,"pageTitle":"Introduction","url":"docs/xks/developer-guide/introduction#my-first-application","content":"TBD "},{"title":"Next Steps​","type":1,"pageTitle":"Introduction","url":"docs/xks/developer-guide/introduction#next-steps","content":"TBD "},{"title":"Troubleshoot applications in Kubernetes​","type":1,"pageTitle":"Introduction","url":"docs/xks/developer-guide/introduction#troubleshoot-applications-in-kubernetes","content":"There is a great guide how to debug Kubernetes deployment over at learnk8s.io. To debug flux issues have a look at our Flux docs. "},{"title":"Observability","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/observability","content":"","keywords":""},{"title":"What is observability?​","type":1,"pageTitle":"Observability","url":"docs/xks/developer-guide/observability#what-is-observability","content":"Within observability we normally talk about three pillars. metricsloggingtracing Monitoring applications is an especially important feature when developing microservices and something that all developers of microservices needs to focus on. We currently support two solutions to gather observability data in XKF, Datadog and the Opentelemetry stack which is an open-source solution. "},{"title":"Datadog​","type":1,"pageTitle":"Observability","url":"docs/xks/developer-guide/observability#datadog","content":"When Datadog monitoring is used the Datadog Operator will be added to your cluster. On top of deploying a Datadog agent to every node to collect metrics, logs, and traces it also adds the ability to create Datadog monitors from the cluster. The Datadog agent handles all communication with the Datadog API meaning that individual applications do not have to deal with things such as authentication. "},{"title":"Logging​","type":1,"pageTitle":"Observability","url":"docs/xks/developer-guide/observability#logging","content":"All logs written to stdout and stderr from applications in the tenant namespace will be collected by the Datadog agents. This means that no additional configuration has to be done to the application, other than making sure the logs are written to the correct destination. This means that kubectl logs and Datadog will display the same information. Check the official Datadog Logging Documentation for more detailed information. "},{"title":"Metrics​","type":1,"pageTitle":"Observability","url":"docs/xks/developer-guide/observability#metrics","content":"Datadog can collect Prometheus or OpenMetrics metrics exposed by your application. In simple terms this means that the application needs to expose an endpoint which the Datadog agent can scrape to get the metrics. All that is required is that the Pod contains annotations which tells Datadog where to find the metrics HTTP endpoint. Given that your application is exposing metrics on port 8080 your pod should contain the following annotations. annotations: ad.datadoghq.com/prometheus-example.instances: | [ { &quot;prometheus_url&quot;: &quot;http://%%host%%:8080/metrics&quot; } ] Copy Check the official Datadog Metrics Documentation for more detailed information. "},{"title":"Tracing​","type":1,"pageTitle":"Observability","url":"docs/xks/developer-guide/observability#tracing","content":"Datadog tracing is done with Application Performance Monitoring (APM), which sends traces from an application to Datadog. For traces to work the application needs to be configured with the language specific libraries. Check the Language Documentationfor language specific instructions. Some of the languages that are supported are. GolangC#JavaPython Configure your Deployment with the DD_AGENT_HOST environment for the APM agent to know where to send the traces. apiVersion: apps/v1 kind: Deployment spec: containers: - env: - name: DD_AGENT_HOST valueFrom: fieldRef: fieldPath: status.hostIP Copy Check the official Datadog Tracing Documentation for more detailed information. "},{"title":"Networkpolicy datadog​","type":1,"pageTitle":"Observability","url":"docs/xks/developer-guide/observability#networkpolicy-datadog","content":"When using XKF and your cluster has Datadog enabled the tenant namespace will automatically get a networkpolicy that allows egress for tracing. You can view these rules by typing: kubectl get networkpolicies -n &lt;tenant-namespace&gt; Copy "},{"title":"Opentelemetry​","type":1,"pageTitle":"Observability","url":"docs/xks/developer-guide/observability#opentelemetry","content":"To gather opentelemetry data we rely on the grafana agent operator. The grafana agent operator deploys a grafana-agent in a central namespace configured as a part of XKF. The grafana agent gathers both metrics and logs and is able to receive traces. "},{"title":"Metrics​","type":1,"pageTitle":"Observability","url":"docs/xks/developer-guide/observability#metrics-1","content":"To gather metrics data we use servicemonitors or podmonitors that is managed in XKF using the prometheus-operator. The prometheus-operator has a great getting started guidebut if you want a quick example you can look below. In order for the grafana agent to find the pod you have to put this exact label on the pod/service monitor yaml: xkf.xenit.io/monitoring: tenant, or else the grafana agent will not find the rule to gather the metric. The selectors is used to find ether the pod or the service that you want to monitor. Use a podmonitor when you do not have a service in front of your pod. For example this might be the case when your application does not use an HTTP endpoint to get requests. apiVersion: monitoring.coreos.com/v1 kind: PodMonitor metadata: name: podmonitor-example labels: xkf.xenit.io/monitoring: tenant spec: selector: matchLabels: app.kubernetes.io/name: app1 podMetricsEndpoints: - port: http-metrics Copy In general use a servicemonitor when you have a service in front of your pod. apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: labels: xkf.xenit.io/monitoring: tenant name: servicemonitor-example spec: endpoints: - interval: 60s port: metrics selector: matchLabels: app.kubernetes.io/instance: app1 Copy You can do a lot of configuration when it comes to metrics gathering but the above config will get you started. "},{"title":"Logging​","type":1,"pageTitle":"Observability","url":"docs/xks/developer-guide/observability#logging-1","content":"To gather logs from your application you need to define a PodLogs object. Just like metrics you have to define a label like xkf.xenit.io/monitoring: tenant in your PodLogs. The PodLogs CRD is created by the grafana agent operator and functions very similarly to how the prometheus operator works, especially when it comes to selectors. Below you will find a very basic example that will scrape a single pod in the namespace where it is created. apiVersion: monitoring.grafana.com/v1alpha1 kind: PodLogs metadata: name: app1 labels: xkf.xenit.io/monitoring: tenant spec: selector: matchLabels: app.kubernetes.io/name: app1 pipelineStages: - cri: {} Copy You can do a lot of configuration when it comes to log filtering using PodLogs. For example you can drop specific log types that you do not want to send to your long time storage. Sadly the grafana agent operator does not supply great documentation around how to define this configuration in the operator. However, together with running kubectl explain podlogs.monitoring.grafana.com.spec.pipelineStages on the cluster and reading the official documentation on how to create pipelines you can get a good understanding of how to create the configuration that you need. If you do not have any needs to filter or do any custom config per application you can create a namespace-wide PodLogs gatherer. apiVersion: monitoring.grafana.com/v1alpha1 kind: PodLogs metadata: name: tenant-namespace-log labels: xkf.xenit.io/monitoring: tenant spec: selector: {} pipelineStages: - cri: {} Copy "},{"title":"Tracing​","type":1,"pageTitle":"Observability","url":"docs/xks/developer-guide/observability#tracing-1","content":"The tracing setup is a bit different compared to logging and metrics, instead of having some yaml file where you define how to gather metrics and logs from your application, you instead push data to a central collector. Opentelemetry supports both HTTP and gRPC communication to gather traces from your application. To send HTTP data we use 4318 and for gRPC we use 4317. Point your OpenTelemetry SDK to http://grafana-agent-traces.opentelemetry.svc.cluster.local:4318/v1/tracesor http://grafana-agent-traces.opentelemetry.svc.cluster.local:4317/v1/traces. Tail-based sampling​ By default the grafana agent that is deployed by XKF forwards all traces without any special config to your service provider. This can cause high costs thanks to the amount of data that is sent. You can configure the agent to use probabilistic sampling which grafana agent delivers their own solution for called tail-based sampling, which can help you solve this issue. To setup a custom agent with tail-based sampling you can setup your own trace agent with the custom config that you want and then have it forward all the traffic to our central trace agent in the opentelemetry namespace. Below you can find a simple example configmap that you can use together with your trace agent to send data to the central agent. kind: ConfigMap apiVersion: v1 metadata: name: grafana-agent-traces data: agent.yaml: | tempo: configs: - name: default remote_write: - endpoint: &quot;grafana-agent-traces.grafana-agent.svc.cluster.local:4317&quot; insecure: true receivers: otlp: protocols: http: {} grpc: {} tail_sampling: # policies define the rules by which traces will be sampled. Multiple policies # can be added to the same pipeline. # For more information: https://grafana.com/docs/agent/latest/configuration/traces-config/ # https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/b2327211df976e0a57ef0425493448988772a16b/processor/tailsamplingprocessor policies: - probabilistic: {sampling_percentage: 10} - status_code: {status_codes: [ERROR, UNSET]} Copy "},{"title":"Networkpolicy grafana agent​","type":1,"pageTitle":"Observability","url":"docs/xks/developer-guide/observability#networkpolicy-grafana-agent","content":"When using XKF and your cluster have enabled the grafana agent your tenant namespace will automatically get a networkpolicy that allows incoming metrics gathering and egress for tracing. You can view these rules by typing: kubectl get networkpolicies -n &lt;tenant-namespace&gt; Copy "},{"title":"Reports","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/reports","content":"","keywords":""},{"title":"Security​","type":1,"pageTitle":"Reports","url":"docs/xks/developer-guide/reports#security","content":"Xenit is not responsible for the security of your application but as a part of the report we state the number of CVE:s in your applications found by our automated scanning tools that we have in XKF. This is a extra way for us at Xenit to help you as a developer, but you cannot rely on Xenit to point out specific security flaws in your application. Security is something that you and your company is in charge of, we are just trying to visualize some of the data that we have access to. As a part of XKF we continuously scan all images running in our clusters. To do this we depend onStarboard together with Trivy. If you are using the Xenit CI solution you are by default scanning your container images with Trivy at creation time. However, new CVE:s may have been released since you created your image and that is why continuous scanning of images is important. For more information how to get deeper information of the CVE reports look at our Starboard documentation. "},{"title":"Networking","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/networking","content":"","keywords":""},{"title":"Network Policies​","type":1,"pageTitle":"Networking","url":"docs/xks/developer-guide/networking#network-policies","content":"Network Policies in Kubernetes add the ability to allow and deny network traffic from specific pods and namespaces. Both egress traffic from a Pod and ingress traffic to a Pod can be controlled. In a vanilla Kubernetes cluster all traffic between all namespaces is allowed by default. This is not the case in XKS. Out of the box in XKS all tenant namespaces have a default deny rule added to them. This default deny rule will block any traffic going between namespaces. It will deny both ingress traffic from other namespaces and egress traffic to other namespaces. All traffic within the namespace between Pods is allowed. The reasoning behind this setup is that Pods should not have more network access than they require to function, as it reduces the blast radius in case of an exploit.  The default deny Network Policy contains an exception for traffic destined to the cluster's DNS service. Without this exception DNS resolution would not work. The Pod selector in the Network Policy is empty, this means that the Network Policy will apply for all Pods in the namespace. apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny namespace: tenant spec: egress: - ports: - port: 53 protocol: UDP to: - namespaceSelector: {} podSelector: matchLabels: k8s-app: kube-dns - to: - podSelector: {} ingress: - from: - podSelector: {} podSelector: {} policyTypes: - Ingress - Egress Copy There may come a time when you have to create new Network Policies to allow specific Pods traffic, as the default can be pretty restrictive. A common situation when this is required is when a Pod needs to communicate with the public Internet, or communicate with other Pods in other tenant namespaces. When creating new Network Policies make sure that you do not open up more than is actually required. A good source of example Network Policies is the Github repository kubernetes-network-policy-recipes. It contains a lot of good examples with diagrams and descriptions. The examples found there contain the most common use cases to make things simpler for you. A helpful tool when create new Network Policies is the Cilium Network Policy Editor. "},{"title":"Examples​","type":1,"pageTitle":"Networking","url":"docs/xks/developer-guide/networking#examples","content":"Allow Internet Egress​ A common scenario is opening up traffic to the public Internet. A current limitation with Network Policies today is that it is not possible to create egress rules based on DNS names. This means that the simplest solution is to allow traffic to all public IPs, as trying to resolve the DNS to an IP would only work short term. apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-internet-egress spec: egress: - to: - ipBlock: cidr: 0.0.0.0/0 podSelector: matchLabels: app: foo policyTypes: - Egress Copy Allow Ingress Nginx​ Traffic from the ingress controller has to be explicitly allowed as no traffic is allowed from outside the namespace by default. This can be considered a fail-safe to protect against accidental Ingress creation, where an application is exposed to the Internet when that was not the intent. It is enough to allow ingress from the ingress controller even if the traffic actually originates from outside the cluster. apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-ingress-controller spec: ingress: - from: - namespaceSelector: matchLabels: name: ingress-nginx podSelector: matchLabels: app: foo policyTypes: - Ingress Copy Allow Cross Namespace​ When allowing network traffic across tenant namespaces considerations have to be made for the default deny Network Policy in both namespaces. An allow rule has to be created to allow the source namespace (the side initating the connection) to send traffic to the destination namespace. The destination namespace has to allow traffic from the source namespace. The first Network Policy should be used in the source namespace and the second should be used in the destination namespace. apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-egress-to-destination namespace: source spec: egress: - to: - namespaceSelector: matchLabels: name: destination podSelector: matchLabels: app: bar podSelector: matchLabels: app: foo policyTypes: - Egress Copy apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-ingress-from-source namespace: destination spec: ingress: - from: - namespaceSelector: matchLabels: name: source podSelector: matchLabels: app: foo podSelector: matchLabels: app: bar policyTypes: - Ingress Copy "},{"title":"Debugging​","type":1,"pageTitle":"Networking","url":"docs/xks/developer-guide/networking#debugging","content":"TBD "},{"title":"Ingress​","type":1,"pageTitle":"Networking","url":"docs/xks/developer-guide/networking#ingress","content":"Ingress in Kubernetes is used to allow network traffic from the outside the cluster to reach Pods inside the cluster. Ingress works as a layer on top of Kubernetes Services by exposing the Service with a hostname. All Ingress traffic is Layer 7 routed, meaning that traffic is routed based on the host header in the HTTP request. This also means that Ingress only works with HTTP traffic. Doing it this way means that only a single load balancer is required reducing cost compared to running multiple load balancers, one per Ingress.  XKS comes with everything pre-configured for Ingress to work. The cluster will either have a single Nginx Ingress Controller which is exposed to the public Internet or two controllers where one is public and one is private. On top of that the cluster is configured with External DNS(which creates DNS records) and Cert Manager (which deals with certificate creation and renewal). Together these three tools offer an automated solution where the complexity of DNS and certificates are not handled by the application. The recommendation is to always enable TLS for all Ingress resources no matter how small the service is. Updating a certificate is quick and easy so there is no reason not to do this. Every XKS cluster comes with a preconfigured Cluster Issuer which will provision certificates from Let's Encrypt. Start off by creating a Certificate resource for your Ingress. It is possible to have Cert Manager automatically create a Certificate when an Ingress resource is created. This however has the downside that every Ingress resource will receive its own Certificate. Lets Encrypt has rate limits for the same domain, if one were to create Certificate per ingress that rate limit would be hit pretty quickly. For this reason it is better to create a shared Certificate per tenant namespace with multiple DNS names instead. Each DNS name will be present in the Certificate so that it can be used for multiple Ingress resources. When the Certificate is provisioned it will be written to a Secret. apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: shared namespace: tenant spec: issuerRef: group: cert-manager.io kind: ClusterIssuer name: letsencrypt dnsNames: - app-one.example.com - app-two.example.com secretName: shared-cert Copy To complete the ingress configuration an Ingress resource has to be created. The Ingress resource defines the Service where the traffic should be routed to and the DNS name which should resolve to that Service. An additional configuration is the TLS configuration which configures the certificate to use. Cert Manager writes the certificate data to a Secret which is configured in the CertificatesecretName. That same Secret should be referenced in the TLS configuration. A DNS record will be automatically created when the Ingress is applied to the cluster. apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: app-one namespace: tenant spec: rules: - host: app-one.example.com http: paths: - path: / backend: service: name: app-one port: name: http tls: - hosts: - app-one.example.com secretName: shared-cert Copy "},{"title":"Private Ingress​","type":1,"pageTitle":"Networking","url":"docs/xks/developer-guide/networking#private-ingress","content":"TBD "},{"title":"Nginx Configuration​","type":1,"pageTitle":"Networking","url":"docs/xks/developer-guide/networking#nginx-configuration","content":"It is useful to be aware of annotation configuration in the Nginx ingress controller. Sometimes a specific Ingress requires custom behavior that is not default in the ingress controller, this behavior can be customized with the help of annotations for a specific Ingress resource. For example, changing the client body buffer size may be useful if the header size in the expected requests is larger than the buffer. apiVersion: extensions/v1beta1 kind: Ingress metadata: name: foo namespace: bar annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/client-body-buffer-size: 1M spec: rules: - host: foo.dev.example.com http: paths: - backend: serviceName: foo servicePort: http tls: - hosts: - foo.dev.example.com Copy "},{"title":"Debugging​","type":1,"pageTitle":"Networking","url":"docs/xks/developer-guide/networking#debugging-1","content":"Common networking problems include forgetting to set up egress or ingress rules that apply for your pods - or setting them up and then having the requirements change, which then causes connection errors. Remember that you can inspect your network policies with kubectl get networkpolicies. If you cannot see your policy there, verify if it is actually present in your kustomization.yaml file. "},{"title":"Linkerd​","type":1,"pageTitle":"Networking","url":"docs/xks/developer-guide/networking#linkerd","content":"Linkerd is an optional service mesh that can be added to XKS. The component is opt-in as it adds a certain amount of overhead, so unless it has been requested Linkerd will not be present in XKS. A service mesh extends the networking functionality in a Kubernetes cluster. It is useful when features such as end-to-end encryption or GRPC load balancing is required. Linkerd will automatically handle TCP loadbalancing so when GRPC is used Linkerd will detect this and loadbalance between instances of GRPC servers. Refer to the official documentation for documentation that may be missing from this page. Linkerd works by injecting a sidecar into every Pod which uses Linkerd. All network requests have to be sent through the sidecar which will then be responsible for forwarding it. The sidecar will handle things like traffic encryption before sending the packets outside of the node.  "},{"title":"Get Started​","type":1,"pageTitle":"Networking","url":"docs/xks/developer-guide/networking#get-started","content":"To enable sidecar injection the Pod has to have the annotation linkerd.io/inject: enabled added to it. A common mistake when enabling Linkerd is that the annotation is added to Deployment and not the Pod template, make sure that you do not do this as the sidecar will not be injected if you do. apiVersion: apps/v1 kind: Deployment metadata: name: linkerd-test spec: replicas: 1 selector: matchLabels: app: linkerd-test template: metadata: annotations: linkerd.io/inject: enabled labels: app: linkerd-test spec: containers: - name: linkerd-test image: alpine:latest ports: - containerPort: 8080 name: http protocol: TCP Copy Eventually a Pod should be created. An important detail is that there should be two containers in the Pod. One container should be the one defined in the Deployment and the other one the Linkerd sidecar. This can be verified by getting the Pod's containers: $ kubectl get pods &lt;POD_NAME&gt; -o jsonpath=&quot;{.spec.containers[*].name}&quot; linkerd-test linkerd-proxy Copy With the sidecar added all traffic going out of the container will automatically be proxied through the sidecar. "},{"title":"FAQ​","type":1,"pageTitle":"Networking","url":"docs/xks/developer-guide/networking#faq","content":"Is all network traffic encrypted?​ No, it depends on the traffic type and is something that should be verified rather than assumed. More information can be found in the Linkerd documentation. What overhead can I expect?​ Each Pod will at a minimum consume an additional 10 MB due to the extra sidecar, and the number can grow as traffic increases. "},{"title":"Security","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/security","content":"","keywords":""},{"title":"Constraint Policies​","type":1,"pageTitle":"Security","url":"docs/xks/developer-guide/security#constraint-policies","content":"Certain policies will always be enforced in the cluster as a guard rail to minimize the risk of security exposures. The policies are implemented on the Kubernetes API level so that any request to create or update a Kubernetes resource will first have to pass through the policy check. If the resource in question does not comform to the policy it will be rejected by the API server when applying the change. Knowing this is important as certain features or options documented on the Internet may not be available or are restricted in the XKS service. This can include things like certain types of volume mounts, labeling requirements or container capabilities for a Pod. "},{"title":"Secrets Management","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/secrets-management","content":"","keywords":""},{"title":"Auto updating secrets​","type":1,"pageTitle":"Secrets Management","url":"docs/xks/developer-guide/secrets-management#auto-updating-secrets","content":"The CSI drivers sits and pools the cloud provider for changes in the secrets defined in your secretProviderClass. Depending on which cloud provider they have different default values. But the CSI driver only pools the secrets if you have a pod that is actively using the secret defined in the cloud. This can cause issues if you are using CSI driver in a cronjob and that secret don't have any other long running pods that mounts it. For example a cronjob that runs for under 2 minutes is not guaranteed to get the latest updated secret from the cloud provider. And even if you have a longer running cronjob that would get a update you would then have to restart that job to make sure that you got the latest secret if someone have updated the secret. To workaround this issue you have to create a pod that uses the same secret to make sure that the secret is up to date. All this long running pod needs to do is to mount the secret and sleep. Below you can find a suggestion on how to that: apiVersion: apps/v1 kind: Deployment metadata: name: foo spec: template: spec: containers: - name: busybox image: busybox:latest command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;--&quot;] args: [&quot;while true; do sleep 30; done;&quot;] tty: true volumeMounts: - name: secret-store mountPath: &quot;/mnt/secrets-store&quot; readOnly: true env: - name: BAR valueFrom: secretKeyRef: name: bar key: bar volumes: - name: secret-store csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: foo Copy "},{"title":"Cloud Providers​","type":1,"pageTitle":"Secrets Management","url":"docs/xks/developer-guide/secrets-management#cloud-providers","content":""},{"title":"Azure​","type":1,"pageTitle":"Secrets Management","url":"docs/xks/developer-guide/secrets-management#azure","content":"The Azure provider for the CSI driver requires some configuration to work, as it is possible to have multiple Azure Key Vaults. For that reason the Secret Provider Class has to specify the name of the Key Vault and the tenant id where the CSI Driver can find the secret. Additionally it is important to set usePodIdentity: &quot;true&quot; as authentication to the Azure API will be done with the help of AAD Pod Identity. apiVersion: secrets-store.csi.x-k8s.io/v1alpha1 kind: SecretProviderClass metadata: name: connection-string-test spec: provider: azure parameters: usePodIdentity: &quot;true&quot; keyvaultName: &quot;kvname&quot; objects: | array: - | objectName: connectionstring objectType: secret tenantId: &quot;11111111-1111-1111-1111-111111111111&quot; Copy To use the Secret Provider Class simply mount it as a volume in the Pod where you want to read the secret. The only extra configuration that is required is setting the label aadpodidbinding to the name of the Azure Identity. This is required as the CSI Driver will assume the Pods identity when authenticating with the Azure API. Without this label the fetching of the secret will fail. apiVersion: apps/v1 kind: Deployment metadata: name: connection-string-test namespace: tenant spec: selector: matchLabels: app: connection-string-test template: metadata: labels: app: connection-string-test aadpodidbinding: tenant spec: containers: - name: connection-string-test image: alpine:latest volumeMounts: - name: secret-store mountPath: &quot;/mnt/secrets-store&quot; readOnly: true volumes: - name: secret-store csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: connection-string-test Copy "},{"title":"AWS​","type":1,"pageTitle":"Secrets Management","url":"docs/xks/developer-guide/secrets-management#aws","content":"There are two secret store services in AWS that is supported by the CSI Driver, AWS Secret Manager and AWS System Manager Parameter Store. Both services have their own pros and cons in regards to features and pricing, but in the end both services deliver the same features in the cluster. The example below shows how to read the secret application/connection-string-test/connectionstring with examples for both Secret Manager and System Manager Parameter Store. Create an IAM role which gives permission to read the specific secret, note that the full ARN path including the secret name is included in the resource field. This is to limit secret acccess for the application as there is only a single service instance per account and region. The CSI Driver also requires the secretsmanager:ListSecrets permission or ssm:DescribeParameters to function properly. It will not be able to read any secret values with this permission, just list them. data &quot;aws_iam_policy_document&quot; &quot;db_connection_string&quot; { statement { effect = &quot;Allow&quot; actions = [ &quot;secretsmanager:ListSecrets&quot;, ] resources = [&quot;*&quot;] } statement { effect = &quot;Allow&quot; actions = [ &quot;secretsmanager:GetSecretValue&quot;, &quot;secretsmanager:DescribeSecret&quot;, &quot;secretsmanager:GetResourcePolicy&quot;, &quot;secretsmanager:ListSecretVersionIds&quot; ] resources = [&quot;arn:aws:secretsmanager:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:secret:application/connection-string-test/connectionstring&quot;] } } Copy or data &quot;aws_iam_policy_document&quot; &quot;db_connection_string&quot; { statement { effect = &quot;Allow&quot; actions = [ &quot;ssm:DescribeParameters&quot;, ] resources = [&quot;*&quot;] } statement { effect = &quot;Allow&quot; actions = [ &quot;ssm:GetParameter&quot;, &quot;ssm:GetParameters&quot;, ] resources = [&quot;arn:aws:ssm:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:parameter/db-*&quot;] } } Copy Complete the configuration by passing the policy document to the IRSA module which will create the IAM policy and role, this should be the same for both Secret Manager and System Manager Parameter Store. module &quot;irsa_test&quot; { source = &quot;github.com/xenitab/terraform-modules//modules/aws/irsa?ref=2021.08.9&quot; name = &quot;irsa-test&quot; oidc_providers = [ for v in var.oidc_urls : { url = v arn = aws_iam_openid_connect_provider.this[v].arn } ] kubernetes_namespace = &quot;tenant&quot; kubernetes_service_account = &quot;connection-string-test&quot; policy_json = data.aws_iam_policy_document.get_login_profile.json } Copy After the IAM role and policy have been created a Secret Provider Class has to be created specifying the secrets that should be read. Make sure to specify the correct object type, it should either besecretsmanager or ssmparameter. Note the configuration of objectAlias for the object. This is required as the secret name contains the character / in the name. By default the CSI Driver uses the name as the file name, which would cause issues as this is not permitted in Linux. The solution is to give the secret an alias instead. apiVersion: secrets-store.csi.x-k8s.io/v1alpha1 kind: SecretProviderClass metadata: name: connection-string-test namespace: tenant spec: provider: aws parameters: objects: | - objectName: &quot;application/connection-string-test/connectionstring&quot; objectType: &quot;secretsmanager&quot; | &quot;ssmparameter&quot; objectAlias: &quot;connectionstring&quot; secretObjects: - data: - key: password objectName: &quot;connectionstring&quot; secretName: connectionstring type: Opaque Copy Create a deployment which mounts the secret from the remote service. The secret is mounted as a volume in the Pod and will be populated with the value stored in the remote service. It is important that the Service Account is configured properly as the CSI Driver will assume the Pod's role when fetching the secret value. apiVersion: v1 kind: ServiceAccount metadata: name: connection-string-test namespace: tenant annotations: eks.amazonaws.com/role-arn: arn:aws:iam::111111111111:role/connection-string-test --- apiVersion: apps/v1 kind: Deployment metadata: name: connection-string-test namespace: tenant spec: selector: matchLabels: app: connection-string-test template: metadata: labels: app: connection-string-test spec: serviceAccountName: connection-string-test containers: - name: connection-string-test image: alpine:latest volumeMounts: - name: secret-store mountPath: &quot;/mnt/secrets-store&quot; readOnly: true volumes: - name: secret-store csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: connection-string-test Copy "},{"title":"Automatic Reloading​","type":1,"pageTitle":"Secrets Management","url":"docs/xks/developer-guide/secrets-management#automatic-reloading","content":"A Pod will get the latest version of the Secret Provider Class when started. The CSI Driver will poll the secret and update when the secret value is updated. However the Pod will not be updated as this would require the application to be able to restart the process and read the file instead. The Pod will not receive the new value until a new instance of the Pod is created. This could become annoying for situations where the secret value may change often or there are a lot of secrets being read. The solution in XKS is to configure the Secret Provider Class to annotate the Pod to be recreated when the Secret value is updated. The Pod recreation is done with theReloader project which is present in all XKS clusters. Reloader works by adding an annotation with the key secret.reloader.stakater.com/reload, where the value is the name of the secret. If you need to recreate your Pod when any of multiple secrets are changed, use comma-separated values: secret.reloader.stakater.com/reload: &quot;foo,bar&quot; Copy When using an object alias the object name in the secrets objects refers to the alias and not to the original object name. Below is an example of creating a Service Provider Class which also creates a Kubernetes Secret, there is no need to actually use the created secret but in the example below it is mounted as an environment variable. apiVersion: secrets-store.csi.x-k8s.io/v1alpha1 kind: SecretProviderClass metadata: name: application namespace: tenant spec: provider: &lt;provider&gt; parameters: objects: | - objectName: &quot;foo&quot; objectType: &quot;&lt;type&gt;&quot; secretObjects: - data: - key: bar objectName: foo secretName: foo type: Opaque --- apiVersion: apps/v1 kind: Deployment metadata: name: application namespace: tenant spec: selector: matchLabels: app: application template: metadata: annotations: secret.reloader.stakater.com/reload: &quot;foo&quot; labels: app: application spec: serviceAccountName: application containers: - name: application image: alpine:latest env: - name: BAR valueFrom: secretKeyRef: name: foo key: bar volumeMounts: - name: secret-store mountPath: &quot;/mnt/secrets-store&quot; readOnly: true volumes: - name: secret-store csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: foo Copy "},{"title":"Troubleshooting​","type":1,"pageTitle":"Secrets Management","url":"docs/xks/developer-guide/secrets-management#troubleshooting","content":"There are a lot of things that can go wrong when configuring Secrets. Here are some pointers for things to check: "},{"title":"Did you forget to declare the SecretProviderClass?​","type":1,"pageTitle":"Secrets Management","url":"docs/xks/developer-guide/secrets-management#did-you-forget-to-declare-the-secretproviderclass","content":"Remember, getting access to your secrets consists of 2 separate parts: a SecretProviderClass, which tells Kubernetes where it can get the stored secreta mount of the provided secrets-store for your deployment "},{"title":"Verifying your loaded YAML​","type":1,"pageTitle":"Secrets Management","url":"docs/xks/developer-guide/secrets-management#verifying-your-loaded-yaml","content":"By running kubectl get secretproviderclasspodstatuses -o yaml you can get a lot of information about if and how your secrets got correctly loaded. Check here first! For example, look out to see that all the secrets you expect to see are available, and that they are mounted: status: mounted: true Copy "},{"title":"Is your key vault correctly configured, and does the pod have access to it?​","type":1,"pageTitle":"Secrets Management","url":"docs/xks/developer-guide/secrets-management#is-your-key-vault-correctly-configured-and-does-the-pod-have-access-to-it","content":"Speaking from experience, it is all too easy to setup access to the wrong key vault. If you are accessing the right key vault and you are using Azure, double check thatusePodIdentity: &quot;true&quot; is set on the SecretProviderClass. You also need to make sure that the metadata of your deployment's template section contains a declaration of which aadpodidbinding to use (which is always your tenant's name): apiVersion: apps/v1 kind: Deployment metadata: name: connection-string-test namespace: tenant spec: selector: matchLabels: app: connection-string-test template: metadata: labels: app: connection-string-test aadpodidbinding: tenant Copy Please also verify that aadpodidbinding is set on the metadata section under template and not on the root metadata section. The latter will not work. "},{"title":"Are your environment variables correctly set?​","type":1,"pageTitle":"Secrets Management","url":"docs/xks/developer-guide/secrets-management#are-your-environment-variables-correctly-set","content":"If you want to load your secret as an environment variable, remember that it still needs to be mounted as a volume. Also, don't forget that it doesn't automatically become available as a corresponding environment variable, you still need to load it explicitly, like this: env: - name: BAR valueFrom: secretKeyRef: name: foo key: bar Copy "},{"title":"Are your secret names matching the names in the reloader statement?​","type":1,"pageTitle":"Secrets Management","url":"docs/xks/developer-guide/secrets-management#are-your-secret-names-matching-the-names-in-the-reloader-statement","content":"If this is not the case, you will see errors when running kubectl describe &lt;podname&gt;. A well-behaved reloader will emit events that look like this: Normal SecretRotationComplete 4m22s (x889 over 29h) csi-secrets-store-rotation successfully rotated K8s secret &lt;secret-name&gt; Copy "},{"title":"Starboard","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/starboard","content":"","keywords":""},{"title":"Vulnerability reports​","type":1,"pageTitle":"Starboard","url":"docs/xks/developer-guide/starboard#vulnerability-reports","content":"To get specific data about your container image you need to login in to the Kubernetes cluster and look at the generated CRD:s. To view all the vulnerability reports that you have in the current namespace: kubectl get vulnerabilityreports Copy To get a specific vulnerability report just run a describe on the object in current namespace. kubectl describe vulnerabilityreports.aquasecurity.github.io &lt;the-specific-report&gt; Copy For more specific questions around Starboard we recommend reading the official documentation. "},{"title":"Overview","type":0,"sectionRef":"#","url":"docs/xks/index","content":"Overview Xenit Kubernetes Service is an opinionated Kubernetes deployment on top of a cloud provider's managed Kubernetes service. XKS currently supports Azure Kubernetes Service (AKS) and AWS Elastic Kubernetes service (EKS). Xenit Kubernetes Service: is secure by defaultis DevOps-orientedhas batteries includedhas a team-oriented permissions model This documentation consists of two main sections: Developer Guide: This documentation is targeted towards developers using XKS. It covers the basics of Kubernetes and the custom features that are offered by XKS. Operator Guide: This section is meant primarily for Xenit's operations staff. It collects Xenit's internal documentation for operating XKS clusters. It is public and part of this documentation because we believe in transparency. It serves as a reference to how the various services included in XKS are set up. It also describes various recurring procedures, such as replacing an existing Kubernetes cluster. XKS is assembled from Open Source services, some of which are provided to XKS customers. This assembly is itself Open Source and the important components are documented under the Projects section in the menu bar. For more information about the services, please refer to their respective documentation. Some of the more prominent projects: KubernetesFluxNginx Ingress Controller","keywords":""},{"title":"Working with XKF from Windows","type":0,"sectionRef":"#","url":"docs/xks/developer-guide/wsl2","content":"","keywords":""},{"title":"Installation of WSL2 - Windows Subsystem for Linux​","type":1,"pageTitle":"Working with XKF from Windows","url":"docs/xks/developer-guide/wsl2#installation-of-wsl2---windows-subsystem-for-linux","content":"Install via Powershell Installation is also possible via Microsoft Store. Search for “linux”. Make sure that Windows Subsystem for Linux is enabled as a feature in Windows. In Windows: Go to Control Panel → Programs and Features. In the left-hand menu, select “Turn Windows features on or off”  "},{"title":"Install Docker-Desktop​","type":1,"pageTitle":"Working with XKF from Windows","url":"docs/xks/developer-guide/wsl2#install-docker-desktop","content":"Download and install Docker. Once installation is complete, verify that the application works. We experienced issues when trying to start Docker-desktop within a managed organization using AD accounts, this caused an error with us not being members of a group called “docker-users“. To solve this, open up “Computer Management” in Windows as an administrator. Navigate to “local users and groups” → Groups and locate the &quot;docker-users&quot; group and double-click.Press “Add” and search for “Authenticated Users” and add to the group. Now sign out from Windows and sign back in, and the Docker application should work. Verify in settings that the WSL2-based engine is used.  Also under settings, go to Resources → WSL Integration and verify that you have access to the WSL integration with your installed WSL (in this case Ubuntu), and make sure it is checked.  To verify functionality: In your Ubuntu WSL2 instance - Run the command: docker run hello-world Copy Wait for the image to be pulled and if everything works properly the output should be: Hello from Docker! This message shows that your installation appears to be working correctly. "},{"title":"Utilising Make with WSL2, Terraform and Docker​","type":1,"pageTitle":"Working with XKF from Windows","url":"docs/xks/developer-guide/wsl2#utilising-make-with-wsl2-terraform-and-docker","content":"We have noticed when running Terraform from within our Ubuntu instance, that there appears to be some network issues. We saw quite slow network connections, probably caused by the TCP connection, which resulted in the following error: │ Error: Failed to install provider │ │ Error while installing hashicorp/azurerm v2.64.0: local error: tls: bad record MAC We ran the Terraform command again - and it worked perfectly. "},{"title":"Issues​","type":1,"pageTitle":"Working with XKF from Windows","url":"docs/xks/developer-guide/wsl2#issues","content":""},{"title":"File lock issues​","type":1,"pageTitle":"Working with XKF from Windows","url":"docs/xks/developer-guide/wsl2#file-lock-issues","content":"If your .azure folder is mounted towards Windows from your WSL2 environment we have seen potential lock issues when running terraform and azure-cli 2.32.0, this might apply to other versions as well. We think this have something to do with how WSL2 and Windows manages locking of files, to workaround you can make sure that your .azure folder is only in your Linux environment. Assuming that you have not defined a custom AZURE_CONFIG_DIR you can perform the following to verify you are mounting your .azure folder to Windows: $ cd $ ls -la drwx------ 61 user1 user1 4096 Jan 14 09:21 . drwxr-xr-x 4 root root 4096 Feb 12 2021 .. drwxr-xr-x 7 user1 user1 4096 Dec 12 13:04 .azure -&gt; /mnt/c/Users/user1/.azure Copy Running the following commands will create a new .azure folder in your current working directory and tell azure-cli to use that folder to store its login data. Remember that the export command only is per terminal. You can make the config persistent by adding the export command to your .bashrc file located in your home folder. export AZURE_CONFIG_DIR=$(pwd)/.azure # Store Azure CLI configuration here [ -d $(pwd)/.azure ] || mkdir $(pwd)/.azure # login to azure agaiin az login Copy This is a example how the traceback look liked when we encountered this error: │ Error: getting authenticated object ID: Error parsing json result from the Azure CLI: Error waiting for the Azure CLI: exit status 1: ERROR: The command failed with an unexpected error. Here is the traceback: │ ERROR: [Errno 2] No such file or directory │ Traceback (most recent call last): │ File &quot;/opt/az/lib/python3.6/site-packages/azure/cli/core/commands/command_operation.py&quot;, line 352, in handler │ client = self.client_factory(self.cli_ctx) if self.client_factory else None │ TypeError: get_graph_client_signed_in_users() missing 1 required positional argument: '_' │ │ During handling of the above exception, another exception occurred: │ │ Traceback (most recent call last): │ File &quot;/opt/az/lib/python3.6/site-packages/knack/cli.py&quot;, line 231, in invoke │ cmd_result = self.invocation.execute(args) │ File &quot;/opt/az/lib/python3.6/site-packages/azure/cli/core/commands/__init__.py&quot;, line 657, in execute │ raise ex │ File &quot;/opt/az/lib/python3.6/site-packages/azure/cli/core/commands/__init__.py&quot;, line 720, in _run_jobs_serially │ results.append(self._run_job(expanded_arg, cmd_copy)) │ File &quot;/opt/az/lib/python3.6/site-packages/azure/cli/core/commands/__init__.py&quot;, line 712, in _run_job │ return cmd_copy.exception_handler(ex) │ File &quot;/opt/az/lib/python3.6/site-packages/azure/cli/command_modules/role/commands.py&quot;, line 69, in graph_err_handler │ raise ex │ File &quot;/opt/az/lib/python3.6/site-packages/azure/cli/core/commands/__init__.py&quot;, line 691, in _run_job │ result = cmd_copy(params) │ File &quot;/opt/az/lib/python3.6/site-packages/azure/cli/core/commands/__init__.py&quot;, line 328, in __call__ │ return self.handler(*args, **kwargs) │ File &quot;/opt/az/lib/python3.6/site-packages/azure/cli/core/commands/command_operation.py&quot;, line 354, in handler │ client = self.client_factory(self.cli_ctx, command_args) if self.client_factory else None │ File &quot;/opt/az/lib/python3.6/site-packages/azure/cli/command_modules/role/commands.py&quot;, line 89, in get_graph_client_signed_in_users │ return _graph_client_factory(cli_ctx).signed_in_user │ File &quot;/opt/az/lib/python3.6/site-packages/azure/cli/command_modules/role/_client_factory.py&quot;, line 25, in _graph_client_factory │ resource=cli_ctx.cloud.endpoints.active_directory_graph_resource_id) │ File &quot;/opt/az/lib/python3.6/site-packages/azure/cli/core/_profile.py&quot;, line 335, in get_login_credentials │ credential = self._create_credential(account, client_id=client_id) │ File &quot;/opt/az/lib/python3.6/site-packages/azure/cli/core/_profile.py&quot;, line 588, in _create_credential │ return identity.get_user_credential(username_or_sp_id) │ File &quot;/opt/az/lib/python3.6/site-packages/azure/cli/core/auth/identity.py&quot;, line 182, in get_user_credential │ return UserCredential(self.client_id, username, **self._msal_app_kwargs) │ File &quot;/opt/az/lib/python3.6/site-packages/azure/cli/core/auth/msal_authentication.py&quot;, line 41, in __init__ │ accounts = self.get_accounts(username) │ File &quot;/opt/az/lib/python3.6/site-packages/msal/application.py&quot;, line 872, in get_accounts │ accounts = self._find_msal_accounts(environment=self.authority.instance) │ File &quot;/opt/az/lib/python3.6/site-packages/msal/application.py&quot;, line 912, in _find_msal_accounts │ query={​​&quot;environment&quot;: environment}​​) │ File &quot;/opt/az/lib/python3.6/site-packages/msal_extensions/token_cache.py&quot;, line 53, in find │ with CrossPlatLock(self._lock_location): │ File &quot;/opt/az/lib/python3.6/site-packages/msal_extensions/cache_lock.py&quot;, line 29, in __enter__ │ file_handle = self._lock.__enter__() │ File &quot;/opt/az/lib/python3.6/site-packages/portalocker/utils.py&quot;, line 199, in __enter__ │ return self.acquire() │ File &quot;/opt/az/lib/python3.6/site-packages/portalocker/utils.py&quot;, line 161, in acquire │ fh = self._prepare_fh(fh) │ File &quot;/opt/az/lib/python3.6/site-packages/portalocker/utils.py&quot;, line 194, in _prepare_fh │ fh.truncate(0) │ FileNotFoundError: [Errno 2] No such file or directory │ To open an issue, please run: 'az feedback' │ │ with provider[&quot;registry.terraform.io/hashicorp/azuread&quot;], │ on main.tf line 19, in provider &quot;azuread&quot;: │ 19: provider &quot;azuread&quot; {​​}​​ Copy "},{"title":"AWS azure-devops","type":0,"sectionRef":"#","url":"docs/xks/operator-guide/aws-azdo","content":"","keywords":""},{"title":"Terraform​","type":1,"pageTitle":"AWS azure-devops","url":"docs/xks/operator-guide/aws-azdo#terraform","content":"All this is possible to solve using Terraform but we currently do not have any terraform module to handle it so for now you can read about the manual steps. Create an IAM user Tip: put both these users under a specific path, for example CISave the access key in a safe place Attach IAM policy AmazonEKSClusterPolicyAdministratorAccess "},{"title":"GitOps user​","type":1,"pageTitle":"AWS azure-devops","url":"docs/xks/operator-guide/aws-azdo#gitops-user","content":"In our GitOps pipelines using Flux we need to be able to push and delete images in ECR. So very similar to how we created the Terraform user: Create an IAM user Tip: put both these users under a specific path, for example CISave the credentials in a safe place Attach IAM policy AmazonEC2ContainerRegistryFullAccess "},{"title":"Blast Radius","type":0,"sectionRef":"#","url":"docs/xks/operator-guide/blast-radius","content":"","keywords":""},{"title":"Workflow​","type":1,"pageTitle":"Blast Radius","url":"docs/xks/operator-guide/blast-radius#workflow","content":"Our normal way of working uses a Makefile to make managing commands easier. When running Terraform locally and in our CI pipelines we use our Markdown files. "},{"title":"OPA Blast Radius calculation​","type":1,"pageTitle":"Blast Radius","url":"docs/xks/operator-guide/blast-radius#opa-blast-radius-calculation","content":"The calculation of the blast radius value is done in the Dockerfile that is started through the Makefile. We use OPA to calculate the blast radius itself. To see exactly how we do it you can look in this bash script The default value is currently set to 50. "},{"title":"Overwrite OPA blast radius locally​","type":1,"pageTitle":"Blast Radius","url":"docs/xks/operator-guide/blast-radius#overwrite-opa-blast-radius-locally","content":"Run your make command normally, but when running plan add OPA_BLAST_RADIUS and the value you want. make plan ENV=dev DIR=governance OPA_BLAST_RADIUS=51 Copy "},{"title":"Overwrite OPA blast radius in CI​","type":1,"pageTitle":"Blast Radius","url":"docs/xks/operator-guide/blast-radius#overwrite-opa-blast-radius-in-ci","content":"We are using Just Enough Administration (JEA) at Xenit and in many cases our admins do not have enough access to run Terraform plan/apply locally. Instead we are forced to use our CI/CD systems to manage this for us. If you look at the Makefile you will see that we do not use any environment variables to overwrite the OPA_BLAST_RADIUS value. So how should we change the OPA_BLAST_RADIUS without having to update the pipeline file every time we want to overwrite the default value? Sadly here comes some magic, if you look in https://github.com/XenitAB/azure-devops-templates/you will see that we are listening for the environment variable opaBlastRadius. So to overwrite the OPA_BLAST_RADIUS value during a single run we can utilize the opaBlastRadius environment variable. To add a custom value to your pipeline in Azure DevOps do the following: Pipelines -&gt; &quot;pipeline-you-want-to-run&quot; -&gt; Run pipeline -&gt; Variables -&gt; Add variable Add opaBlastRadius=51, it should look something like this:  To start the job you have to push &quot;&lt;-&quot; and Run. Remember, this variable is only set for one run. "},{"title":"Agents","type":0,"sectionRef":"#","url":"docs/xks/operator-guide/agents","content":"","keywords":""},{"title":"Governance​","type":1,"pageTitle":"Agents","url":"docs/xks/operator-guide/agents#governance","content":"The first step is to create the resource groups for the hub and agents. In the governance Terraform two resource groups have to be added to the common.tfvars file in the variables directory. The hub resource group has to be created for both Azure DevOps and GitHub, the only difference is the name of the resource group where the agents are located.  { common_name = &quot;hub&quot;, delegate_aks = false, delegate_key_vault = true, delegate_service_endpoint = false, delegate_service_principal = false, lock_resource_group = false, tags = { &quot;description&quot; = &quot;Hub Network&quot; } }, # Azure DevOps { common_name = &quot;azpagent&quot;, delegate_aks = false, delegate_key_vault = true, delegate_service_endpoint = false, delegate_service_principal = true, lock_resource_group = false, tags = { &quot;description&quot; = &quot;Azure Pipelines Agent&quot; } }, # GitHub { common_name = &quot;ghrunner&quot;, delegate_aks = false, delegate_key_vault = true, delegate_service_endpoint = false, delegate_service_principal = true, lock_resource_group = false, tags = { &quot;description&quot; = &quot;GitHub Runner&quot; } }, Copy The Service Principal credentials need to be stored as a secret when running Packer from GitHub. This step does not have to be followed when setting up Azure DevOps. The Service Principal id and credentials can be retrieved after the Terraform has been applied. Read the getting started guide for information about how to get the credential information, the difference being that the application will be named sp-rg-xks-prod-ghrunner-contributor instead of az-mg-lz-xks-owner. The secret should be added to the repository packer, as the VM image only has to be built for production it is enough to create the secret AZURE_CREDENTIALS_PROD. The format of the secret content should be as in the example below. {&quot;clientId&quot;: &quot;00000000-0000-0000-0000-000000000000&quot;, &quot;clientSecret&quot;: &quot;super-duper-secret-value&quot;, &quot;subscriptionId&quot;: &quot;00000000-0000-0000-0000-000000000000&quot;, &quot;tenantId&quot;: &quot;00000000-0000-0000-0000-000000000000&quot;} Copy "},{"title":"VM Image​","type":1,"pageTitle":"Agents","url":"docs/xks/operator-guide/agents#vm-image","content":"We make use of Packer to create the VM images. Packer allows for the automation of the process, creating the VM snapshot for us. The VM image has to be created before any VM image can be created. Create a repository called packer that is going to contain the CI jobs that will build the VM images. Doing this will allow for tracking of versions and automate the complicated build process. There are templates for Azure DevOps that can be used to build the VM images for the agents. The following pipeline definition should be commited to the file .ci/azure-pipelines-agent.yaml in the new packer repository. After that is done create a Azure DevOps pipeline for the given pipeline definition. name: $(Build.BuildId) trigger: none resources: repositories: - repository: templates type: git name: XKS/azure-devops-templates ref: refs/tags/2020.12.5 stages: - template: packer-docker/main.yaml@templates parameters: poolNameTemplate: &quot;&quot; azureSubscriptionTemplate: &quot;xks-{0}-owner&quot; resourceGroupTemplate: &quot;rg-{0}-we-azpagent&quot; packerTemplateRepo: &quot;https://github.com/XenitAB/packer-templates.git&quot; packerTemplateRepoBranch: &quot;2021.06.1&quot; packerTemplateFile: &quot;templates/azure/azure-pipelines-agent/azure-pipelines-agent.json&quot; Copy There is also a template for GitHub that can be used for building with Packer. The following pipeline definition should be committed to the file .github/workflows/github-runner.yaml in the packerrepository. name: packer_github_runner on: workflow_dispatch: {} jobs: packer: uses: xenitab/azure-devops-templates/.github/workflows/packer-docker.yaml@2021.11.1 with: ENVIRONMENTS: | { &quot;environments&quot;:[ {&quot;name&quot;:&quot;prod&quot;} ] } RESOURCE_GROUP_NAME_SUFFIX: &quot;ghrunner&quot; PACKER_TEMPLATE_REPO: &quot;https://github.com/XenitAB/packer-templates.git&quot; PACKER_TEMPLATE_REPO_BRANCH: &quot;2021.06.1&quot; PACKER_TEMPLATE_FILE: &quot;templates/azure/azure-pipelines-agent/azure-pipelines-agent.json&quot; secrets: AZURE_CREDENTIALS_PROD: ${{ secrets.AZURE_CREDENTIALS_PROD }} Copy Start the Packer build pipeline and allow it to run until completion. This may take up to 40 minutes to run so give it time. Afte the build is completed a new VM image should be created and stored in the agent's resource group in Azure. The name of the image is dynamic and includes a timestamp to allow versioning of the images. The following Azure CLI command gets the name of the image: # Assuming that you do not have any other image this RG. az image list -o json --query '[0].name' Copy The name should be similar to azp-agent-2021-04-09T08-18-30Z. "},{"title":"Pre Setup​","type":1,"pageTitle":"Agents","url":"docs/xks/operator-guide/agents#pre-setup","content":""},{"title":"GitHub​","type":1,"pageTitle":"Agents","url":"docs/xks/operator-guide/agents#github","content":"When using GitHub Runners a GitHub application has to be created that will allow the agent to communicate back to GitHub. Follow the steps in the GitHub Runner Documentation for instructions in how to create the GitHub Application with the correct permissions. In the end you should have created and installed a GitHub Application and have an application id, installation id, and private key. These parameters should all be stored in the already created Azure Key Vault in the ghrunner resource group. The secrets should be named github-app-id, github-private-key,github-installation-id, and github-organization. "},{"title":"Terraform​","type":1,"pageTitle":"Agents","url":"docs/xks/operator-guide/agents#terraform","content":"When setting up the Terraform make sure to set the correct value for azure_pipelines_agent_image_name or github_runner_image_name. If everything has been configured properly the hub VNET and VMs should be created without any issues. module &quot;hub&quot; { source = &quot;github.com/xenitab/terraform-modules//modules/azure/hub?ref=2021.05.12&quot; environment = var.environment location_short = var.location_short subscription_name = var.subscription_name azure_ad_group_prefix = var.azure_ad_group_prefix name = var.name vnet_config = var.vnet_config peering_config = var.peering_config } # Azure DevOps module &quot;azpagent&quot; { source = &quot;github.com/xenitab/terraform-modules//modules/azure/azure-pipelines-agent-vmss?ref=2021.05.12&quot; environment = var.environment location_short = var.location_short unique_suffix = var.unique_suffix name = &quot;azpagent&quot; azure_pipelines_agent_image_name = &quot;azp-agent-2021-06-11T06-44-34Z&quot; vmss_sku = &quot;Standard_F4s_v2&quot; vmss_disk_size_gb = 64 vmss_subnet_config = { name = module.hub.subnets[&quot;sn-${var.environment}-${var.location_short}-${var.name}-servers&quot;].name virtual_network_name = module.hub.virtual_networks.name resource_group_name = module.hub.resource_groups.name } } # GitHub module &quot;ghrunner&quot; { source = &quot;github.com/xenitab/terraform-modules//modules/azure/github-runner?ref=2021.05.12&quot; environment = var.environment location_short = var.location_short name = &quot;ghrunner&quot; github_runner_image_name = &quot;github-runner-2020-12-07T22-06-18Z&quot; vmss_sku = &quot;Standard_D2s_v3&quot; vmss_instances = 2 vmss_disk_size_gb = 50 unique_suffix = var.unique_suffix vmss_subnet_config = { name = module.hub.subnets[&quot;sn-${var.environment}-${var.location_short}-${var.name}-servers&quot;].name virtual_network_name = module.hub.virtual_networks.name resource_group_name = module.hub.resource_groups.name } } Copy "},{"title":"Post Setup​","type":1,"pageTitle":"Agents","url":"docs/xks/operator-guide/agents#post-setup","content":"After the cloud resources have been created their respective git providers have to be configured to be aware of the agent pools. Follow the instructions below to complete the post setup. "},{"title":"Azure DevOps​","type":1,"pageTitle":"Agents","url":"docs/xks/operator-guide/agents#azure-devops","content":"This step only has to be followed when setting up Azure DevOps Agents. To be able to communicate with the VMSS we need to configure a Service Connection. You will find service connection under a random project within Azure DevOps. To setup the Service Connection you need to get a secret generated by Terraform. # Assuming that you are connected to the correct subscription az keyvault secret show --vault-name &lt;vault-name&gt; --name &lt;secret-name&gt; -o tsv --query value # Example az keyvault secret show --vault-name kv-prod-we-core-1337 --name sp-rg-xks-prod-azpagent-contributor -o tsv --query value Copy Service Connections​ To create a new Service connection from Azure DevOps: Project settings -&gt; Service connections -&gt; New service connection -&gt; Azure Resource Manager -&gt; Service principal (manual) {&quot;clientId&quot;:&quot;12345&quot;,&quot;clientSecret&quot;:&quot;SoMuchSecret&quot;,&quot;subscriptionId&quot;:&quot;sub-id&quot;,&quot;tenantId&quot;:&quot;tenant-id&quot;} Subscription Id = subscriptionIdService Principal Id = clientIdService principal key = clientSecretTenant ID = tenantIdService connection name = random-name Agent Pool​ In Azure DevOps under project settings. Agent pools -&gt; Add Pool -&gt; Pick VMSS from dropdown  Billing​ Configure billing. This will increase your azure cost. Read up on how much on your own. Organization Settings -&gt; Billing Under &quot;Self-Hosted CI/CD&quot; set &quot;Paid parallel jobs&quot; = 3 "},{"title":"Peering Configuration​","type":1,"pageTitle":"Agents","url":"docs/xks/operator-guide/agents#peering-configuration","content":"To complete the setup we need to configure the VNET peering between the new hub VNET and the environments VNETs. This enables the agents to communicate with private resources without having to egress into the public Internet first. In the hubs prod.tfvars you want to add configuration to all VNETs that the VNET should have access to. If you have multiple environments there should be multiple entries in the list. peering_config = [ { name = &quot;core-dev&quot; remote_virtual_network_id = &quot;/subscriptions/your-sub-id/resourceGroups/rg-dev-we-core/providers/Microsoft.Network/virtualNetworks/vnet-dev-we-core&quot; allow_forwarded_traffic = true use_remote_gateways = false allow_virtual_network_access = true }, ] Copy A similar configuration has to be done in the core Terraform to complete the peering. peering_config = [ { name = &quot;hub&quot; remote_virtual_network_id = &quot;/subscriptions/your-sub-id/resourceGroups/rg-prod-we-hub/providers/Microsoft.Network/virtualNetworks/vnet-prod-we-hub&quot; allow_forwarded_traffic = true use_remote_gateways = false allow_virtual_network_access = true }, ] Copy "},{"title":"Blue Green Clusters","type":0,"sectionRef":"#","url":"docs/xks/operator-guide/blue-green","content":"","keywords":""},{"title":"Workflow​","type":1,"pageTitle":"Blue Green Clusters","url":"docs/xks/operator-guide/blue-green#workflow","content":"We assume that the workloads on the clusters are stateless and can run multiple instances. Set up a new cluster in the target environment using TerraformVerify that the new cluster is functioning as intended You will not be able to verify any ingressYou will not be able to use AZAD-proxy in the newly created cluster Change the TXT DNS records over to the newly created clusterVerify that the ingress traffic is migrated to the new cluster and it is working as intendedDestroy the old cluster using terraform "},{"title":"DNS migration​","type":1,"pageTitle":"Blue Green Clusters","url":"docs/xks/operator-guide/blue-green#dns-migration","content":"You can find a small small script bellow to make the migration of DNS easier. As always use at your own risk and make sure that you understand what the script does. Our recommendation is that you migrate one DNS record manually and verify that the ingress and the new cluster is working as intended, when you know that you can run the script. "},{"title":"Azure​","type":1,"pageTitle":"Blue Green Clusters","url":"docs/xks/operator-guide/blue-green#azure","content":"ENVIRONMENT=&quot;dev&quot; OLD_OWNER_ID=&quot;${ENVIRONMENT}-aks1&quot; NEW_OWNER_ID=&quot;${ENVIRONMENT}-aks2&quot; RESOURCE_GROUP_NAME=&quot;rg-${ENVIRONMENT}-we-aks&quot; ZONE_NAME=&quot;${ENVIRONMENT}.domain.se&quot; ZONE_RECORDS=$(az network dns record-set txt list -g ${RESOURCE_GROUP_NAME} -z ${ZONE_NAME} | jq -rc '.[]') ZONE_RECORDS_CSV_ARRAY=( $(jq -rc '. | [.name, (.txtRecords[0].value[0] | @base64)] | join(&quot;;&quot;)' &lt;&lt;&lt; &quot;${ZONE_RECORDS}&quot;) ) for ZONE_RECORD_CSV in &quot;${ZONE_RECORDS_CSV_ARRAY[@]}&quot;; do ZONE_RECORD_NAME=$(awk -F';' '{print $1}' &lt;&lt;&lt; $ZONE_RECORD_CSV) OLD_ZONE_TXT_VALUE=$(awk -F';' '{print $2}' &lt;&lt;&lt; $ZONE_RECORD_CSV | base64 -d) if [[ ${OLD_ZONE_TXT_VALUE} =~ &quot;owner=${OLD_OWNER_ID}&quot; ]]; then NEW_ZONE_TXT_VALUE=${OLD_ZONE_TXT_VALUE/owner=${OLD_OWNER_ID}/owner=${NEW_OWNER_ID}} echo Updating external-dns owner of ${ZONE_RECORD_NAME}: ${OLD_OWNER_ID} to ${NEW_OWNER_ID} az network dns record-set txt add-record --resource-group ${RESOURCE_GROUP_NAME} --zone-name ${ZONE_NAME} --record-set-name ${ZONE_RECORD_NAME} --value &quot;${NEW_ZONE_TXT_VALUE}&quot; 1&gt;/dev/null az network dns record-set txt remove-record --resource-group ${RESOURCE_GROUP_NAME} --zone-name ${ZONE_NAME} --record-set-name ${ZONE_RECORD_NAME} --value &quot;${OLD_ZONE_TXT_VALUE}&quot; 1&gt;/dev/null fi done Copy "},{"title":"XKF on Github","type":0,"sectionRef":"#","url":"docs/xks/operator-guide/github","content":"","keywords":""},{"title":"Terraform​","type":1,"pageTitle":"XKF on Github","url":"docs/xks/operator-guide/github#terraform","content":"How to run Terraform plan and apply through a GitHub action workflow. "},{"title":"Workflow​","type":1,"pageTitle":"XKF on Github","url":"docs/xks/operator-guide/github#workflow","content":"Just like in the Azure DevOps case we have created a basic pipeline for easy use. Below you can find an example pipeline that uses the Github Actions workflow. Read further down to see how to create the secrets needed to run the pipeline. You should store this GitHub action in your Terraform repository under .github/workflows/name.yaml name: terraform_core on: push: branches: - main paths: - core/** pull_request: paths: - core/** workflow_dispatch: inputs: OPA_BLAST_RADIUS: description: OPA Blast Radius required: true default: &quot;50&quot; jobs: terraform: uses: xenitab/azure-devops-templates/.github/workflows/terraform-docker.yaml@2021.10.1 with: DIR: core runs-on: '[&quot;self-hosted&quot;, &quot;linux&quot;]' # If you do not want to use the default ubuntu-latest ENVIRONMENTS: | { &quot;environments&quot;:[ {&quot;name&quot;:&quot;dev&quot;}, {&quot;name&quot;:&quot;qa&quot;}, {&quot;name&quot;:&quot;prod&quot;} ] } secrets: AZURE_CREDENTIALS_DEV: ${{ secrets.AZURE_CREDENTIALS_DEV }} AZURE_CREDENTIALS_QA: ${{ secrets.AZURE_CREDENTIALS_QA }} AZURE_CREDENTIALS_PROD: ${{ secrets.AZURE_CREDENTIALS_PROD }} Copy "},{"title":"Self-hosted runners​","type":1,"pageTitle":"XKF on Github","url":"docs/xks/operator-guide/github#self-hosted-runners","content":"It is currently not possible to use self-hosted runners hosted in GitHub organization X while calling on workflows in GitHub organization Y. To be able to use self-hosted runners you have to import (not fork) the repository to organization X the workflow repositoryand make it public. If you do not do this private repositories located in organization X will not be able to find the workflows. "},{"title":"Azure Service Principal​","type":1,"pageTitle":"XKF on Github","url":"docs/xks/operator-guide/github#azure-service-principal","content":"Create a Service Principal(SP) with the access that Terraform requires to perform all the tasks you want. You can read more about SP creation in our getting started guide The workflow is using Azure Login GitHub Actionto login to Azure. When uploading your SP to GitHub make sure to follow the formatting in the examples. This is to prevent unnecessary masking of { } in your logs which are in dictionary form. For example, do: {&quot;clientId&quot;: &quot;00000000-0000-0000-0000-000000000000&quot;, &quot;clientSecret&quot;: &quot;super-duper-secret-value&quot;, &quot;subscriptionId&quot;: &quot;00000000-0000-0000-0000-000000000000&quot;, &quot;tenantId&quot;: &quot;00000000-0000-0000-0000-000000000000&quot;} Copy instead of: { &quot;clientId&quot;: &quot;00000000-0000-0000-0000-000000000000&quot;, &quot;clientSecret&quot;: &quot;super-duper-secret-value&quot;, &quot;subscriptionId&quot;: &quot;00000000-0000-0000-0000-000000000000&quot;, &quot;tenantId&quot;: &quot;00000000-0000-0000-0000-000000000000&quot; } Copy Upload the entire JSON as your GitHub secret. The workflow uses one secret per environment and we recommend that you follow our naming standard. The secret name the workflow uses is AZURECREDENTIALS\\&lt;ENV&gt;, for example AZURE_CREDENTIALS_DEV. To upload the secret to GitHub you can use the GitHub UI or you can use the GitHub CLI to upload secrets to GitHub. Assuming that you are storing the SP JSON data in a file you could do: gh secret -R ORG/xks-terraform set AZURE_CREDENTIALS_DEV &lt; dev-secrets.json gh secret -R ORG/xks-terraform set AZURE_CREDENTIALS_QA &lt; qa-secrets.json gh secret -R ORG/xks-terraform set AZURE_CREDENTIALS_PROD &lt; prod-secrets.json Copy "},{"title":"Getting Started","type":0,"sectionRef":"#","url":"docs/xks/operator-guide/getting-started","content":"","keywords":""},{"title":"Bootstrap​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#bootstrap","content":""},{"title":"Add New Tenant​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#add-new-tenant","content":"When creating a new tenant there are a number of (for now manual) processes to perform. In this scenario we are assuming that you are using Azure DevOps and that you have already created a project and organization. In many places in the text we have provided names, they are just examples, all of these names can be exchanged to fit your needs. In this case let us call it project1. In the future we should manually import a repository.https://github.com/XenitAB/azure-devops-templates "},{"title":"Import azure-devops-templates pipeline​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#import-azure-devops-templates-pipeline","content":"To make sure that the azure-devops-templates repo is up to date we have an automatic CI that fetches updates from upstream to your local Azure DevOps clone. Go to pipelines -&gt; New pipeline -&gt; Azure Repos Git -&gt; azure-devops-templates -&gt; Existing Azure Pipelines YAML file Import the pipeline from the following path: /.ci/pipeline.yaml "},{"title":"Setup XKS​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#setup-xks","content":"In this case we will only setup a single XKS cluster in one environment, in our case dev. It is easy to add more environments when you have created your first one. At Xenit we are using Terraform modules that we share upstream To setup XKS we will utilize 4 modules: governance-globalgovernance-regionalcoreaks "},{"title":"Create Terraform repo​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#create-terraform-repo","content":"Of course we need a place to store our Terraform code so create one in your Azure DevOps organization. TODO create a example repo that uses our Terraform modules. You can today see a example of the Makefile. This is how we normally structure our tenant repo. ├── Makefile ├── README.md ├── aks │ ├── main.tf │ ├── outputs.tf │ ├── variables │ │ ├── common.tfvars │ │ ├── dev.tfvars │ │ ├── prod.tfvars │ │ └── qa.tfvars │ └── variables.tf ├── core │ ├── main.tf │ ├── outputs.tf │ ├── variables │ │ ├── common.tfvars │ │ ├── dev.tfvars │ │ ├── prod.tfvars │ │ └── qa.tfvars │ └── variables.tf ├── global.tfvars ├── governance │ ├── main.tf │ ├── outputs.tf │ ├── variables │ │ ├── common.tfvars │ │ ├── dev.tfvars │ │ ├── prod.tfvars │ │ └── qa.tfvars │ └── variables.tf Copy "},{"title":"Update repo​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#update-repo","content":"We need to update a number of settings in a number of places in your Terraform repo. Generate a SUFFIX that should be tfstate + a few random numbers, for example tfstate1234. Update the Makefile SUFFIX variable with the suffix and the random number. Also update global.tfvars with the same random number. "},{"title":"Create Terraform storage​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#create-terraform-storage","content":"In order to store a Terraform state we need to prepare that. We have written a small Go tool that will help out with that. Instead of running these scripts manually we will use the makefile. We use one Terrafrom state per DIR and ENV. Lets create the first Terraform state, in this case governance. make prepare ENV=dev DIR=governance You will need to run the prepare command for each separate Terraform folder. "},{"title":"Configure governance​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#configure-governance","content":"After defining the variables and you have have applied your config: make plan ENV=dev DIR=governance # If everything looks good make apply ENV=dev DIR=governance Copy "},{"title":"Configure core​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#configure-core","content":"Get a CIDR network for your AKS env per env. Define in core/variables/env.tfvars. "},{"title":"Configure AKS cluster​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#configure-aks-cluster","content":"How big should your cluster be?Which version of Kubernetes should you run?What DNS zone should you use?What SKU tier should your cluster have?What size should your k8s nodes have? All of this is configured under aks/variables/prod.tfvars. environment = &quot;prod&quot; dns_zone = &quot;prod.aks.xenit.io&quot; aks_config = { kubernetes_version = &quot;1.20.7&quot; sku_tier = &quot;Free&quot; default_node_pool = { orchestrator_version = &quot;1.20.7&quot; vm_size = &quot;Standard_D2as_v4&quot; min_count = 1 max_count = 1 node_labels = {} }, additional_node_pools = [ { name = &quot;standard2&quot; orchestrator_version = &quot;1.20.7&quot; vm_size = &quot;Standard_D2as_v4&quot; min_count = 1 max_count = 4 node_labels = {} node_taints = [] }, ] } Copy Notice the vm_size = Standard_D2as_v4 "},{"title":"GitOps using Flux​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#gitops-using-flux","content":"If you want Flux to manage your GitOps repo from the get go you can enable this in aks/variables/common.tfvars. In my case I will have Flux manage a namespace called monitor and sync a repo under monitor-gitops. You need to create the repository in Azure Devops that you link to before applying this Terraform. The repository can be empty. You will also need to create a separate repository for fleet-infra, this repo is used to store Flux config. This repo cannot be empty and needs a README file or something similar to work as intended before you run Terraform. In the example below we are using Azure DevOps as our CSM system, but we also support GitHub. If you want to use GitHub just fill in its config and make azure_devops empty instead. namespaces = [ { name = &quot;monitor&quot; delegate_resource_group = true labels = { &quot;terraform&quot; = &quot;true&quot; } flux = { enabled = true azure_devops = { org = &quot;organization1&quot; proj = &quot;project1&quot; repo = &quot;monitor-gitops&quot; } github = { repo = &quot;&quot; } } } ] Copy "},{"title":"Terraform CI/CD​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#terraform-cicd","content":"We have one CI/CD pipeline per terraform directory. You can find ready to use pipelines under .ci/ in the Terraform repo. "},{"title":"Configure service principal​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#configure-service-principal","content":"There are a few manual steps that you need to perform before we can start to configure the CI/CD pipeline. Service principal access​ Follow the instructions on how to create a SP. In some cases it might be useful to create a group where both a admin group for you as an admin the SP can use and assign the group the needed access. Depending on how your global.tfvars looks like it will be called something like: az-mg-lz-xks-owner. Create Service principal key​ The SP that we use is generated by Terraform but we do not store the key anywhere, so this is among the few times that we have to do something manual. First find the SP that you will use, this will depend on your Terraform config. There is no CLI command to create a new key so it is done through the portal. AAD -&gt; App registrations -&gt; All applications -&gt; search for the application -&gt; Certificates &amp; secrets -&gt; New client secret The key is only shown once, so copy it some where safe for long-term storage. This key will be used when creating the service connection in Azure DevOps. "},{"title":"Setup Service Connection​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#setup-service-connection","content":"To be able to talk from Azure DevOps to Azure and be able to run Terraform on push we need to configure service connections. Now you will also need the key that we created in the SP earlier. Get the config: # Service Principal Id APP_ID=$(az ad sp list --display-name sp-sub-xks-all-owner -o tsv --query '[].appId') # Tenant ID TENANT_ID=$(az account show -o tsv --query tenantId) # Subscription Id SUB_ID=$(az account show -o tsv --query id) Copy In Azure DevOps: Project settings -&gt; Service connections -&gt; New service connection -&gt; Azure Resource Manager -&gt; Service principal (manual) Subscription Id = $SUB_IDService Principal Id = $APP_IDService principal key = The key created in the earlier stepTenant ID = $TENANT_IDService connection name = xks-${environment}-owner "},{"title":"Update pipelines​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#update-pipelines","content":"Update the variable azureSubscriptionTemplate. You can find the value under Project settings -&gt; Service Connections  In my case sp-sub-project1-xks: name: $(Build.BuildId) variables: - name: azureSubscriptionTemplate value: &quot;sp-sub-project1-xks-{0}-owner&quot; - name: terraformFolder value: &quot;governance&quot; - name: sourceBranch value: &quot;refs/heads/main&quot; Copy Also update the project path in &quot;name&quot;. Also notice the ref, the ref points to which version of the module that you are using: resources: repositories: - repository: templates type: git name: project1/azure-devops-templates ref: refs/tags/2021.03.1 Copy "},{"title":"Add CI/CD pipelines​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#add-cicd-pipelines","content":"Once again add a pipeline. Assuming that you named your repository to Terraform Pipelines -&gt; New pipeline -&gt; Azure Repos Git -&gt; Terraform -&gt; Existing Azure Pipelines YAML file Import the pipeline from the following path: .ci/pipeline-governance.yaml Hopefully after adding the pipeline the pipeline should automatically trigger and the plan and apply stages should go through without any problems. "},{"title":"Create PAT secret​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#create-pat-secret","content":"To make it possible for flux to clone repos from azure devops we need to create a Personal Access Token(PAT). User Settings -&gt; Personal access tokens -&gt; New Token  Create a PAT  Copy the generated key, we will need it for the next step. "},{"title":"Add PAT to Azure Key Vaults​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#add-pat-to-azure-key-vaults","content":"To make it possible for terraform to reach the PAT in a easy and secure way we have chosen to store the PAT in Azure Key Vaults which you need to add manually. Azure CLI​ You can add the secret using the az CLI. Call the secret azure-devops-pat and the value should be the token you created in Azure DevOps. # List all key vaults az keyvault list # Create the secret az keyvault secret set --vault-name kv-dev-we-core-1234 --name azure-devops-pat --value &quot;SuperSecretToken123&quot; Copy Azure portal​ Or if you prefer use the UI. In the Azure portal search for &quot;Key vaults&quot; and pick the core one that matches the unique_suffix that you have specified in global.tfvars, in our case 1234. Key vaults -&gt; core-1234 -&gt; Secrets -&gt; Generate/Import  Call the secret azure-devops-pat and add the PAT key that you created in the previous step. "},{"title":"Admin and developer access​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#admin-and-developer-access","content":"Hopefully you should now have one XKS cluster up and running, but currently no developer can actually reach the cluster. In XKF we see clusters as cattle and at any time we can decide to recreate an XKS cluster. To be able to do this without our developers even knowing we use blue green clusters. TODO write a document on how blue green clusters works and link. We use GitOps together with DNS to be able to migrate applications without any impact to end-users assuming that our developers have written 12 step applications. To store state we utilize the cloud services available in the different clouds that XKF supports. To make sure that our developers do not notice when we change our the cluster we have written a Kubernetes API Proxy called azad-kube-proxy. "},{"title":"Azure AD Kubernetes Proxy​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#azure-ad-kubernetes-proxy","content":"AZAD as we also call it, is a deployment that runs inside XKS and sits in front of the Kubernetes API. We also supply a krew/kubectl plugin to make it easy for our developers to use AZAD. For instructions on how to setup and configure this see. AZAD Usage​ Install krew: https://krew.sigs.k8s.io/docs/user-guide/setup/install/#windowsInstall the azad-proxy plugin: kubectl krew install azad-proxyLogin with the Azure CLI (a valid session with azure cli is always required): az loginList all the available clusters: kubectl azad-proxy menu You can also use the discover function: kubectl azad-proxy discover kubectl azad-proxy generate --cluster-name dev-cluster --proxy-url https://dev.example.com --resource https://dev.example.com Copy "},{"title":"AAD groups​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#aad-groups","content":"To make it possible for our developers and admins to actually login to the cluster we need to add them to a AAD group. If you are a AAD guest user you need to add the AAD Role: Directory Reader to your user account. AZAD proxy parses the AAD and that is why the user needs Directory Reader. No subscription​ If you have not gotten any of the RG groups that XKF generates and perform az login you might see an error saying that you do not have any subscriptions. This is more likely if you are running XKF in AWS but also possible in Azure. Do as the error suggest and use the --allow-no-subscription flag. # The variable TENANT_ID = your tenant id az login $TENANT_ID --allow-no-subscription Copy AZAD proxy should still work. Developer groups​ Depending on what configuration you did in global.tfvars this will differ but the group name should be something like bellow. This group will give your developers contributor access in the namespaces where they have access. &lt;azure_ad_group_prefix&gt;-rg-xks-&lt;cluster-env&gt;-aks-contributor Example: az-rg-xks-dev-aks-contributor Admin groups​ To make it easy for you as a admin you should also use AZAD. To give yourself cluster-admin access: &lt;azure_ad_group_prefix&gt;-xks-&lt;cluster-env&gt;-clusteradmin Example: aks-xks-dev-clusteradmin Verify access​ There is a flag in Kubernetes called --as, which enables you to see if a specific user got access to a specific resource. Note this will not work if you are connecting to the cluster using AZAD-proxy due to it using the --as flag to run the commands for you. Since we are using OIDC we also need to provide the group id, you can find the group id in AAD. You can find the UUID of the group in AAD. kubectl get pods --as-group=12345678-1234-1234-1234-00000000000 --as=&quot;fake&quot; If you already have a rolebinding where a existing UUID exist you can run the following command: kubectl get pods --as-group=$(kubectl get rolebinding &lt;rolebiding-name&gt; -o jsonpath='{.subjects[0].name}') --as=&quot;fake&quot; "},{"title":"Authorized IPs​","type":1,"pageTitle":"Getting Started","url":"docs/xks/operator-guide/getting-started#authorized-ips","content":"To minimize the exposure of the XKS clusters we define a list of authorized IP:s that is approved to connect the Kubernetes cluster API. We need to approve multiple infrastructure networks and user networks. If you are using the HUB module and you are running VMSS Azure Devops Agent you need to approve those IP:s as authorized.The AKS public IPYour developers' public IP A recommendation is to add a comment with what IP you have added. aks_authorized_ips = [ &quot;8.8.8.8/32&quot;, # google dns &quot;1.2.3.4/32&quot;, # developer x ip &quot;2.3.4.5/30&quot;, # AKS0 dev ] "},{"title":"Overview","type":0,"sectionRef":"#","url":"docs/xks/operator-guide/index","content":"","keywords":""},{"title":"Architecture​","type":1,"pageTitle":"Overview","url":"docs/xks/operator-guide/index#architecture","content":"XKS is set up from a set of Terraform modules that when combined creates the full XKS service. There are multiple individual states that all fulfill their own purpose and build upon each other in a hierarchical manner. The first setup requires applying the Terraform in the correct order, but after that ordering should not matter. Separate states are used as it allows for a more flexible architecture that could be changed in parallel.  "},{"title":"Network diagram​","type":1,"pageTitle":"Overview","url":"docs/xks/operator-guide/index#network-diagram","content":"Looking at a cluster, the simple network diagram looks like this:  "},{"title":"Terraform modules​","type":1,"pageTitle":"Overview","url":"docs/xks/operator-guide/index#terraform-modules","content":"The following Terraform modules are used in XKS. "},{"title":"Governance​","type":1,"pageTitle":"Overview","url":"docs/xks/operator-guide/index#governance","content":"Governance is split into global and regional, it handles the creation and delegation of Azure Resource Groups, Azure KeyVaults, Azure AD groups, Service Principals and resources like that. "},{"title":"Core​","type":1,"pageTitle":"Overview","url":"docs/xks/operator-guide/index#core","content":"Core sets up the main network for an environment. "},{"title":"Hub​","type":1,"pageTitle":"Overview","url":"docs/xks/operator-guide/index#hub","content":"Hub is setup in the production subscription and is used for things like Azure Pipelines agents. "},{"title":"AKS​","type":1,"pageTitle":"Overview","url":"docs/xks/operator-guide/index#aks","content":"The AKS Terraform contains three modules that are used to setup a Kubernetes cluster. To allow for blue/green deployments of AKS clusters resources have to be split up into global resources that can be shared between the clusters, and cluster specific resources. The aks-global module contains the global resources like ACR, DNS and Azure AD configuration. The aks and aks-core modules create a AKS cluster and configures it. This cluster will have a suffix, normally a number to allow for temporarily creating multiple clusters when performing a blue/green deployment of the clusters. Namespaces will be created in the cluster for each of the configured tenants. Each namespaces is linked to a resource group in Azure where namespace resources are expected to be created.  "},{"title":"AKS","type":0,"sectionRef":"#","url":"docs/xks/operator-guide/kubernetes/aks","content":"","keywords":""},{"title":"System Node Pool​","type":1,"pageTitle":"AKS","url":"docs/xks/operator-guide/kubernetes/aks#system-node-pool","content":"AKS requires the configuration of a system node pool when creating a cluster. This system node pool is not like the other additional node pools. It is tightly coupled to the AKS cluster. It is not possible without manual intervention to change the instance type or taints on this node pool without recreating the cluster. Additionally the system node pool cannot scale down to zero, for AKS to work there has to be at least one instance present. This is because critical system pods like Tunnelfront and CoreDNS will by default run on the system node pool. For more information about AKS system node pool refer to the official documentation. XKS follows the Azure recommendation and runs only system critical applications on the system node pool. Doing this protects services like CoreDNS from starvation or memory issues caused by user applications running on the same nodes. This is achieved by adding the taint CriticalAddonsOnly to all of the system nodes. "},{"title":"Sizing Nodes​","type":1,"pageTitle":"AKS","url":"docs/xks/operator-guide/kubernetes/aks#sizing-nodes","content":"Smaller AKS clusters can survive with a single node as the load on the system applications will be moderately low. In larger clusters and production clusters it is recommended to run at least three system nodes that may be larger in size. This section aims to describe how to properly size the system nodes. The minimum requirement for a system node is a VM with at least 2 vCPUs and 4GB of memory. Burstable B series VMs are not recommended. A good starting point for all clusters are the D series node types which have a balance of CPU and memory resources. A good starting point is a node of type Standard_D2as_v4. More work has to be done in this area regarding sizing and scaling of the system node pools to achieve a standardized solution. "},{"title":"Modifying Nodes​","type":1,"pageTitle":"AKS","url":"docs/xks/operator-guide/kubernetes/aks#modifying-nodes","content":"There may come times when Terraform wants to recreate the AKS cluster when the system node pool has been updated. This happens when updating certain properties in the system node pool. It is still possible to do these updates without recreating the cluster, but it requires some manual intervention. AKS requires at least one system node pool but does not have an upper limit. This makes it possible to manually add a new temporary system node pool. Remove the existing default node pool created by Terraform. Create a new system node pool with the same name but with the updated parameters. Finally remove the temporary node pool. Terraform will just assume that the changes have already been applied and import the new state without any other complaints. Start off with creating a temporary system pool. Make sure to replace the cluster name and resource groups to the correct values. az aks nodepool add --cluster-name aks-dev-we-aks1 --resource-group rg-dev-we-aks --name temp --mode &quot;System&quot; --node-count 1 Copy It may not be possible to create a new node pool with the current Kubernetes version if the cluster has not been updated in a while. Azure will remove minor versions as new versions are released. In that case you will need to upgrade the cluster to the latest minor version before making changes to the system pool, as AKS will not allow a node with a newer version than the control plane. Delete the system node pool created by Terraform: az aks nodepool delete --cluster-name aks-dev-we-aks1 --resource-group rg-dev-we-aks --name default Copy Create a new node pool with the new configuration. In this case it is setting a new instance type and adding a taint: az aks nodepool add --cluster-name aks-dev-we-aks1 --resource-group rg-dev-we-aks --name default --mode &quot;System&quot; --zones 1 2 3 --node-vm-size &quot;Standard_D2as_v4&quot; --node-taints &quot;CriticalAddonsOnly=true:NoSchedule&quot; --node-count 1 Copy Delete the temporary pool: az aks nodepool delete --cluster-name aks-dev-we-aks1 --resource-group rg-dev-we-aks --name temp Copy For additional information about updating the system nodes refer to this blog post. "},{"title":"Update AKS cluster​","type":1,"pageTitle":"AKS","url":"docs/xks/operator-guide/kubernetes/aks#update-aks-cluster","content":""},{"title":"Useful commands in Kubernetes​","type":1,"pageTitle":"AKS","url":"docs/xks/operator-guide/kubernetes/aks#useful-commands-in-kubernetes","content":"When patching an AKS cluster or just upgrading nodes it can be useful to watch your resources in Kubernetes. # Show node version kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{&quot;\\t&quot;}{.metadata.labels.kubernetes\\.azure\\.com\\/node-image-version}{&quot;\\n&quot;}{end}' # Watch nodes watch kubectl get nodes # Check the status of all pods in the cluster kubectl get pods -A Copy "},{"title":"Terraform update Kubernetes version​","type":1,"pageTitle":"AKS","url":"docs/xks/operator-guide/kubernetes/aks#terraform-update-kubernetes-version","content":"TBD "},{"title":"CLI update Kubernetes version​","type":1,"pageTitle":"AKS","url":"docs/xks/operator-guide/kubernetes/aks#cli-update-kubernetes-version","content":"export RG=rg1 export POOL_NAME=default export CLUSTER_NAME=cluster1 export AZURE_LOCATION=westeurope Copy What AKS versions can I pick in this Azure location: az aks get-versions --location $AZURE_LOCATION -o table Copy az aks get-upgrades --resource-group $RG --name $CLUSTER_NAME --output table Copy "},{"title":"Upgrading node pools without upgrading cluster​","type":1,"pageTitle":"AKS","url":"docs/xks/operator-guide/kubernetes/aks#upgrading-node-pools-without-upgrading-cluster","content":"From time to time you might want to upgrade your Node Pools without upgrading the Kubernetes version. We always recommend to look at the official documentationas well. The node pool will spin up a new node and drain the existing one. When this is done the old node will be deleted. The below command works great for smaller clusters. If you want to upgrade more nodes faster it is possible to do so. Read the documentation for more information. export RG=rg1 export POOL_NAME=default export CLUSTER_NAME=cluster1 Copy Get the latest available node versions for your node pool: az aks nodepool get-upgrades --nodepool-name $POOL_NAME --cluster-name $CLUSTER_NAME --resource-group $RG Copy Upgrade the image on the specified node pool: az aks nodepool upgrade --resource-group $RG --cluster-name $CLUSTER_NAME --name $POOL_NAME --node-image-only Copy "},{"title":"Change vm size through Terraform​","type":1,"pageTitle":"AKS","url":"docs/xks/operator-guide/kubernetes/aks#change-vm-size-through-terraform","content":"If you want to use terraform to change the your node pools VM size you can't just change the vm_size in the additional_node_pools config. This will tell Azure to drain all the nodes and then delete the existing ones, then Azure will spin up a new node pool after the existing one is gone. This might be fine if you already have multiple additional node pools and you pods don't have specific node affinities. But if that isn't the case terraform will most likely run for ever since it won't be able to destroy the nodes that you already have workload on. Or even worse it will destroy the existing node and you won't have any node pools in your cluster to manage your workloads. Instead you have to add a second additional node pool in to your cluster. For example:  additional_node_pools = [ { name = &quot;standard&quot; orchestrator_version = &quot;1.21.2&quot; vm_size = &quot;Standard_E2s_v4&quot; min_count = 1 max_count = 5 node_labels = {} node_taints = [] spot_enabled = false spot_max_price = null }, { name = &quot;standard2&quot; orchestrator_version = &quot;1.21.2&quot; vm_size = &quot;Standard_F4s_v2&quot; min_count = 1 max_count = 5 node_labels = {} node_taints = [] spot_enabled = false spot_max_price = null } ] Copy Run terraform and see that standard2 is up and running. Now you can remove the standard node pool and standard2 should be able to handle the new load. Azure will automatically drain all the data from the old standard node pool. Remember to set min_count so that your current workload fits, you can always reduce min_count later. The cluster autoscaler will scale up new vm:s of standard2 but it will take time. During the creation of more standard2 nodes much of your workload might become pending. "},{"title":"AKS resources​","type":1,"pageTitle":"AKS","url":"docs/xks/operator-guide/kubernetes/aks#aks-resources","content":"To get a quick overview of what is happening in AKS you can look at its changelog. "},{"title":"EKS","type":0,"sectionRef":"#","url":"docs/xks/operator-guide/kubernetes/eks","content":"","keywords":""},{"title":"Differences​","type":1,"pageTitle":"EKS","url":"docs/xks/operator-guide/kubernetes/eks#differences","content":"To setup XKF using EKS you still need an Azure environment. XKF is heavily relying on Azure AD (AAD) and we have developed our own tool to manage access to our clusters called azad-kube-proxy. Our governance solution is still fully located in Azure together with our Terraform state. "},{"title":"Repo structure​","type":1,"pageTitle":"EKS","url":"docs/xks/operator-guide/kubernetes/eks#repo-structure","content":"This is how an AWS repo structure can look like: ├── Makefile ├── README.md ├── aws-core │ ├── main.tf │ ├── outputs.tf │ ├── variables │ │ ├── common.tfvars │ │ ├── dev.tfvars │ │ ├── prod.tfvars │ │ └── qa.tfvars │ └── variables.tf ├── aws-eks │ ├── main.tf │ ├── outputs.tf │ ├── variables │ │ ├── common.tfvars │ │ ├── dev.tfvars │ │ ├── prod.tfvars │ │ └── qa.tfvars │ └── variables.tf ├── azure-governance │ ├── main.tf │ ├── outputs.tf │ ├── variables │ │ ├── common.tfvars │ │ ├── dev.tfvars │ │ ├── prod.tfvars │ │ └── qa.tfvars │ └── variables.tf ├── global.tfvars Copy "},{"title":"EKS​","type":1,"pageTitle":"EKS","url":"docs/xks/operator-guide/kubernetes/eks#eks","content":"Just like in AKS we use Calico as our CNI. AWS CNI does not support network policiesAWS CNI heavily limits how many pods we can run on a single nodeWe want to be consistent with AKS Just after setting up the EKS cluster we use a null_resource to first delete the AWS CNI daemon set and then install calico. This is all done before we add a single node to the cluster. After this we add an EKS node group and Calico starts. "},{"title":"IRSA​","type":1,"pageTitle":"EKS","url":"docs/xks/operator-guide/kubernetes/eks#irsa","content":"In AKS we use AAD Pod Identity to support access to Azure resources. We support the same thing in EKS but use IAM roles for service accounts IRSA. To make it easier to use IRSA we have developed a small terraform module. "},{"title":"Bootstrap​","type":1,"pageTitle":"EKS","url":"docs/xks/operator-guide/kubernetes/eks#bootstrap","content":"By default AWS CNI limits the amount of pods that you can have on a single node. Since we are using Calico we do not have this limit, but when setting up a default EKS environment the EKS bootstrap scriptdefines a pod limit. To remove this limit we have created our own AWS launch template for our EKS node group. It sets --use-max-pods false and some needed Kubernetes node labels. If these labels are not set the EKS cluster is unable to &quot;find&quot; the nodes in the node group. "},{"title":"Tenants account peering​","type":1,"pageTitle":"EKS","url":"docs/xks/operator-guide/kubernetes/eks#tenants-account-peering","content":"In Azure we separates XKF and our tenants by using Resource Groups, in AWS we use separate accounts. To setup a VPC peering you need to know the target VPC id, this creates a chicken and egg problem. To workaround this problem we sadly have to run the eks/core module multiple times on both the XKF side and the tenant side. Run Terraform in the following order: XKF core without any vpc_peering_config_requester defined.Tenant core without any vpc_peering_config_accepter defined.XKF core defines vpc_peering_config_requester, manually getting the needed information from the tenant account.Tenant core defines vpc_peering_config_accepter, manually getting the needed information from the XKF account. Make sure that you only have one peering request open at the same time, else the accepter side will not be able to find a unique request. Now you should be able to see the VPC peering connected on both sides. "},{"title":"Update cluster version​","type":1,"pageTitle":"EKS","url":"docs/xks/operator-guide/kubernetes/eks#update-cluster-version","content":"Updating the EKS cluster version can not be done by updating Terraform code only, it also involves the AWS CLI and kubectl. Find your EKS version to upgrade to here: EKS versions For further information on the AWS CLI commands used in this section, please refer to the AWS EKS CLI documentation. "},{"title":"Update the control plane using AWS CLI​","type":1,"pageTitle":"EKS","url":"docs/xks/operator-guide/kubernetes/eks#update-the-control-plane-using-aws-cli","content":"Get the name of the cluster to update: aws eks list-clusters --region eu-west-1 Copy Update the control plane version by running the following command: aws eks update-cluster-version --region eu-west-1 --name &lt;cluster-name&gt; --kubernetes-version &lt;version&gt; Copy The above command provides an id that can be use to check the status of the update: aws eks describe-update --region eu-west-1 --name &lt;cluster-name&gt; --update-id &lt;id&gt; Copy The update is finished when status is Successful. Previous updates have taken approximately 45 minutes. In the aws-eks/variables/&lt;environment&gt;.tfvars Terraform file that corresponds to the actual environment, update the kubernetes_version in eks_config and make a terraform plan. No difference in the plan output is expected. Also perform a terraform apply just to make sure state the state is updated (might not be needed). "},{"title":"Update the control plane using Terrafrom​","type":1,"pageTitle":"EKS","url":"docs/xks/operator-guide/kubernetes/eks#update-the-control-plane-using-terrafrom","content":"TBD "},{"title":"Update the addons​","type":1,"pageTitle":"EKS","url":"docs/xks/operator-guide/kubernetes/eks#update-the-addons","content":"The addons to be updated are coredns and kube-proxy. Can be verified with: aws eks list-addons --region eu-west-1 --cluster-name &lt;cluster-name&gt; Copy The current addon version can be found by: aws eks describe-addon --region eu-west-1 --cluster-name &lt;cluster-name&gt; --addon-name &lt;addon-name&gt; Copy The version to update to can be found by: aws eks describe-addon-versions --region eu-west-1 --kubernetes-version &lt;version&gt; --addon-name &lt;addon-name&gt; Copy The addons are updated with: aws eks update-addon --region eu-west-1 --cluster-name &lt;cluster-name&gt; --addon-name &lt;name&gt; --addon-version &lt;version&gt; --resolve-conflicts OVERWRITE Copy Check that the status of new addon vestion is ACTIVE: aws eks describe-addon --region eu-west-1 --cluster-name &lt;cluster-name&gt; --addon-name &lt;name&gt; Copy The corresponding Pods can be checked by: kubectl get pods -n kube-system Copy Also perform a health check: https://ingress-healthz.&lt;environment&gt;.&lt;customer&gt;.se/ Copy "},{"title":"Update the nodes​","type":1,"pageTitle":"EKS","url":"docs/xks/operator-guide/kubernetes/eks#update-the-nodes","content":"In the aws-eks/variables/&lt;environment&gt;.tfvars Terraform file that corresponds to the actual environment, add a new node group in eks_config. The example below shows a node upgrade from 1.20 to 1.21 where standard2 is the new node group. The value of release_version must match an AMI version (preferrably the latest) for the actual Kubernetes version (can be found here): eks_config = { kubernetes_version = &quot;1.21&quot; cidr_block = &quot;10.100.64.0/18&quot; node_groups = [ { name = &quot;standard&quot; release_version = &quot;1.20.4-20210621&quot; min_size = 3 max_size = 4 instance_types = [&quot;t3.large&quot;] }, { name = &quot;standard2&quot; release_version = &quot;1.21.5-20220123&quot; min_size = 3 max_size = 4 instance_types = [&quot;t3.large&quot;] }, ] } Copy When this change is applied, there will be a new set of nodes running the new version added to the cluster. The following command will show all nodes and their versions: kubectl get nodes Copy Now it is time to drain the old nodes one by one with: kubectl drain &lt;node-name&gt; --ignore-daemonsets --delete-local-data Copy When all nodes are drained, remove the old node group in eks_config. From the example above: eks_config = { kubernetes_version = &quot;1.21&quot; cidr_block = &quot;10.100.64.0/18&quot; node_groups = [ { name = &quot;standard2&quot; release_version = &quot;1.21.5-20220123&quot; min_size = 3 max_size = 4 instance_types = [&quot;t3.large&quot;] }, ] } Copy When applied, the old nodes are removed. The update is now complete. "},{"title":"Command examples​","type":1,"pageTitle":"EKS","url":"docs/xks/operator-guide/kubernetes/eks#command-examples","content":"The following AWS CLI commands are an example of an update from 1.20 to 1.21: Control plane: aws eks list-clusters --region eu-west-1 aws eks update-cluster-version --region eu-west-1 --name qa-eks2 --kubernetes-version 1.21 aws eks describe-update --region eu-west-1 --name qa-eks2 --update-id 25b9f04f-0be3-40ca-bc37-aaf841070012 Copy Addons: aws eks describe-addon-versions --kubernetes-version 1.21 --addon-name coredns aws eks update-addon --cluster-name qa-eks2 --addon-name coredns --addon-version v1.8.4-eksbuild.1 --resolve-conflicts OVERWRITE aws eks describe-addon --region eu-west-1 --cluster-name qa-eks2 --addon-name coredns aws eks describe-addon-versions --kubernetes-version 1.21 --addon-name kube-proxy aws eks update-addon --cluster-name qa-eks2 --addon-name kube-proxy --addon-version v1.21.2-eksbuild.2 --resolve-conflicts OVERWRITE aws eks describe-addon --region eu-west-1 --cluster-name qa-eks2 --addon-name kube-proxy Copy "},{"title":"Break glass​","type":1,"pageTitle":"EKS","url":"docs/xks/operator-guide/kubernetes/eks#break-glass","content":"We are very dependent on azad-proxy to work but if something happens with the ingress, azad-proxy or the AAD we need to have ways of reaching the cluster: aws eks --region eu-west-1 update-kubeconfig --name dev-eks1 --alias dev-eks1 --role-arn arn:aws:iam::111111111111:role/xkf-eu-west-1-dev-eks-admin Copy "},{"title":"EKS resources​","type":1,"pageTitle":"EKS","url":"docs/xks/operator-guide/kubernetes/eks#eks-resources","content":"To get a quick overview of what is happening in EKS you can look at its changelog. When upgrading node groups you need to correlate with your Kubernetes release, you can find which node group is available to which node group. "},{"title":"Networking","type":0,"sectionRef":"#","url":"docs/xks/operator-guide/networking","content":"","keywords":""},{"title":"Kubernetes​","type":1,"pageTitle":"Networking","url":"docs/xks/operator-guide/networking#kubernetes","content":"TBD "},{"title":"Azure​","type":1,"pageTitle":"Networking","url":"docs/xks/operator-guide/networking#azure","content":"XKS in Azure uses a single VNET with a single subnet per AKS cluster. The VNET and subnets are created by the core module. Additionally each AKS cluster also creates a load balancer. The load balancer is used for both ingress and egress traffic. When a Kubernetes service of type LoadBalancer is created a new IP is attached to the load balancer. An Azure load balancer can have multiple IPs attached to it so unlike AWS it does not have to create a new load balancer. During the creation of the AKS cluster a public IP prefix is attached to the load balancer for egress traffic. This ensures that all traffic egress with the same source IP, enabling the use of IP white listing in external sources. This does however mean that all outbound traffic will also go through the same load balancer as the incoming traffic. There is currently work underway to enable the use of managed NAT gateways for egress traffic in AKS, but it is currently in preview right now. "},{"title":"SNAT Exhaustion​","type":1,"pageTitle":"Networking","url":"docs/xks/operator-guide/networking#snat-exhaustion","content":"Applications making large numbers of outgoing TCP or UDP connections to the same IP and port can cause an issue known as SNAT port exhaustion. This is mostly due to the network architecture in Azure and AKS. All of the outgoing traffic from AKS goes through the load balancer, and for each outgoing request the load balancer needs to allocate an SNAT port to receive the response. Each Azure load balancer will allocate 64000 SNAT ports. This may seem like a lot, but there is a caveat as AKS will limit the amount of SNAT ports per node. The amount of SNAT ports available per node depends on the amount of nodes per cluster. Node Count\tSNAT Ports per Node1-50\t1024 51-100\t512 101-200\t256 201-400\t128 401-800\t64 801-1000\t32 A symptom of exhausting the SNAT ports is that outgoing requests will just fail. This is of course not a good situation, and may be hard to debug as a failing request could be caused by many different factors. Links​ https://docs.microsoft.com/en-us/azure/aks/load-balancer-standard#troubleshooting-snathttps://docs.microsoft.com/en-us/azure/load-balancer/troubleshoot-outbound-connectionhttps://www.danielstechblog.io/detecting-snat-port-exhaustion-on-azure-kubernetes-service/https://medium.com/asos-techblog/an-aks-performance-journey-part-1-sizing-everything-up-ee6d2346ea99https://medium.com/asos-techblog/an-aks-performance-journey-part-2-networking-it-out-e253f5bb4f69 "},{"title":"AWS​","type":1,"pageTitle":"Networking","url":"docs/xks/operator-guide/networking#aws","content":"TBD "}]