<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>http://xenitab.github.io/blog</id>
    <title>Xenit Blog</title>
    <updated>2022-04-12T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="http://xenitab.github.io/blog"/>
    <subtitle>Xenit Blog</subtitle>
    <icon>http://xenitab.github.io/img/favicon.ico</icon>
    <entry>
        <title type="html"><![CDATA[Kubernetes Ephemeral Containers Security]]></title>
        <id>Kubernetes Ephemeral Containers Security</id>
        <link href="http://xenitab.github.io/blog/2022/04/12/ephemeral-containers"/>
        <updated>2022-04-12T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Ephemeral containers is a new concept in Kubernetes which allows attaching  containers to already running Pods. It also introduces new security concerns which have to be resolved before it can be enabled.
]]></summary>
        <content type="html"><![CDATA[<p>Attempting to debug a Pod and realizing that you can&#x27;t install curl due to security settings has to be a meme at this point. Good security practices are always nice but it often comes at the cost of usability. To the point where some may even solve this problem by installing debug tools into their production images. Shudders.</p><img src="https://i.imgflip.com/6cczqi.jpg" title="made at imgflip.com"/><p>Kubernetes has introduced a new concept called <a href="https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/">ephemeral containers</a> to deal with this problem. Ephemeral containers are temporary containers that can be attached after a Pod has been created. Rejoice! We can now attach a temporary container with all the tools which we desire. While the applications container may have &quot;annoying security features&quot; like a read only file system the ephemeral container can enjoy all the freedom which writing files entails. I love this feature so I need to upgrade my cluster immediately!</p><h2>Digging Deeper</h2><p>Now that we have the new feature we can start a ephemeral container in any Pod we like.</p><pre><code class="language-shell">kubectl run ephemeral-demo --image=k8s.gcr.io/pause:3.1 --restart=Never
kubectl debug -it ephemeral-demo --image=busybox:1.28
</code></pre><p>We get a shell and life is now much simpler, but wait a minute. This post is not about how to use ephemeral containers, there are enough of those already, but rather the security implications of enabling ephemeral containers. Let&#x27;s have a look at the YAML for the Pod that we created the ephemeral container in.</p><pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: ephemeral-demo
spec:
  ...
  ephemeralContainers:
  - name: debugger-r59b7
    image: busybox:1.28
    imagePullPolicy: IfNotPresent
    stdin: true
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    tty: true
</code></pre><p>Interesting, there is a new field called <code>ephemeralContainers</code> in the Pod definition. This new field contains a list of containers similar to <code>initContainers</code> and <code>containers</code>. It is not identical as there are certain options which are not available, refer to the <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#ephemeralcontainer-v1-core">API documentation</a> for more information. It does however allow configuration of the container security context, which could in theory allow a bad actor to escalate the container&#x27;s privileges. This should not affect those of us who use a policy enforcement tool right? The answer is yes and no depending on the tool and version that is being used. It also depends on if you are using policies from the project&#x27;s library or policies developed in house.</p><h3>OPA Gatekeeper</h3><p><a href="https://github.com/open-policy-agent/gatekeeper">OPA Gatekeeper</a> does not require any code changes as all of its policies are written in <a href="https://www.openpolicyagent.org/docs/latest/policy-language/">rego</a>. It&#x27;s sub project <a href="https://github.com/open-policy-agent/gatekeeper-library/">Gateekper Library</a> does however have to be updated. The library contains an implementation of the common Pod Security Policies. This includes policies like not allowing containers in privileged mode. The issue with the all of the policies is that they currently only check containers specified in <code>initContainers</code> and <code>containers</code>, analyze the <a href="https://github.com/open-policy-agent/gatekeeper-library/blob/275a1628694dcdf9daf5f6dda1373de6af78e7da/library/pod-security-policy/privileged-containers/template.yaml#L49-L55">following</a> rego as an example.</p><p>The good news is that this is a pretty easy fix, the bad news is that it requires end users to update the policies pulled from the library.</p><h3>Kyverno</h3><p><a href="https://github.com/kyverno/kyverno">Kyverno</a> seems to have resolved the issues faster. Compared to OPA Gatekeeper however it did require a small code change which means that version <a href="https://github.com/kyverno/kyverno/releases/tag/v1.5.3">1.5.3</a> or later is needed to write policies for ephemeral containers. They have also <a href="https://github.com/kyverno/policies/pull/241">updated their policy library</a> to include checking ephemeral containers. Kyverno has done a great job solving these issues quickly. It does still require end users to update however.</p><h3>Pod Security Policies</h3><p><a href="https://kubernetes.io/docs/concepts/security/pod-security-policy/">Pod Security Policies</a> used to be the default policy tool for Kubernetes, and a lot of projects have rules based on Pod Security Policies (PSP). However if you are relying on PSP in a modern cluster you should really start looking for other options like OPA Gatekeeper or Kyverno. PSP has been deprecated since Kubernetes v1.21 and will be removed in v1.25.</p><p>If PSP is your only policy tool and you are planning to upgrade to v1.23, don&#x27;t. As PSP is deprecated no new features have been added, and that includes policy enforcement on ephemeral containers. Which means that any security context in an ephemeral container is allowed no matter the PSP in the cluster.  The PSP below will have no affect when adding an ephemeral container to a Pod which is privileged.</p><pre><code class="language-yaml">apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: default
spec:
  privileged: false
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  runAsUser:
    rule: RunAsAny
  fsGroup:
    rule: RunAsAny
  volumes:
  - &#x27;*&#x27;
</code></pre><h3>RBAC</h3><p>Disallowing ephemeral containers with RBAC could be an option if the feature is not needed and it is not possible to disable the feature completely. The <a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/277-ephemeral-containers/README.md">KEP-277: Ephemeral Containers</a> state the following about using RBAC to disable ephemeral containers.</p><blockquote><p>Cluster administrators will be expected to choose from one of the following mechanisms for restricting usage of ephemeral containers:</p><ul><li>Use RBAC to control which users are allowed to access the /ephemeralcontainers subresource.</li><li>Write or use a third-party admission controller to allow or reject Pod updates that modify ephemeral containers based on the content of the update.</li><li>Disable the feature using the EphemeralContainers feature gate.</li></ul></blockquote><p>RBAC is additive which means that it is not possible to remove permissions from a role. This type of mitigation obviously does not matter if all users a cluster admin, which they should not be, so we assume that new roles are created for the cluster consumers. In this case having a look at the existing roles can be enough to make sure that the subresource <code>/ephemeralcontainers</code> is not included in the role.</p><pre><code class="language-yaml">apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: edit
rules:
- apiGroups:
  - &quot;&quot;
  resources:
  - pods
  - pods/attach
  - pods/exec
  - pods/portforward
  - pods/proxy
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
</code></pre><h2>Checking Policy Enforcement</h2><p>Let&#x27;s say that you upgraded your cluster and informed all end users of the great new feature. How do you know that the correct policies are enforced in accordance to your security practices. You may have been aware of the API changes and taken the correct precautionary steps. Or you just updated Kyverno and it&#x27;s policies out of pure happenstance. Either way it is good to trust but verify that it is not for example possible to create a privileged ephemeral container. Annoyingly the debug command does not expose any options to set any security context configuration, so we need another option. Ephemeral containers cannot be defined in a Pod when it is created and it can neither be added with an update. We need some other method to create these specific ephemeral containers.</p><blockquote><p>Ephemeral containers are created using a special ephemeralcontainers handler in the API rather than by adding them directly to pod.spec, so it&#x27;s not possible to add an ephemeral container using <code>kubectl edit</code>.</p></blockquote><p>The simplest method to add an ephemeral container with a security context to a Pod is to use the Go client. A couple of lines of code can add a new ephemeral container running as privileged or use any other security context setting which is to your liking.</p><pre><code class="language-go">package main

import (
    &quot;context&quot;
    &quot;fmt&quot;
    &quot;os&quot;

    corev1 &quot;k8s.io/api/core/v1&quot;
    metav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;
    &quot;k8s.io/client-go/kubernetes&quot;
    &quot;k8s.io/client-go/tools/clientcmd&quot;
)

func main() {
    if len(os.Args) != 4 {
        panic(&quot;expected three args&quot;)
    }
    podNamespace := os.Args[1]
    podName := os.Args[2]
    kubeconfigPath := os.Args[3]

    // Create the client
    client, err := getKubernetesClients(kubeconfigPath)
    if err != nil {
    panic(fmt.Errorf(&quot;could not create client: %w&quot;, err))
    }
    ctx := context.Background()

    // Get the Pod
    pod, err := client.CoreV1().Pods(podNamespace).Get(ctx, podName, metav1.GetOptions{})
    if err != nil {
    panic(fmt.Errorf(&quot;could not get pod: %w&quot;, err))
    }

    // Add a new ephemeral container
    trueValue := true
    ephemeralContainer := corev1.EphemeralContainer{
        EphemeralContainerCommon: corev1.EphemeralContainerCommon{
            Name:  &quot;debug&quot;,
            Image: &quot;busybox&quot;,
            TTY:   true,
            SecurityContext: &amp;corev1.SecurityContext{
                Privileged:               &amp;trueValue,
                AllowPrivilegeEscalation: &amp;trueValue,
            },
        },
    }
    pod.Spec.EphemeralContainers = append(pod.Spec.EphemeralContainers, ephemeralContainer)
    pod, err = client.CoreV1().Pods(pod.Namespace).UpdateEphemeralContainers(ctx, pod.Name, pod, metav1.UpdateOptions{})
    if err != nil {
    panic(fmt.Errorf(&quot;could not add ephemeral container: %w&quot;, err))
    }
}

func getKubernetesClients(path string) (kubernetes.Interface, error) {
    cfg, err := clientcmd.BuildConfigFromFlags(&quot;&quot;, path)
    if err != nil {
        return nil, err
    }
    client, err := kubernetes.NewForConfig(cfg)
    if err != nil {
        return nil, err
    }
    return client, nil
}
</code></pre><p>Run the program and pass the namespace, pod name, and path to a kube config file. We assume that the ephemeral-demo Pod is still running. </p><pre><code class="language-shell">go run main.go default ephemeral-demo $KUBECONFIG
</code></pre><p>If it completes with no error a privileged ephemeral container should have been added to the Pod. Exec into it and list the host&#x27;s devices to prove that it is a privileged container.</p><pre><code class="language-shell">kubectl exec -it ephemeral-demo -c debug -- sh
ls /dev
</code></pre><h2>Conclusion</h2><p>If there is one takeaway from this post, it is that any policy tool that has not been updated in the last couple of months will not enforce rules on ephemeral containers. This also includes all policies written in house! It is not enough to update the community policies.</p><p>Some may argue that this type of oversight is not really an issue. Ephemeral containers can&#x27;t mount <a href="https://kubernetes.io/docs/concepts/storage/volumes/#hostpath">host paths</a>, or access the <a href="https://kubernetes.io/docs/concepts/security/pod-security-policy/#host-namespaces">hosts namespaces</a>. All it can do is set the common container security context. That is a fair comment, because it&#x27;s true. Being able to create a privileged container is however still not ideal, and there are <a href="https://bishopfox.com/blog/kubernetes-pod-privilege-escalation#Pod3">methods to escalate privileges</a> when this is possible. Either way it is important to be aware of how policies are enforced and the security contexts which are allowed.</p><p>I am still not sure how much of an issue this will be short term. Cloud providers are currently in the process of rolling out Kubernetes v1.23 in their SaaS offerings. In these solutions it is still a possibility that they chose to disable ephemeral containers. Those rolling their own clusters may have already upgraded to v1.23 and not be aware of the new feature. That is the biggest issue really, that the platform administrator has to be aware of the existence of ephemeral containers. The fact that kubectl does not expose the option to set a security context will make even less people aware that it is still possible to set one with other means. Investing in a security audit 6 months ago will only be valuable as long as the same Kubernetes version is used. Kubernetes is by design <strong>not</strong> secure by default, so each new feature introduced has to be analyzed. The fact that upgrading from Kubernetes v1.22 to v.23 could make your cluster less secure is part of the difficulties of working with Kubernetes, requiring platform administrators to always stay on top of things. The reality is that these types of things are easy to miss, so hopefully this post has helped someone make their cluster a bit more secure.</p>]]></content>
        <author>
            <name>Philip Laine</name>
            <uri>https://github.com/phillebaba/</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Twelve-factor app anno 2022]]></title>
        <id>Twelve-factor app anno 2022</id>
        <link href="http://xenitab.github.io/blog/2022/02/23/12factor"/>
        <updated>2022-02-23T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The Twelve-factor app methodology turns 10. This blog posts re-evaluates  the original factors against a decade of experience with  software-as-a-service development and the maturing of serverless  development.
]]></summary>
        <content type="html"><![CDATA[<p><a href="https://12factor.net/">The Twelve-factor app</a> is a methodology for building software-as-a-service apps that was first formulated by developers associated with Heroku. It&#x27;s been ten years since the first presentation of this methodology. Despite the criticism that it is only applicable to Heroku and similar webapp services, it remains a relevant yard stick for software-as-a-service development. Some of its tenets have been incorporated into Docker and thence into OCI, effectively making them the law of container-land. This blog post looks at each of the twelve factors and tries to evaluate whether they remain relevant or whether they need updating.</p><p>In one respect, the criticism of being Heroku-centric is relevant. Heroku (and Google App Engine and similar services) offer a single packaging model which we today might refer to as &quot;IaC-less&quot;: you provide an HTTP server and Heroku runs it for you (or a war file in Google App Engine&#x27;s case). Any non-trivial software-as-a-service offering require composing many apps into a service, where each has a distinct role: authentication, caching, API, serving static files, et.c., with some infrastructure-as-code that describes how these apps are exposed and interconnect. We end up with an &quot;app&quot; level and a &quot;service&quot; level and we have to be careful when considering the original text, since it talks only about the app level, but some of its concerns now reside at the service level.</p><h2>Factor I: Codebase</h2><blockquote><p>A codebase is any single repository. <!-- -->[...]<!-- --> The codebase is the same across all deploys, although different versions may be active in each deploy.</p></blockquote><p>This factor is first and foremost an argument for expressing as much as possible of your service as code that can be put under version control. This was probably already a strawman attack ten years ago. Nevertheless, the latest incarnation of mandating version control is <a href="https://www.weave.works/technologies/gitops/">GitOps</a>, namely the idea that your infrastructure maintains itself by reading your version control system and automatically applies changes as necessary - remarkably similar to the original Heroku model.</p><blockquote><p>There is always a one-to-one correlation between the codebase and the app. If there are multiple codebases, it’s not an app – it’s a distributed system. Each component in a distributed system is an app, and each can individually comply with twelve-factor.</p></blockquote><p>This part of the factor remains relevant at the app level, but modern public cloud providers&#x27; infrastructure-as-code tooling and platforms like Kubernetes allow us to describe a set of apps and their supporting resources (e.g. secrets) as one package or service. Some organizations separate infrastructure-as-code and app code into separate repositories while others keep the app and its supporting IaC together; neither of these can be said unconditionally to be best practice. Similarly, <a href="https://en.wikipedia.org/wiki/Single-page_application">Single-page apps</a> are often deployed to a <a href="https://www.cloudflare.com/learning/cdn/what-is-a-cdn/">Content Distribution Network</a>, while their backend may be deployed to a public cloud provider or to a <a href="https://kubernetes.io/">Kubernetes</a> cluster. Whether these should be kept in the same repository or in different repositories depends on how tightly coupled they are.</p><p>The 2022 developer considers the relationship between repositories and artifacts carefully. Pertinent aspects include:</p><ul><li>branching strategy</li><li>continuous integration completeness and run times</li><li>continuous delivery pipelines</li><li>infrastructure-as-code maintainability</li><li>configuration management</li><li>automated deployments</li></ul><p>Expect to reorganize your sources as your apps evolve.</p><h2>Factor II: Dependencies</h2><blockquote><p>A twelve-factor app never relies on implicit existence of system-wide packages. It declares all dependencies, completely and exactly, via a dependency declaration manifest. <!-- -->[...]<!-- --> Twelve-factor apps also do not rely on the implicit existence of any system tools.</p></blockquote><p>In the container and function-as-a-service worlds, this factor has been elevated to fact. These execution environments provide virtually no implicit dependencies.</p><p>Modern apps tend to have more than one dependency declaration manifest, namely its project manifest(s) (e.g. <code>go.mod</code> or <code>package.json</code>) and a <code>Dockerfile</code>. A consequence of this factor is that you should use Docker base images that are as bare-bone as they come, forcing the explicit installation of supporting libs and tools.</p><p>The up-to-date interpretation of this factor is that upgrading dependency versions should always be a conscious action. This slightly shifts the interpretation of the original factor&#x27;s &quot;exactly&quot;. The various ecosystems and tool chains (maven, npm, cargo, et.c.) work differently in when they resolve dependencies. Some resolve dependencies when the developer performs a build and some require an explicit &quot;upgrade&quot; operation to change what goes into a build. It is therefore vital to have a codified workflow for updating dependencies. For example, when using Node.js and npm, a developer should normally do <a href="https://blog.npmjs.org/post/171556855892/introducing-npm-ci-for-faster-more-reliable">npm ci</a> and only use the traditional <code>npm install</code> (or <code>npm update</code>) when the intent is to modernize dependencies.</p><blockquote><p>it uses a dependency isolation tool during execution to ensure that no implicit dependencies “leak in” from the surrounding system. The full and explicit dependency specification is applied uniformly to both production and development.</p></blockquote><p>One of the innovations introduced by Docker is that this factor is enforced already at build time, making it easy to ensure uniformity across dev and prod. Run your automated tests with the built container and there is very little space for execution environment differences.</p><h2>Factor III: Config</h2><blockquote><p>An app’s config is everything that is likely to vary between deploys (staging, production, developer environments, etc). The twelve-factor app stores config in environment variables. Env vars are easy to change between deploys without changing any code; unlike config files, there is little chance of them being checked into the code repo accidentally; and unlike custom config files, <!-- -->[...]<!-- --> they are a language- and OS-agnostic standard. <!-- -->[...]<!-- --> A litmus test for whether an app has all config correctly factored out of the code is whether the codebase could be made open source at any moment, without compromising any credentials.</p></blockquote><p>This factor remains mostly relevant as written, but there are some nuances to consider.</p><p>Infrastructure-as-code tools like Terraform allows us to create files and database entries. Kubernetes allows us to create <a href="https://kubernetes.io/docs/concepts/configuration/configmap/">ConfigMaps</a> which will be made available to a container as a file. In both cases, the source configuration can be put under version control, any manual edits will be overwritten on the next deploy and the mechanism is easy for a developer to emulate locally. Thus, they achieve the same result as using environment variables by different means.</p><p>Also, while environment variables are operations-friendly, they are problematic when writing tests, since they are global state. In ecosystems that default to parallel test execution (e.g. Rust) environment variables cannot be used. Thus, while an environment variable remains the preferred way to accept simple configuration values, a 12-factor app should convert them to internal state as early as possible.</p><p>This factor is now obsolete in one respect. As much as possible, secrets (passwords, private keys, et c) should be stored using a secrets management system such as Hashicorp Vault or Azure Key Vault. Particularly where we can rely on the infrastructure to authenticate the calling process (e.g. via a Kubernetes service account) access to secrets will not directly require credentials. The existence of the secret is under version control, but the actual secret content is immaterial.</p><p>Furthermore, with platforms such as Kubernetes, service discovery means that some aspects need no configuration at all. Additionally, some forms of configuration can better be managed as references between IaC-controlled resources, which removes them from direct configuration management consideration.</p><h2>Factor IV: Backing services</h2><blockquote><p>A backing service is any service the app consumes over the network as part of its normal operation. The code for a twelve-factor app makes no distinction between local and third party services. To the app, both are attached resources, accessed via a URL or other locator/credentials stored in the config. <!-- -->[...]<!-- --> Resources can be attached to and detached from deploys at will.</p></blockquote><p>This factor remains relevant as written. Its current iteration is sometimes referred to as <a href="https://swagger.io/resources/articles/adopting-an-api-first-approach/">&quot;API first&quot;</a> which can be described as the principle that all services you create should be able to act as a backing service. More generally, with the advent of <a href="https://www.crowdstrike.com/cybersecurity-101/zero-trust-security/">Zero Trust</a> and the proliferation of cloud services, the logical end result of this factor is that any service can interact with any other service on the planet.</p><p>Even your orchestration platform itself is a backing service, not just the services that run inside it. A service can leverage the Kubernetes control plane to run a one-off job or provision cloud resources to serve a new customer.</p><p>The original text focuses a lot on relational databases. It is worth pointing out that you can create a service which scalably serves long-lived state without violating the 12 factors: as long as there is a procedure to claim or negotiate access to a particular shard of the state (e.g. backups stored in an Azure storage account), the actual process can remain stateless. Contemporary thinking in this matter is still coloured by software that predates the cloud era (e.g. MySQL, Elasticsearch, RabbitMQ). For an example of what is possible in 2022, we can look at <a href="https://github.com/grafana/loki">Loki</a>.</p><h2>Factor V: Build, release, run</h2><blockquote><p>The twelve-factor app uses strict separation between the build, release, and run stages. The build stage is a transform which converts a code repo into an executable bundle known as a build.</p></blockquote><p>This factor is more or less a prerequisite for developing software-as-a-service in 2022, but we need to complement this factor with a requirement for automating these stages. The maturing of CI/CD software-as-a-service providers such as GitHub, <a href="https://docs.microsoft.com/en-us/azure/container-registry/container-registry-tasks-overview">ACR Tasks</a> and <a href="https://circleci.com/">Circle CI</a> means that it is now relatively easy to automate this process.</p><p>Typically, the build stage will push a container image to some container registry or upload a function-as-a-service zip files to cloud storage.</p><blockquote><p>The release stage takes the build produced by the build stage and combines it with the deploy’s current config</p></blockquote><p>The normal practice today is for a pipeline to push the release to the runtime environment. This is very useful early in the lifecycle of an app since the release process typically evolves with the app. To achieve stronger separation between the build and release, you might want to consider going GitOps. In Kubernetes-land, you would use a service such as <a href="https://fluxcd.io">Flux</a>.</p><h2>Factor VI: Processes</h2><blockquote><p>Twelve-factor processes are stateless and share-nothing. Any data that needs to persist must be stored in a stateful backing service.</p></blockquote><p>This factor remains relevant as written. In the world of REST APIs, this effectively means that we should hold no domain state in memory between HTTP requests - it should always be handed over to a caching service. This is the main enabler for scale-out in a software-as-a-service.</p><p>Adhering to this rule is also a good way to avoid memory leaks, which tend to plague garbage-collected ecosystems such as Java, Node, Python and Ruby. You will still get leaks after you out-source your caching to Redis, but it will be much easier to measure and the incitement to properly architecture the caching is stronger.</p><h2>Factor VII: Port binding</h2><blockquote><p>The twelve-factor app is completely self-contained and does not rely on runtime injection of a webserver into the execution environment to create a web-facing service. The web app exports HTTP as a service by binding to a port, and listening to requests coming in on that port.</p></blockquote><p>This factor is now standard in containerized scenarios. Widespread adoption of port binding has enabled a whole ecosystem of supporting proxies (e.g. <a href="https://www.envoyproxy.io/">Envoy</a>, <a href="https://traefik.io/">Traefik</a>, <a href="https://github.com/Shopify/toxiproxy">Toxiproxy</a>) which (ironically) means that a typical app today is often not self-contained, but depends on other containerized apps to perform e.g. authentication and tracing. This is a improves <a href="https://deviq.com/principles/separation-of-concerns">separation of concerns</a> and consequently, in 2022 we consider this factor at the service level.</p><blockquote><p>The port-binding approach means that one app can become the backing service for another app, by providing the URL to the backing app as a resource handle in the config for the consuming app.</p></blockquote><p>The original text focuses on network protocols such as HTTP and <a href="https://xmpp.org/extensions/">XMPP</a>. In order to become a backing service in 2022, the app should also adhere to an <a href="https://apievangelist.com/2019/07/15/what-is-an-api-contract/">API contract</a> of some sort, defining the backing service&#x27;s intended role.</p><p>Many developers implicitly assume that using high-level protocols like HTTP incurs latency. The overhead of a REST call over a local network (e.g. within a cloud provider) is typically 2-4 ms so you need to get a significant number of non-parallelizable requests before this overhead is noticeable over RDBMS operations and latency towards the consumer.</p><h2>Factor VIII: Concurrency</h2><blockquote><p>In the twelve-factor app, processes are a first class citizen. Processes in the twelve-factor app take strong cues from the unix process model for running service daemons. <!-- -->[...]<!-- --> This does not exclude individual processes from handling their own internal multiplexing. But an individual VM can only grow so large (vertical scale), so the <!-- -->[app]<!-- --> must also be able to span multiple processes running on multiple physical machines.</p></blockquote><p>This factor is now more or less written into law. Function-as-a-service platforms typically provide transparent horizontal scaling. In Kubernetes deployments you just give the number of pods you expect.</p><p>The mechanics of horizontal scalability is thus addressed in 2022. The central challenge for any successful app is to distribute work across multiple processes and backing services in such a way that it actually achieves meaningful horizontal scalability. The typical RDBMS-backed web app usually has its scalability completely constrained by its backing database, forcing vertical scaling of the RDBMS - a matter of allocating additional CPUs. This is often expensive and tends to yield only marginal improvement.</p><p>Is short, horizontal scalability is much preferable over vertical scalability but it is strictly a result of software architecture. It is therefore vital to identify early those apps that will actually require significant scale-out so that they can be properly architected.</p><p>Despite the dominance of the serverless paradigm in the software-as-a-service realm, there is still an overhang from the era of vertical scaling which the Twelve Factor App tries to break with. For example, the virtual machines of Java, Node, Python and Ruby maintain large volumes of reflection metadata and are very reluctant to de-allocate memory, leading to significant inefficiency on scale-out. A new generation of ecosystems, lead by Go and Rust, are more frugal in this respect.</p><h2>Factor IX: Disposability</h2><blockquote><p>The twelve-factor app’s processes are disposable, meaning they can be started or stopped at a moment’s notice. <!-- -->[...]<!-- --> Processes should strive to minimize startup time. <!-- -->[...]<!-- --> Processes shut down gracefully when they receive a SIGTERM signal, <!-- -->[...]<!-- --> allowing any current requests to finish. <!-- -->[...]<!-- --> A twelve-factor app is architected to handle unexpected, non-graceful terminations.</p></blockquote><p>This factor remains relevant as written and remains nearly as elusive today as it was ten years ago. For example, the HTTP server included in Node.js does <a href="https://blog.dashlane.com/implementing-nodejs-http-graceful-shutdown/">not by default perform graceful shutdown</a> (see also <a href="https://github.com/nodejs/node/issues/2642">nodejs issue 2642</a>).</p><p>Fulfilling this factor on an existing code base is much harder than it sounds. It means mapping all the (often implicit) state machines involved in the app and coordinating them so that there are no undefined transitions. For example, the database connection pool must be ready before we bring up our HTTP listener and must not shut down until the HTTP listener is down <em>and</em> all in-flight HTTP requests are done. Workers must &quot;hand back&quot; in-progress work items so that replacement workers do not have to wait for timeout to process those work items. This difficulty is compounded with distributed systems as a conceptual &quot;transaction&quot; may span more than one app or backing service (e.g. writing to file storage and sending mail), requiring <a href="https://en.wikipedia.org/wiki/Two-phase_commit_protocol">two-phase commit</a> semantics.</p><p>This factor is nevertheless the key to the always-on experience that we take for granted in large cloud services. A service with working graceful shutdown and comprehensive health checks can be updated at any time and builds developer and operations confidence. This can result in significant productivity gains, particularly when combined with automated testing.</p><h2>Factor X: Dev/prod parity</h2><blockquote><p>Historically, there have been substantial gaps between development <!-- -->[...]<!-- --> and production <!-- -->[...]<!-- -->, the time gap, the personnel gap <!-- -->[and]<!-- --> the tools gap. <!-- -->[...]<!-- --> The twelve-factor app is designed for continuous deployment by keeping the gap between development and production small.</p></blockquote><p>This factor is now colloquially known as <a href="https://aws.amazon.com/devops/what-is-devops/">DevOps</a> and remains as relevant as ever, but has still not established itself fully in software-as-a-service development: many production environments are hard to squeeze onto a developer&#x27;s laptop. Generally speaking, the public cloud providers put too little effort into supporting development use cases for their services. Kubernetes goes furthest in this respect: <a href="https://kind.sigs.k8s.io/">Kind</a> deserves mentioning for its heroic effort to achieve a dev-friendly, multi-node Kuberentes cluster using only Docker.</p><p>Docker has introduced a borderland where it is possible to develop a container using just Docker Engine for dev environment and still be reasonably confident that it will execute properly in e.g. Kubernetes. Still, some provisioning of backend services is still needed and time is wasted maintaining two different sets of instrumentation. For example, apps often have a Docker Compose file to get developers started, and a Kubernetes manifest for test/prod. When these desynch, nasty surprises can occur at deployment.</p><blockquote><p>The twelve-factor developer resists the urge to use different backing services between development and production</p></blockquote><p>Many full-stack development setups includes &quot;dev&quot; servers whose role is to automatically reload the app as its source code change. Similarly, tools like Docker Desktop and <a href="https://tilt.dev/">Tilt</a> provide capabilities and environments that are subtly different from e.g. Azure Container instances or Kubernetes. All these will color developers&#x27; choices and risk introducing issues that will not be discovered until late in the process.</p><p>The 2022 developer considers both developer experience, continuous integration/delivery/deployment and operability when choosing tools.</p><h2>Factor XI: Logs</h2><blockquote><p>Logs provide visibility into the behavior of a running app. <!-- -->[...]<!-- --> A twelve-factor app never concerns itself with routing or storage of its output stream. It should not attempt to write to or manage logfiles. Instead, each running process writes its event stream, unbuffered, to stdout. <!-- -->[...]<!-- --> Destinations are not visible to or configurable by the app, and instead are completely managed by the execution environment.</p></blockquote><p>Interestingly, this factor does not actually advise on the use of logging. Rather it treats them much as pre-contraceptive times viewed children: as something that inevitably accumulates as a result of marriage.</p><p>The factor should thus be updated to mandate that an app should be &quot;observable&quot;, meaning that it should volunteer information on its performance and behavior. We normally break this down into logging, metrics, tracing and audit trails. The relative merit of these differ greatly between apps, but all apps should have an observability strategy. Often, the need changes as an app evolve: early in the lifecycle, logging may dominate, but as usage grows, focus shifts to metrics. The apps in a service are considered as a unit and typically provide different observability needs.</p><h2>Factor XII: Admin processes</h2><blockquote><p>One-off admin processes should be run in an identical environment as the regular long-running processes of the app. They run against a release, using the same codebase and config as any process run against that release.</p></blockquote><p>This factor captures a practice that is common in the <a href="https://rubyonrails.org/">Rails</a> and <a href="https://www.drupal.org/">Drupal</a> ecosystems, among others. These are based on interpreted languages where it is relatively easy to give scripting or interactive access to the app&#x27;s internals: the main reason is to ensure that database access occurs using the same models that are used during runtime. In compiled languages, this requires a modularization (e.g. making the data model a separate library) that would complicate development.</p><p>However, the factor has two underlying principles which holds even today. First, that tools used to administer an app should be versioned and released with the same discipline as your app is. Second, that operating and upgrading an app is part of its ordinary usage and there is nothing strange with adding endpoints for internal administrative use. Put differently, at least factors I - IV should apply to the app&#x27;s tooling just as it does to the app itself.</p><h2>What else is there?</h2><p>Various additional factors have been proposed over the years, for example <a href="https://raw.githubusercontent.com/ffisk/books/master/beyond-the-twelve-factor-app.pdf">Beyond the Twelve-factor app</a> and <a href="https://www.ibm.com/cloud/blog/7-missing-factors-from-12-factor-applications">Seven missing factors from the Twelve-factor app</a>. The appeal of the original methodology springs from the universality of its factors. These contributions have relevance, but often only for a subsection of all apps that could (should) strive to live up to the original twelve factors. However, there is two aspects that are clearly missing: security and automated testing.</p><p>The good people at WhiteHat Security has written a good analysis called <a href="https://www.devopsdigest.com/security-and-12-factor-app-step-1">Security and the twelve-factor app</a> which analyses each factor from a security perspective and provides recommendations for securing your app. Their main point is that rather than being an additional factor, security needs to permeate all the twelve factors. The charge that security is underrepresented in the original methodology har merit, and the 2022 developer no longer has the luxury of treating security as an afterthought.</p><p>Finally, much of the benefit of adhering to the methodology is lost without extensive and automated testing. Version control hygiene and horizontal scalability matters little if apps break as soon as they are deployed. Ten years ago, there was still a discussion about whether writing programmatic tests was worth the effort. That discussion is now settled and in 2022, the discussion is about what the automated testing strategy should look like for a particular app or service. The discerning developer considers:</p><ul><li>when to apply unit, component, integration and end-to-end tests at app and/or service level</li><li>when to use in-memory implementations of backing services</li><li>what long-lived testing environments are needed</li><li>how much automated <a href="https://en.wikipedia.org/wiki/Static_program_analysis">static analysis</a> to include</li></ul><p>These additions notwithstanding, the Twelve-factor app methodology remains remarkably relevant today. All developers producing software-as-a-service offerings can benefit from adhering to its factors.</p>]]></content>
        <author>
            <name>Anders Qvist</name>
        </author>
    </entry>
</feed>