<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Xenit Blog</title>
        <link>http://xenitab.github.io/blog</link>
        <description>Xenit Blog</description>
        <lastBuildDate>Mon, 25 Apr 2022 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <item>
            <title><![CDATA[Designing RESTful APIs for cloud services]]></title>
            <link>http://xenitab.github.io/blog/2022/04/25/designing-restful-apis-for-cloud-services</link>
            <guid>Designing RESTful APIs for cloud services</guid>
            <pubDate>Mon, 25 Apr 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[This blog post provides guidelines for RESTful APIs for software-as-a-service offerings, with the goal to maximize their life span and support rapid evolution throughout it.
]]></description>
            <content:encoded><![CDATA[<p>HTTP has become the de-facto standard transport protocol for programmatic communication in software-as-a-service offerings. This mostly entails publishing request-response style APIs. We often refer to these APIs as &quot;RESTful APIs&quot;:</p><blockquote><p><em>[When a]<!-- --> request is made via a RESTful API, <!-- -->[the response is]<!-- --> a representation of the state of the resource<!-- -->[.]</em> -- <a href="https://www.redhat.com/en/topics/api/what-is-a-rest-api">https://www.redhat.com/en/topics/api/what-is-a-rest-api</a></p></blockquote><p>Let&#x27;s decrypt that: &quot;representation&quot; has come to mean JSON, while &quot;state&quot; refers to those ubiquitous (and unwieldy) relational databases and &quot;resource&quot; is an object from our domain model.</p><p>The literature on API design will exhort you to analyze the problem space and consider your design choices carefully. Indeed, design choices will significantly impact the lifecycle of APIs that are part of a software-as-a-service offering. However, when designing a software-as-a-service API, we do not really know the details of future usage. The goal must therefore be to maximize our freedom to evolve the APIs without having to change the formal or informal contracts that regulate their usage. The API should become a facade behind which we are free to evolve the implementation.</p><p>This post proposes and motivates a set of guidelines for RESTful APIs (and by extension their contracts) intended to maximize their life span and support rapid evolution throughout it. The post focuses on organizations that want to provide commercial software-as-a-service offerings, though many of the guidelines have wider application.</p><h2>Start with the basics</h2><p>The guidelines below should be considered in addition to established good practice, so if you are new to REST, you may want to start by reading the literature. Here are some good good articles about RESTful API practices:</p><ul><li><a href="http://www.restfulwebapis.org/">http://www.restfulwebapis.org/</a></li><li><a href="https://restfulapi.net/rest-api-design-tutorial-with-example/">https://restfulapi.net/rest-api-design-tutorial-with-example/</a></li><li><a href="https://docs.microsoft.com/en-us/azure/architecture/best-practices/api-design">https://docs.microsoft.com/en-us/azure/architecture/best-practices/api-design</a></li></ul><p>Note that these articles and the recommendations are not in total harmony, for example when it comes to the extent to which use cases should be allowed to influence API design.</p><p>Also, please remember that HTTP is a very rich transport protocol which provides solutions to many common API needs. For example, <a href="https://tools.ietf.org/html/rfc7231#section-5.3">content negotiation</a> and <a href="https://tools.ietf.org/html/rfc7232">conditional requests</a> can help solve various problems.</p><h2>Practices for rapid evolution</h2><p>Additionally, there are a number of good practices which are relevant when building software-as-a-service RESTful APIs.</p><h3>Your API is a collection of nouns</h3><p><em>The most central tenet of REST bears repeating: your API is expressed in nouns, each of which is a class of resources. (If expressing your API in terms of nouns feels contrived, you may want to consider an RPC style API instead.) Those resources are queried and manipulated using basic <a href="https://developer.mozilla.org/en-US/docs/Glossary/CRUD">&quot;CRUD&quot;</a> operations. A car sharing service might expose <code>GET /vehicles</code> for finding available vehicles and an individual vehicle would be <code>GET /vehicles/:id</code>.</em></p><p><strong>Motivation</strong>: Focusing on resources reduces the risk of implementation details bleeding into the API, which means that it becomes easier to change the backing implementation. This is akin to Kant&#x27;s <a href="https://en.wikipedia.org/wiki/Thing-in-itself">Der ding an sich</a>, in that we are trying to discover what properties a resource should reasonably have to match the sum of all observations.</p><p>An important consequence of realizing a service as a series of nouns is that in order to be able to keep to CRUD operations, we may need to introduce new nouns (i.e. sub-resource), for example giving cars &quot;services&quot; so that we have <code>POST /vehicles/:id/services/heater</code> for activating the car&#x27;s heater. With this design, the developer using the API knows that discovery will be <code>GET /vehicles/:id/services</code> and heater status can be checked with <code>GET /vehicles/:id/services/heater</code>. &quot;CRUD plus noun&quot; becomes a contract building blocks.</p><h3>The caller is responsible for the use case</h3><p><em>The RESTful API concerns itself with effective access to the data model (the &quot;resources&quot;). Generally speaking, the use case is the caller&#x27;s concern. For example, the caller may be required to combine data from numerous API calls, read more data than it needs and perform its own sorting.</em></p><p><strong>Motivation</strong>: An endpoint that is optimized for one use case will be hard pressed to accommodate a second use case. It will be hard to avoid adding a second similar endpoint, risking divergence. Furthermore, use cases will evolve over time and if too much of processing, filtering and sorting quirks is handled by the endpoint, it is very easy to end up in a situation where we have to spend our time optimizing specific database queries for individual use cases rather than improving the performance for all callers by refactoring storage.</p><p>Properly implemented RESTful APIs have a good chance of ageing gracefully. By pushing parts of the business logic to the caller (e.g. a batch job or a <a href="https://samnewman.io/patterns/architectural/bff/">backend-for-frontend</a>) it ensures that the RESTful API can be reused across many use cases.</p><p>One of the most common mistakes with RESTful APIs is to treat the backend as a layer that translates API calls into SQL. Under this fallacy, as APIs evolve, their queries grow more complex (joining, sorting and complex mutations are common examples) making it ever harder to maintain response times. An extreme version of this is &quot;passthru-sql&quot; (e.g. a query parameter like <code>?filter=&quot;username eq &#x27;bittrance&#x27;&quot;</code>). When developers try to follow the precepts of REST but retain an RPC mindset, they frequently create endpoints that allow the caller to pass query fragments that are appended to the resulting database query more or less verbatim.</p><h3>A published API is an eternal promise</h3><p><em>As long as we have paying customers on a particular API, we maintain that API. We may refuse access by new customers and we may cancel entire services, but as long as a customer uses a service, all APIs on that service are maintained. Where an API in use needs to be decommissioned, that is a commercial decision.</em></p><p><strong>Motivation</strong>: There is no good time to decommission an API. Any change you force on a customer will incur costs for that customer, with limited benefit. Furthermore, in many cases your success will come through partners using your APIs to design new services on top of yours. Adding customers to a well-designed multi-tenant service has very low marginal cost, potentially enabling different business models. Strong sun-setting clauses will constrain our partners&#x27; business models.</p><p>You may still want to retain the right to decommission APIs in your contracts; sometimes runaway success may incur unacceptable operational costs and you are forced to redesign. Just be aware that regular use of that clause will damage your reputation. Google Ads ability to <a href="https://developers.google.com/google-ads/api/docs/sunset-dates">regularly decommission</a> their APIs is a strong indicator of its undue marketing power. For a counter-point look at <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-classic-platform.html">retiring EC2 Classic</a>. It took over 10 years from the decision was made to retire EC2 Classic until AWS decided it was commercially acceptable to evict the last stragglers in late 2022.</p><h3>Endpoints make no assumptions about the URL space</h3><p><em>We frequently use HTTP load balancers (and API gateways) to compose our URL space. They may direct any arbitrary part of that space to a particular process. Thus, <code>POST /customers</code> may be one service (which writes to the master database) and <code>GET /customers/:id</code> goes to read replica: a particular endpoint or process must not assume that it &quot;owns&quot; the customer resources, for example by assuming that it will see all writes. Similarly, endpoints should minimize the part of the object model that it requires to present its resource. For example, <code>GET /users/:id</code> should not include additional company information in order to be useful, since users and companies may need to be split across different services tomorrow.</em></p><p><strong>Motivation</strong>: Our users will be successful by creating innovative things on top of our APIs. Almost by definition, they will use our APIs in ways we did not forsee, thus creating unexpected loads. Therefore, a large part of evolving a cloud service is about changing how data is partitioned and what storage systems are used. Therefore, it is very important to retain flexibility in this regard.</p><p>A service typically starts small, as a single process exposing all your endpoints. However, as the service grows in popularity and scope, simple horizontal scaling is often not possible and you need to diversify: you may add new overlapping (micro-)services or you may want to split reads and writes into separate processes (i.e. go <a href="https://docs.microsoft.com/en-us/azure/architecture/patterns/cqrs">CQRS</a>).</p><h3>JSON objects are maps</h3><p><em>Adding properties to any returned object is considered a non-breaking change. API docs should point out that properties are new. Similarly, an API can start accepting new optional query parameters on the URL or properties in the input body or add HTTP headers in either direction without being considered breaking.</em></p><p><strong>Motivation</strong>: REST fundamentally limits us to CRUD and behavior will be implicit from the resource state. In order to implement new behavior it follows that we will over time introduce new properties which controls that behavior.</p><h3>Versioning is part of the URI</h3><p><em>The URI should contain a version number. In <a href="https://semver.org/">semver</a> terms, this is a &quot;major&quot; version and we use it to signal breaking changes. Given that we have the ability to extend input and output (see <a href="#json-objects-are-maps">JSON objects are maps</a>), it should be possible to accommodate most &quot;minor&quot; changes within existing APIs. Ideally, resources with different versions have an implicit relation. For example, if we serve both <code>GET /v1/customers/acme</code> and <code>GET /v2/customers/acme</code>, they refer to the same customer.</em></p><p><strong>Motivation</strong>: Versions in the URI serve two purposes. First, it signals that one resource should be preferred over another. Second, enables us to write new implementations of a service incrementally.</p><h3>Authorization is based on method + resource</h3><p><em>Client authorization should depend only on the HTTP request method and the URI (and in some cases on headers). It should preferably not on depend on the request body and particularly not on the state of the resource.</em></p><p><strong>Motivation</strong>: Ideally, both authentication and authorization should be handled outside of your endpoint. This may be by middleware in your API or by a load balancer or API gateway. Furthermore, having bespoke authorization logic in your endpoints invites security bugs. It is also hard to document and understand for the caller. Loading the underlying resource to know whether the caller is permitted to perform the request risks being expensive - if you deactivate the caller&#x27;s credentials you don&#x27;t want to continue accruing the cost of those calls (or higher: the client may well retry several times). Finally, filtering lists on permission defeats caching.</p><p>A consequence of this rule is that you should avoid APIs that use permissions as filter criteria for resource listings; everyone who calls <code>GET /users</code> should get the same list. If the user list is secret, you introduce a <code>GET /departments/:id/users</code> that has only the relevant users. Similarly, if you have restricted parts of a resource, you can make it into a sub-resource, e.g. <code>GET /users/:id/access_tokens</code>.</p><h3>Be tough on clients</h3><p><em>Clients are expected to:</em></p><ul><li>implement HTTP properly. For example, if a resource was missing a correct Content-Type, a client that breaks when this is rectified is at fault.</li><li>be reasonably parallel. A client should be able to make thousands of requests in a reasonable time frame. For example, a search operation may return summaries and if the client wants more information, it is expected to request the full object for each returned item.</li><li>do local caching. A client that excessively requests resources that has caching directives should be considered as misbehaving.</li></ul><p><strong>Motivation</strong>: We are building REST APIs to be used by many different callers and our situation would quickly be untenable if we had to respect quirky clients and inexperienced developers. For example, it is relatively straight-forward to horizontally scale a service that can answer 1 million GET/s, but very tricky to answer one GET request which is supposed to return 1 million entries per second.</p><p>Someone may protest that browsers only allow 6-8 concurrent HTTP sessions against one host and that data must therefore be aggregated or pre-processed for clients to be performant. Normally, introducing <a href="https://datatracker.ietf.org/doc/html/rfc7540#section-5">HTTP/2.0 multiplexing</a> and ensuring observed response times of &lt;50ms will do the trick just as well.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Kubernetes Ephemeral Container Security]]></title>
            <link>http://xenitab.github.io/blog/2022/04/12/ephemeral-container-security</link>
            <guid>Kubernetes Ephemeral Container Security</guid>
            <pubDate>Tue, 12 Apr 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[Ephemeral containers is a new concept in Kubernetes which allows attaching  containers to already running Pods. It also introduces new security concerns which have to be resolved before it can be enabled.
]]></description>
            <content:encoded><![CDATA[<p>Attempting to debug a Pod and realizing that you can&#x27;t install curl due to security settings has to be a meme at this point. Good security practices are always nice but it often comes at the cost of usability. To the point where some may even solve this problem by installing debug tools into their production images. Shudders.</p><img src="https://i.imgflip.com/6cczqi.jpg" title="made at imgflip.com"/><p>Kubernetes has introduced a new concept called <a href="https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/">ephemeral containers</a> to deal with this problem. Ephemeral containers are temporary containers that can be attached after a Pod has been created. Rejoice! We can now attach a temporary container with all the tools which we desire. While the applications container may have &quot;annoying security features&quot; like a read only file system the ephemeral container can enjoy all the freedom which writing files entails. I love this feature so I need to upgrade my cluster immediately!</p><h2>Digging Deeper</h2><p>Now that we have the new feature we can start a ephemeral container in any Pod we like.</p><pre><code class="language-shell">kubectl run ephemeral-demo --image=k8s.gcr.io/pause:3.1 --restart=Never
kubectl debug -it ephemeral-demo --image=busybox:1.28
</code></pre><p>We get a shell and life is now much simpler, but wait a minute. This post is not about how to use ephemeral containers, there are enough of those already, but rather the security implications of enabling ephemeral containers. Let&#x27;s have a look at the YAML for the Pod that we created the ephemeral container in.</p><pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: ephemeral-demo
spec:
  ...
  ephemeralContainers:
  - name: debugger-r59b7
    image: busybox:1.28
    imagePullPolicy: IfNotPresent
    stdin: true
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    tty: true
</code></pre><p>Interesting, there is a new field called <code>ephemeralContainers</code> in the Pod definition. This new field contains a list of containers similar to <code>initContainers</code> and <code>containers</code>. It is not identical as there are certain options which are not available, refer to the <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#ephemeralcontainer-v1-core">API documentation</a> for more information. It does however allow configuration of the container security context, which could in theory allow a bad actor to escalate the container&#x27;s privileges. This should not affect those of us who use a policy enforcement tool right? The answer is yes and no depending on the tool and version that is being used. It also depends on if you are using policies from the project&#x27;s library or policies developed in house.</p><h3>OPA Gatekeeper</h3><p><a href="https://github.com/open-policy-agent/gatekeeper">OPA Gatekeeper</a> does not require any code changes as all of its policies are written in <a href="https://www.openpolicyagent.org/docs/latest/policy-language/">rego</a>. It&#x27;s sub project <a href="https://github.com/open-policy-agent/gatekeeper-library/">Gateekper Library</a> does however have to be updated. The library contains an implementation of the common Pod Security Policies. This includes policies like not allowing containers in privileged mode. The issue with the all of the policies is that they currently only check containers specified in <code>initContainers</code> and <code>containers</code>, analyze the <a href="https://github.com/open-policy-agent/gatekeeper-library/blob/275a1628694dcdf9daf5f6dda1373de6af78e7da/library/pod-security-policy/privileged-containers/template.yaml#L49-L55">following</a> rego as an example.</p><p>The good news is that this is a pretty easy fix, the bad news is that it requires end users to update the policies pulled from the library.</p><h3>Kyverno</h3><p><a href="https://github.com/kyverno/kyverno">Kyverno</a> seems to have resolved the issues faster. Compared to OPA Gatekeeper however it did require a small code change which means that version <a href="https://github.com/kyverno/kyverno/releases/tag/v1.5.3">1.5.3</a> or later is needed to write policies for ephemeral containers. They have also <a href="https://github.com/kyverno/policies/pull/241">updated their policy library</a> to include checking ephemeral containers. Kyverno has done a great job solving these issues quickly. It does still require end users to update however.</p><h3>Pod Security Policies</h3><p><a href="https://kubernetes.io/docs/concepts/security/pod-security-policy/">Pod Security Policies</a> used to be the default policy tool for Kubernetes, and a lot of projects have rules based on Pod Security Policies (PSP). However if you are relying on PSP in a modern cluster you should really start looking for other options like OPA Gatekeeper or Kyverno. PSP has been deprecated since Kubernetes v1.21 and will be removed in v1.25.</p><p>If PSP is your only policy tool and you are planning to upgrade to v1.23, don&#x27;t. As PSP is deprecated no new features have been added, and that includes policy enforcement on ephemeral containers. Which means that any security context in an ephemeral container is allowed no matter the PSP in the cluster.  The PSP below will have no affect when adding an ephemeral container to a Pod which is privileged.</p><pre><code class="language-yaml">apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: default
spec:
  privileged: false
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  runAsUser:
    rule: RunAsAny
  fsGroup:
    rule: RunAsAny
  volumes:
  - &#x27;*&#x27;
</code></pre><h3>RBAC</h3><p>Disallowing ephemeral containers with RBAC could be an option if the feature is not needed and it is not possible to disable the feature completely. The <a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/277-ephemeral-containers/README.md">KEP-277: Ephemeral Containers</a> state the following about using RBAC to disable ephemeral containers.</p><blockquote><p>Cluster administrators will be expected to choose from one of the following mechanisms for restricting usage of ephemeral containers:</p><ul><li>Use RBAC to control which users are allowed to access the /ephemeralcontainers subresource.</li><li>Write or use a third-party admission controller to allow or reject Pod updates that modify ephemeral containers based on the content of the update.</li><li>Disable the feature using the EphemeralContainers feature gate.</li></ul></blockquote><p>RBAC is additive which means that it is not possible to remove permissions from a role. This type of mitigation obviously does not matter if all users a cluster admin, which they should not be, so we assume that new roles are created for the cluster consumers. In this case having a look at the existing roles can be enough to make sure that the subresource <code>/ephemeralcontainers</code> is not included in the role.</p><pre><code class="language-yaml">apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: edit
rules:
- apiGroups:
  - &quot;&quot;
  resources:
  - pods
  - pods/attach
  - pods/exec
  - pods/portforward
  - pods/proxy
  verbs:
  - create
  - delete
  - deletecollection
  - patch
  - update
</code></pre><h2>Checking Policy Enforcement</h2><p>Let&#x27;s say that you upgraded your cluster and informed all end users of the great new feature. How do you know that the correct policies are enforced in accordance to your security practices. You may have been aware of the API changes and taken the correct precautionary steps. Or you just updated Kyverno and it&#x27;s policies out of pure happenstance. Either way it is good to trust but verify that it is not for example possible to create a privileged ephemeral container. Annoyingly the debug command does not expose any options to set any security context configuration, so we need another option. Ephemeral containers cannot be defined in a Pod when it is created and it can neither be added with an update. We need some other method to create these specific ephemeral containers.</p><blockquote><p>Ephemeral containers are created using a special ephemeralcontainers handler in the API rather than by adding them directly to pod.spec, so it&#x27;s not possible to add an ephemeral container using <code>kubectl edit</code>.</p></blockquote><p>The simplest method to add an ephemeral container with a security context to a Pod is to use the Go client. A couple of lines of code can add a new ephemeral container running as privileged or use any other security context setting which is to your liking.</p><pre><code class="language-go">package main

import (
    &quot;context&quot;
    &quot;fmt&quot;
    &quot;os&quot;

    corev1 &quot;k8s.io/api/core/v1&quot;
    metav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;
    &quot;k8s.io/client-go/kubernetes&quot;
    &quot;k8s.io/client-go/tools/clientcmd&quot;
)

func main() {
    if len(os.Args) != 4 {
        panic(&quot;expected three args&quot;)
    }
    podNamespace := os.Args[1]
    podName := os.Args[2]
    kubeconfigPath := os.Args[3]

    // Create the client
    client, err := getKubernetesClients(kubeconfigPath)
    if err != nil {
    panic(fmt.Errorf(&quot;could not create client: %w&quot;, err))
    }
    ctx := context.Background()

    // Get the Pod
    pod, err := client.CoreV1().Pods(podNamespace).Get(ctx, podName, metav1.GetOptions{})
    if err != nil {
    panic(fmt.Errorf(&quot;could not get pod: %w&quot;, err))
    }

    // Add a new ephemeral container
    trueValue := true
    ephemeralContainer := corev1.EphemeralContainer{
        EphemeralContainerCommon: corev1.EphemeralContainerCommon{
            Name:  &quot;debug&quot;,
            Image: &quot;busybox&quot;,
            TTY:   true,
            SecurityContext: &amp;corev1.SecurityContext{
                Privileged:               &amp;trueValue,
                AllowPrivilegeEscalation: &amp;trueValue,
            },
        },
    }
    pod.Spec.EphemeralContainers = append(pod.Spec.EphemeralContainers, ephemeralContainer)
    pod, err = client.CoreV1().Pods(pod.Namespace).UpdateEphemeralContainers(ctx, pod.Name, pod, metav1.UpdateOptions{})
    if err != nil {
    panic(fmt.Errorf(&quot;could not add ephemeral container: %w&quot;, err))
    }
}

func getKubernetesClients(path string) (kubernetes.Interface, error) {
    cfg, err := clientcmd.BuildConfigFromFlags(&quot;&quot;, path)
    if err != nil {
        return nil, err
    }
    client, err := kubernetes.NewForConfig(cfg)
    if err != nil {
        return nil, err
    }
    return client, nil
}
</code></pre><p>Run the program and pass the namespace, pod name, and path to a kube config file. We assume that the ephemeral-demo Pod is still running. </p><pre><code class="language-shell">go run main.go default ephemeral-demo $KUBECONFIG
</code></pre><p>If it completes with no error a privileged ephemeral container should have been added to the Pod. Exec into it and list the host&#x27;s devices to prove that it is a privileged container.</p><pre><code class="language-shell">kubectl exec -it ephemeral-demo -c debug -- sh
ls /dev
</code></pre><h2>Conclusion</h2><p>If there is one takeaway from this post, it is that any policy tool that has not been updated in the last couple of months will not enforce rules on ephemeral containers. This also includes all policies written in house! It is not enough to update the community policies.</p><p>Some may argue that this type of oversight is not really an issue. Ephemeral containers can&#x27;t mount <a href="https://kubernetes.io/docs/concepts/storage/volumes/#hostpath">host paths</a>, or access the <a href="https://kubernetes.io/docs/concepts/security/pod-security-policy/#host-namespaces">hosts namespaces</a>. All it can do is set the common container security context. That is a fair comment, because it&#x27;s true. Being able to create a privileged container is however still not ideal, and there are <a href="https://bishopfox.com/blog/kubernetes-pod-privilege-escalation#Pod3">methods to escalate privileges</a> when this is possible. Either way it is important to be aware of how policies are enforced and the security contexts which are allowed.</p><p>I am still not sure how much of an issue this will be short term. Cloud providers are currently in the process of rolling out Kubernetes v1.23 in their SaaS offerings. In these solutions it is still a possibility that they chose to disable ephemeral containers. Those rolling their own clusters may have already upgraded to v1.23 and not be aware of the new feature. That is the biggest issue really, that the platform administrator has to be aware of the existence of ephemeral containers. The fact that kubectl does not expose the option to set a security context will make even less people aware that it is still possible to set one with other means. Investing in a security audit 6 months ago will only be valuable as long as the same Kubernetes version is used. Kubernetes is by design <strong>not</strong> secure by default, so each new feature introduced has to be analyzed. The fact that upgrading from Kubernetes v1.22 to v.23 could make your cluster less secure is part of the difficulties of working with Kubernetes, requiring platform administrators to always stay on top of things. The reality is that these types of things are easy to miss, so hopefully this post has helped someone make their cluster a bit more secure.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Twelve-factor app anno 2022]]></title>
            <link>http://xenitab.github.io/blog/2022/02/23/12factor</link>
            <guid>Twelve-factor app anno 2022</guid>
            <pubDate>Wed, 23 Feb 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[The Twelve-factor app methodology turns 10. This blog posts re-evaluates  the original factors against a decade of experience with  software-as-a-service development and the maturing of serverless  development.
]]></description>
            <content:encoded><![CDATA[<p><a href="https://12factor.net/">The Twelve-factor app</a> is a methodology for building software-as-a-service apps that was first formulated by developers associated with Heroku. It&#x27;s been ten years since the first presentation of this methodology. Despite the criticism that it is only applicable to Heroku and similar webapp services, it remains a relevant yard stick for software-as-a-service development. Some of its tenets have been incorporated into Docker and thence into OCI, effectively making them the law of container-land. This blog post looks at each of the twelve factors and tries to evaluate whether they remain relevant or whether they need updating.</p><p>In one respect, the criticism of being Heroku-centric is relevant. Heroku (and Google App Engine and similar services) offer a single packaging model which we today might refer to as &quot;IaC-less&quot;: you provide an HTTP server and Heroku runs it for you (or a war file in Google App Engine&#x27;s case). Any non-trivial software-as-a-service offering require composing many apps into a service, where each has a distinct role: authentication, caching, API, serving static files, et.c., with some infrastructure-as-code that describes how these apps are exposed and interconnect. We end up with an &quot;app&quot; level and a &quot;service&quot; level and we have to be careful when considering the original text, since it talks only about the app level, but some of its concerns now reside at the service level.</p><h2>Factor I: Codebase</h2><blockquote><p>A codebase is any single repository. <!-- -->[...]<!-- --> The codebase is the same across all deploys, although different versions may be active in each deploy.</p></blockquote><p>This factor is first and foremost an argument for expressing as much as possible of your service as code that can be put under version control. This was probably already a strawman attack ten years ago. Nevertheless, the latest incarnation of mandating version control is <a href="https://www.weave.works/technologies/gitops/">GitOps</a>, namely the idea that your infrastructure maintains itself by reading your version control system and automatically applies changes as necessary - remarkably similar to the original Heroku model.</p><blockquote><p>There is always a one-to-one correlation between the codebase and the app. If there are multiple codebases, it’s not an app – it’s a distributed system. Each component in a distributed system is an app, and each can individually comply with twelve-factor.</p></blockquote><p>This part of the factor remains relevant at the app level, but modern public cloud providers&#x27; infrastructure-as-code tooling and platforms like Kubernetes allow us to describe a set of apps and their supporting resources (e.g. secrets) as one package or service. Some organizations separate infrastructure-as-code and app code into separate repositories while others keep the app and its supporting IaC together; neither of these can be said unconditionally to be best practice. Similarly, <a href="https://en.wikipedia.org/wiki/Single-page_application">Single-page apps</a> are often deployed to a <a href="https://www.cloudflare.com/learning/cdn/what-is-a-cdn/">Content Distribution Network</a>, while their backend may be deployed to a public cloud provider or to a <a href="https://kubernetes.io/">Kubernetes</a> cluster. Whether these should be kept in the same repository or in different repositories depends on how tightly coupled they are.</p><p>The 2022 developer considers the relationship between repositories and artifacts carefully. Pertinent aspects include:</p><ul><li>branching strategy</li><li>continuous integration completeness and run times</li><li>continuous delivery pipelines</li><li>infrastructure-as-code maintainability</li><li>configuration management</li><li>automated deployments</li></ul><p>Expect to reorganize your sources as your apps evolve.</p><h2>Factor II: Dependencies</h2><blockquote><p>A twelve-factor app never relies on implicit existence of system-wide packages. It declares all dependencies, completely and exactly, via a dependency declaration manifest. <!-- -->[...]<!-- --> Twelve-factor apps also do not rely on the implicit existence of any system tools.</p></blockquote><p>In the container and function-as-a-service worlds, this factor has been elevated to fact. These execution environments provide virtually no implicit dependencies.</p><p>Modern apps tend to have more than one dependency declaration manifest, namely its project manifest(s) (e.g. <code>go.mod</code> or <code>package.json</code>) and a <code>Dockerfile</code>. A consequence of this factor is that you should use Docker base images that are as bare-bone as they come, forcing the explicit installation of supporting libs and tools.</p><p>The up-to-date interpretation of this factor is that upgrading dependency versions should always be a conscious action. This slightly shifts the interpretation of the original factor&#x27;s &quot;exactly&quot;. The various ecosystems and tool chains (maven, npm, cargo, et.c.) work differently in when they resolve dependencies. Some resolve dependencies when the developer performs a build and some require an explicit &quot;upgrade&quot; operation to change what goes into a build. It is therefore vital to have a codified workflow for updating dependencies. For example, when using Node.js and npm, a developer should normally do <a href="https://blog.npmjs.org/post/171556855892/introducing-npm-ci-for-faster-more-reliable">npm ci</a> and only use the traditional <code>npm install</code> (or <code>npm update</code>) when the intent is to modernize dependencies.</p><blockquote><p>it uses a dependency isolation tool during execution to ensure that no implicit dependencies “leak in” from the surrounding system. The full and explicit dependency specification is applied uniformly to both production and development.</p></blockquote><p>One of the innovations introduced by Docker is that this factor is enforced already at build time, making it easy to ensure uniformity across dev and prod. Run your automated tests with the built container and there is very little space for execution environment differences.</p><h2>Factor III: Config</h2><blockquote><p>An app’s config is everything that is likely to vary between deploys (staging, production, developer environments, etc). The twelve-factor app stores config in environment variables. Env vars are easy to change between deploys without changing any code; unlike config files, there is little chance of them being checked into the code repo accidentally; and unlike custom config files, <!-- -->[...]<!-- --> they are a language- and OS-agnostic standard. <!-- -->[...]<!-- --> A litmus test for whether an app has all config correctly factored out of the code is whether the codebase could be made open source at any moment, without compromising any credentials.</p></blockquote><p>This factor remains mostly relevant as written, but there are some nuances to consider.</p><p>Infrastructure-as-code tools like Terraform allows us to create files and database entries. Kubernetes allows us to create <a href="https://kubernetes.io/docs/concepts/configuration/configmap/">ConfigMaps</a> which will be made available to a container as a file. In both cases, the source configuration can be put under version control, any manual edits will be overwritten on the next deploy and the mechanism is easy for a developer to emulate locally. Thus, they achieve the same result as using environment variables by different means.</p><p>Also, while environment variables are operations-friendly, they are problematic when writing tests, since they are global state. In ecosystems that default to parallel test execution (e.g. Rust) environment variables cannot be used. Thus, while an environment variable remains the preferred way to accept simple configuration values, a 12-factor app should convert them to internal state as early as possible.</p><p>This factor is now obsolete in one respect. As much as possible, secrets (passwords, private keys, et c) should be stored using a secrets management system such as Hashicorp Vault or Azure Key Vault. Particularly where we can rely on the infrastructure to authenticate the calling process (e.g. via a Kubernetes service account) access to secrets will not directly require credentials. The existence of the secret is under version control, but the actual secret content is immaterial.</p><p>Furthermore, with platforms such as Kubernetes, service discovery means that some aspects need no configuration at all. Additionally, some forms of configuration can better be managed as references between IaC-controlled resources, which removes them from direct configuration management consideration.</p><h2>Factor IV: Backing services</h2><blockquote><p>A backing service is any service the app consumes over the network as part of its normal operation. The code for a twelve-factor app makes no distinction between local and third party services. To the app, both are attached resources, accessed via a URL or other locator/credentials stored in the config. <!-- -->[...]<!-- --> Resources can be attached to and detached from deploys at will.</p></blockquote><p>This factor remains relevant as written. Its current iteration is sometimes referred to as <a href="https://swagger.io/resources/articles/adopting-an-api-first-approach/">&quot;API first&quot;</a> which can be described as the principle that all services you create should be able to act as a backing service. More generally, with the advent of <a href="https://www.crowdstrike.com/cybersecurity-101/zero-trust-security/">Zero Trust</a> and the proliferation of cloud services, the logical end result of this factor is that any service can interact with any other service on the planet.</p><p>Even your orchestration platform itself is a backing service, not just the services that run inside it. A service can leverage the Kubernetes control plane to run a one-off job or provision cloud resources to serve a new customer.</p><p>The original text focuses a lot on relational databases. It is worth pointing out that you can create a service which scalably serves long-lived state without violating the 12 factors: as long as there is a procedure to claim or negotiate access to a particular shard of the state (e.g. backups stored in an Azure storage account), the actual process can remain stateless. Contemporary thinking in this matter is still coloured by software that predates the cloud era (e.g. MySQL, Elasticsearch, RabbitMQ). For an example of what is possible in 2022, we can look at <a href="https://github.com/grafana/loki">Loki</a>.</p><h2>Factor V: Build, release, run</h2><blockquote><p>The twelve-factor app uses strict separation between the build, release, and run stages. The build stage is a transform which converts a code repo into an executable bundle known as a build.</p></blockquote><p>This factor is more or less a prerequisite for developing software-as-a-service in 2022, but we need to complement this factor with a requirement for automating these stages. The maturing of CI/CD software-as-a-service providers such as GitHub, <a href="https://docs.microsoft.com/en-us/azure/container-registry/container-registry-tasks-overview">ACR Tasks</a> and <a href="https://circleci.com/">Circle CI</a> means that it is now relatively easy to automate this process.</p><p>Typically, the build stage will push a container image to some container registry or upload a function-as-a-service zip files to cloud storage.</p><blockquote><p>The release stage takes the build produced by the build stage and combines it with the deploy’s current config</p></blockquote><p>The normal practice today is for a pipeline to push the release to the runtime environment. This is very useful early in the lifecycle of an app since the release process typically evolves with the app. To achieve stronger separation between the build and release, you might want to consider going GitOps. In Kubernetes-land, you would use a service such as <a href="https://fluxcd.io">Flux</a>.</p><h2>Factor VI: Processes</h2><blockquote><p>Twelve-factor processes are stateless and share-nothing. Any data that needs to persist must be stored in a stateful backing service.</p></blockquote><p>This factor remains relevant as written. In the world of REST APIs, this effectively means that we should hold no domain state in memory between HTTP requests - it should always be handed over to a caching service. This is the main enabler for scale-out in a software-as-a-service.</p><p>Adhering to this rule is also a good way to avoid memory leaks, which tend to plague garbage-collected ecosystems such as Java, Node, Python and Ruby. You will still get leaks after you out-source your caching to Redis, but it will be much easier to measure and the incitement to properly architecture the caching is stronger.</p><h2>Factor VII: Port binding</h2><blockquote><p>The twelve-factor app is completely self-contained and does not rely on runtime injection of a webserver into the execution environment to create a web-facing service. The web app exports HTTP as a service by binding to a port, and listening to requests coming in on that port.</p></blockquote><p>This factor is now standard in containerized scenarios. Widespread adoption of port binding has enabled a whole ecosystem of supporting proxies (e.g. <a href="https://www.envoyproxy.io/">Envoy</a>, <a href="https://traefik.io/">Traefik</a>, <a href="https://github.com/Shopify/toxiproxy">Toxiproxy</a>) which (ironically) means that a typical app today is often not self-contained, but depends on other containerized apps to perform e.g. authentication and tracing. This is a improves <a href="https://deviq.com/principles/separation-of-concerns">separation of concerns</a> and consequently, in 2022 we consider this factor at the service level.</p><blockquote><p>The port-binding approach means that one app can become the backing service for another app, by providing the URL to the backing app as a resource handle in the config for the consuming app.</p></blockquote><p>The original text focuses on network protocols such as HTTP and <a href="https://xmpp.org/extensions/">XMPP</a>. In order to become a backing service in 2022, the app should also adhere to an <a href="https://apievangelist.com/2019/07/15/what-is-an-api-contract/">API contract</a> of some sort, defining the backing service&#x27;s intended role.</p><p>Many developers implicitly assume that using high-level protocols like HTTP incurs latency. The overhead of a REST call over a local network (e.g. within a cloud provider) is typically 2-4 ms so you need to get a significant number of non-parallelizable requests before this overhead is noticeable over RDBMS operations and latency towards the consumer.</p><h2>Factor VIII: Concurrency</h2><blockquote><p>In the twelve-factor app, processes are a first class citizen. Processes in the twelve-factor app take strong cues from the unix process model for running service daemons. <!-- -->[...]<!-- --> This does not exclude individual processes from handling their own internal multiplexing. But an individual VM can only grow so large (vertical scale), so the <!-- -->[app]<!-- --> must also be able to span multiple processes running on multiple physical machines.</p></blockquote><p>This factor is now more or less written into law. Function-as-a-service platforms typically provide transparent horizontal scaling. In Kubernetes deployments you just give the number of pods you expect.</p><p>The mechanics of horizontal scalability is thus addressed in 2022. The central challenge for any successful app is to distribute work across multiple processes and backing services in such a way that it actually achieves meaningful horizontal scalability. The typical RDBMS-backed web app usually has its scalability completely constrained by its backing database, forcing vertical scaling of the RDBMS - a matter of allocating additional CPUs. This is often expensive and tends to yield only marginal improvement.</p><p>Is short, horizontal scalability is much preferable over vertical scalability but it is strictly a result of software architecture. It is therefore vital to identify early those apps that will actually require significant scale-out so that they can be properly architected.</p><p>Despite the dominance of the serverless paradigm in the software-as-a-service realm, there is still an overhang from the era of vertical scaling which the Twelve Factor App tries to break with. For example, the virtual machines of Java, Node, Python and Ruby maintain large volumes of reflection metadata and are very reluctant to de-allocate memory, leading to significant inefficiency on scale-out. A new generation of ecosystems, lead by Go and Rust, are more frugal in this respect.</p><h2>Factor IX: Disposability</h2><blockquote><p>The twelve-factor app’s processes are disposable, meaning they can be started or stopped at a moment’s notice. <!-- -->[...]<!-- --> Processes should strive to minimize startup time. <!-- -->[...]<!-- --> Processes shut down gracefully when they receive a SIGTERM signal, <!-- -->[...]<!-- --> allowing any current requests to finish. <!-- -->[...]<!-- --> A twelve-factor app is architected to handle unexpected, non-graceful terminations.</p></blockquote><p>This factor remains relevant as written and remains nearly as elusive today as it was ten years ago. For example, the HTTP server included in Node.js does <a href="https://blog.dashlane.com/implementing-nodejs-http-graceful-shutdown/">not by default perform graceful shutdown</a> (see also <a href="https://github.com/nodejs/node/issues/2642">nodejs issue 2642</a>).</p><p>Fulfilling this factor on an existing code base is much harder than it sounds. It means mapping all the (often implicit) state machines involved in the app and coordinating them so that there are no undefined transitions. For example, the database connection pool must be ready before we bring up our HTTP listener and must not shut down until the HTTP listener is down <em>and</em> all in-flight HTTP requests are done. Workers must &quot;hand back&quot; in-progress work items so that replacement workers do not have to wait for timeout to process those work items. This difficulty is compounded with distributed systems as a conceptual &quot;transaction&quot; may span more than one app or backing service (e.g. writing to file storage and sending mail), requiring <a href="https://en.wikipedia.org/wiki/Two-phase_commit_protocol">two-phase commit</a> semantics.</p><p>This factor is nevertheless the key to the always-on experience that we take for granted in large cloud services. A service with working graceful shutdown and comprehensive health checks can be updated at any time and builds developer and operations confidence. This can result in significant productivity gains, particularly when combined with automated testing.</p><h2>Factor X: Dev/prod parity</h2><blockquote><p>Historically, there have been substantial gaps between development <!-- -->[...]<!-- --> and production <!-- -->[...]<!-- -->, the time gap, the personnel gap <!-- -->[and]<!-- --> the tools gap. <!-- -->[...]<!-- --> The twelve-factor app is designed for continuous deployment by keeping the gap between development and production small.</p></blockquote><p>This factor is now colloquially known as <a href="https://aws.amazon.com/devops/what-is-devops/">DevOps</a> and remains as relevant as ever, but has still not established itself fully in software-as-a-service development: many production environments are hard to squeeze onto a developer&#x27;s laptop. Generally speaking, the public cloud providers put too little effort into supporting development use cases for their services. Kubernetes goes furthest in this respect: <a href="https://kind.sigs.k8s.io/">Kind</a> deserves mentioning for its heroic effort to achieve a dev-friendly, multi-node Kuberentes cluster using only Docker.</p><p>Docker has introduced a borderland where it is possible to develop a container using just Docker Engine for dev environment and still be reasonably confident that it will execute properly in e.g. Kubernetes. Still, some provisioning of backend services is still needed and time is wasted maintaining two different sets of instrumentation. For example, apps often have a Docker Compose file to get developers started, and a Kubernetes manifest for test/prod. When these desynch, nasty surprises can occur at deployment.</p><blockquote><p>The twelve-factor developer resists the urge to use different backing services between development and production</p></blockquote><p>Many full-stack development setups includes &quot;dev&quot; servers whose role is to automatically reload the app as its source code change. Similarly, tools like Docker Desktop and <a href="https://tilt.dev/">Tilt</a> provide capabilities and environments that are subtly different from e.g. Azure Container instances or Kubernetes. All these will color developers&#x27; choices and risk introducing issues that will not be discovered until late in the process.</p><p>The 2022 developer considers both developer experience, continuous integration/delivery/deployment and operability when choosing tools.</p><h2>Factor XI: Logs</h2><blockquote><p>Logs provide visibility into the behavior of a running app. <!-- -->[...]<!-- --> A twelve-factor app never concerns itself with routing or storage of its output stream. It should not attempt to write to or manage logfiles. Instead, each running process writes its event stream, unbuffered, to stdout. <!-- -->[...]<!-- --> Destinations are not visible to or configurable by the app, and instead are completely managed by the execution environment.</p></blockquote><p>Interestingly, this factor does not actually advise on the use of logging. Rather it treats them much as pre-contraceptive times viewed children: as something that inevitably accumulates as a result of marriage.</p><p>The factor should thus be updated to mandate that an app should be &quot;observable&quot;, meaning that it should volunteer information on its performance and behavior. We normally break this down into logging, metrics, tracing and audit trails. The relative merit of these differ greatly between apps, but all apps should have an observability strategy. Often, the need changes as an app evolve: early in the lifecycle, logging may dominate, but as usage grows, focus shifts to metrics. The apps in a service are considered as a unit and typically provide different observability needs.</p><h2>Factor XII: Admin processes</h2><blockquote><p>One-off admin processes should be run in an identical environment as the regular long-running processes of the app. They run against a release, using the same codebase and config as any process run against that release.</p></blockquote><p>This factor captures a practice that is common in the <a href="https://rubyonrails.org/">Rails</a> and <a href="https://www.drupal.org/">Drupal</a> ecosystems, among others. These are based on interpreted languages where it is relatively easy to give scripting or interactive access to the app&#x27;s internals: the main reason is to ensure that database access occurs using the same models that are used during runtime. In compiled languages, this requires a modularization (e.g. making the data model a separate library) that would complicate development.</p><p>However, the factor has two underlying principles which holds even today. First, that tools used to administer an app should be versioned and released with the same discipline as your app is. Second, that operating and upgrading an app is part of its ordinary usage and there is nothing strange with adding endpoints for internal administrative use. Put differently, at least factors I - IV should apply to the app&#x27;s tooling just as it does to the app itself.</p><h2>What else is there?</h2><p>Various additional factors have been proposed over the years, for example <a href="https://raw.githubusercontent.com/ffisk/books/master/beyond-the-twelve-factor-app.pdf">Beyond the Twelve-factor app</a> and <a href="https://www.ibm.com/cloud/blog/7-missing-factors-from-12-factor-applications">Seven missing factors from the Twelve-factor app</a>. The appeal of the original methodology springs from the universality of its factors. These contributions have relevance, but often only for a subsection of all apps that could (should) strive to live up to the original twelve factors. However, there is two aspects that are clearly missing: security and automated testing.</p><p>The good people at WhiteHat Security has written a good analysis called <a href="https://www.devopsdigest.com/security-and-12-factor-app-step-1">Security and the twelve-factor app</a> which analyses each factor from a security perspective and provides recommendations for securing your app. Their main point is that rather than being an additional factor, security needs to permeate all the twelve factors. The charge that security is underrepresented in the original methodology har merit, and the 2022 developer no longer has the luxury of treating security as an afterthought.</p><p>Finally, much of the benefit of adhering to the methodology is lost without extensive and automated testing. Version control hygiene and horizontal scalability matters little if apps break as soon as they are deployed. Ten years ago, there was still a discussion about whether writing programmatic tests was worth the effort. That discussion is now settled and in 2022, the discussion is about what the automated testing strategy should look like for a particular app or service. The discerning developer considers:</p><ul><li>when to apply unit, component, integration and end-to-end tests at app and/or service level</li><li>when to use in-memory implementations of backing services</li><li>what long-lived testing environments are needed</li><li>how much automated <a href="https://en.wikipedia.org/wiki/Static_program_analysis">static analysis</a> to include</li></ul><p>These additions notwithstanding, the Twelve-factor app methodology remains remarkably relevant today. All developers producing software-as-a-service offerings can benefit from adhering to its factors.</p>]]></content:encoded>
        </item>
    </channel>
</rss>