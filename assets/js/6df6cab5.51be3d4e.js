"use strict";(self.webpackChunkhome=self.webpackChunkhome||[]).push([[8694],{3905:function(e,n,t){t.d(n,{Zo:function(){return d},kt:function(){return m}});var a=t(7294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function s(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var i=a.createContext({}),u=function(e){var n=a.useContext(i),t=n;return e&&(t="function"==typeof e?e(n):s(s({},n),e)),t},d=function(e){var n=u(e.components);return a.createElement(i.Provider,{value:n},e.children)},p={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},c=a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,o=e.originalType,i=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),c=u(t),m=r,k=c["".concat(i,".").concat(m)]||c[m]||p[m]||o;return t?a.createElement(k,s(s({ref:n},d),{},{components:t})):a.createElement(k,s({ref:n},d))}));function m(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var o=t.length,s=new Array(o);s[0]=c;var l={};for(var i in n)hasOwnProperty.call(n,i)&&(l[i]=n[i]);l.originalType=e,l.mdxType="string"==typeof e?e:r,s[1]=l;for(var u=2;u<o;u++)s[u]=t[u];return a.createElement.apply(null,s)}return a.createElement.apply(null,t)}c.displayName="MDXCreateElement"},3919:function(e,n,t){function a(e){return!0===/^(\w*:|\/\/)/.test(e)}function r(e){return void 0!==e&&!a(e)}t.d(n,{b:function(){return a},Z:function(){return r}})},4996:function(e,n,t){t.d(n,{C:function(){return o},Z:function(){return s}});var a=t(2263),r=t(3919);function o(){var e=(0,a.Z)().siteConfig,n=(e=void 0===e?{}:e).baseUrl,t=void 0===n?"/":n,o=e.url;return{withBaseUrl:function(e,n){return function(e,n,t,a){var o=void 0===a?{}:a,s=o.forcePrependBaseUrl,l=void 0!==s&&s,i=o.absolute,u=void 0!==i&&i;if(!t)return t;if(t.startsWith("#"))return t;if((0,r.b)(t))return t;if(l)return n+t;var d=t.startsWith(n)?t:n+t.replace(/^\//,"");return u?e+d:d}(o,t,e,n)}}}function s(e,n){return void 0===n&&(n={}),(0,o().withBaseUrl)(e,n)}},2222:function(e,n,t){t.r(n),t.d(n,{frontMatter:function(){return l},contentTitle:function(){return i},metadata:function(){return u},toc:function(){return d},default:function(){return c}});var a=t(7462),r=t(3366),o=(t(7294),t(3905)),s=(t(4996),["components"]),l={id:"eks",title:"EKS"},i=void 0,u={unversionedId:"xks/operator-guide/kubernetes/eks",id:"xks/operator-guide/kubernetes/eks",isDocsHomePage:!1,title:"EKS",description:"Xenit Kubernetes Framework supports both AKS and EKS.",source:"@site/docs/xks/operator-guide/kubernetes/eks.md",sourceDirName:"xks/operator-guide/kubernetes",slug:"/xks/operator-guide/kubernetes/eks",permalink:"/docs/xks/operator-guide/kubernetes/eks",editUrl:"https://github.com/xenitab/xenitab.github.io/edit/main/docs/xks/operator-guide/kubernetes/eks.md",tags:[],version:"current",frontMatter:{id:"eks",title:"EKS"},sidebar:"docs",previous:{title:"AKS",permalink:"/docs/xks/operator-guide/kubernetes/aks"}},d=[{value:"Differences",id:"differences",children:[{value:"Repo structure",id:"repo-structure",children:[],level:3},{value:"EKS",id:"eks",children:[],level:3},{value:"IRSA",id:"irsa",children:[],level:3}],level:2},{value:"Bootstrap",id:"bootstrap",children:[],level:2},{value:"Tenants account peering",id:"tenants-account-peering",children:[],level:2},{value:"Update cluster version",id:"update-cluster-version",children:[{value:"Update the control plane using AWS CLI",id:"update-the-control-plane-using-aws-cli",children:[],level:3},{value:"Update the control plane using Terrafrom",id:"update-the-control-plane-using-terrafrom",children:[],level:3},{value:"Update the addons",id:"update-the-addons",children:[],level:3},{value:"Update the nodes",id:"update-the-nodes",children:[],level:3},{value:"Command examples",id:"command-examples",children:[],level:3}],level:2},{value:"Break glass",id:"break-glass",children:[],level:2},{value:"EKS resources",id:"eks-resources",children:[],level:2}],p={toc:d};function c(e){var n=e.components,t=(0,r.Z)(e,s);return(0,o.kt)("wrapper",(0,a.Z)({},p,t,{components:n,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"Xenit Kubernetes Framework supports both AKS and EKS.\nIn this document we will describe how to setup XKF on EKS and how it differs from AKS."),(0,o.kt)("h2",{id:"differences"},"Differences"),(0,o.kt)("p",null,"To setup XKF using EKS you still need an Azure environment."),(0,o.kt)("p",null,"XKF is heavily relying on Azure AD (AAD) and we have developed our own tool to\nmanage access to our clusters called ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/XenitAB/azad-kube-proxy"},"azad-kube-proxy"),"."),(0,o.kt)("p",null,"Our governance solution is still fully located in Azure together with our Terraform state."),(0,o.kt)("h3",{id:"repo-structure"},"Repo structure"),(0,o.kt)("p",null,"This is how an AWS repo structure can look like:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-txt"},"\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 aws-core\n\u2502\xa0\xa0 \u251c\u2500\u2500 main.tf\n\u2502\xa0\xa0 \u251c\u2500\u2500 outputs.tf\n\u2502\xa0\xa0 \u251c\u2500\u2500 variables\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 common.tfvars\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 dev.tfvars\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 prod.tfvars\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 qa.tfvars\n\u2502\xa0\xa0 \u2514\u2500\u2500 variables.tf\n\u251c\u2500\u2500 aws-eks\n\u2502\xa0\xa0 \u251c\u2500\u2500 main.tf\n\u2502\xa0\xa0 \u251c\u2500\u2500 outputs.tf\n\u2502\xa0\xa0 \u251c\u2500\u2500 variables\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 common.tfvars\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 dev.tfvars\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 prod.tfvars\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 qa.tfvars\n\u2502\xa0\xa0 \u2514\u2500\u2500 variables.tf\n\u251c\u2500\u2500 azure-governance\n\u2502\xa0\xa0 \u251c\u2500\u2500 main.tf\n\u2502\xa0\xa0 \u251c\u2500\u2500 outputs.tf\n\u2502\xa0\xa0 \u251c\u2500\u2500 variables\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 common.tfvars\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 dev.tfvars\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 prod.tfvars\n\u2502\xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 qa.tfvars\n\u2502\xa0\xa0 \u2514\u2500\u2500 variables.tf\n\u251c\u2500\u2500 global.tfvars\n")),(0,o.kt)("h3",{id:"eks"},"EKS"),(0,o.kt)("p",null,"Just like in AKS we use Calico as our CNI."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"AWS CNI does not support network policies"),(0,o.kt)("li",{parentName:"ul"},"AWS CNI heavily limits how many pods we can run on a single node"),(0,o.kt)("li",{parentName:"ul"},"We want to be consistent with AKS")),(0,o.kt)("p",null,"Just after setting up the EKS cluster we use a null_resource to first delete\nthe AWS CNI daemon set and then install calico.\nThis is all done before we add a single node to the cluster."),(0,o.kt)("p",null,"After this we add an EKS node group and Calico starts."),(0,o.kt)("h3",{id:"irsa"},"IRSA"),(0,o.kt)("p",null,"In AKS we use AAD Pod Identity to support access to Azure resources.\nWe support the same thing in EKS but use IAM roles for service accounts IRSA."),(0,o.kt)("p",null,"To make it easier to use IRSA we have developed a small terraform ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/XenitAB/terraform-modules/blob/main/modules/aws/irsa/README.md"},"module"),"."),(0,o.kt)("h2",{id:"bootstrap"},"Bootstrap"),(0,o.kt)("p",null,"By default AWS CNI limits the amount of pods that you can have on a single node.\nSince we are using Calico we do not have this limit,\nbut when setting up a default EKS environment the EKS ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/awslabs/amazon-eks-ami/blob/master/files/bootstrap.sh"},"bootstrap script"),"\ndefines a pod limit. To remove this limit we have created our own AWS launch template for our EKS node group. It sets ",(0,o.kt)("inlineCode",{parentName:"p"},"--use-max-pods false"),' and some needed Kubernetes node labels. If these labels are not set the EKS cluster is unable to "find" the nodes in the node group.'),(0,o.kt)("h2",{id:"tenants-account-peering"},"Tenants account peering"),(0,o.kt)("p",null,"In Azure we separates XKF and our tenants by using Resource Groups, in AWS we use separate accounts."),(0,o.kt)("p",null,"To setup a VPC peering you need to know the target VPC id, this creates a chicken and egg problem.\nTo workaround this problem we sadly have to run the eks/core module multiple times on both the XKF side and the tenant side."),(0,o.kt)("p",null,"Run Terraform in the following order:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"XKF core without any ",(0,o.kt)("inlineCode",{parentName:"li"},"vpc_peering_config_requester")," defined."),(0,o.kt)("li",{parentName:"ul"},"Tenant core without any ",(0,o.kt)("inlineCode",{parentName:"li"},"vpc_peering_config_accepter")," defined."),(0,o.kt)("li",{parentName:"ul"},"XKF core defines ",(0,o.kt)("inlineCode",{parentName:"li"},"vpc_peering_config_requester"),", manually getting the needed information from the tenant account."),(0,o.kt)("li",{parentName:"ul"},"Tenant core defines ",(0,o.kt)("inlineCode",{parentName:"li"},"vpc_peering_config_accepter"),", manually getting the needed information from the XKF account.")),(0,o.kt)("p",null,"Make sure that you only have one peering request open at the same time, else the accepter side will not be able to find a unique request.\nNow you should be able to see the VPC peering connected on both sides."),(0,o.kt)("h2",{id:"update-cluster-version"},"Update cluster version"),(0,o.kt)("p",null,"Updating the EKS cluster version can not be done by updating Terraform code only, it also involves the AWS CLI and kubectl. Find your EKS version to upgrade to here: ",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html"},"EKS versions")),(0,o.kt)("p",null,"For further information on the AWS CLI commands used in this section, please refer to the ",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/cli/latest/reference/eks/index.html"},"AWS EKS CLI")," documentation."),(0,o.kt)("h3",{id:"update-the-control-plane-using-aws-cli"},"Update the control plane using AWS CLI"),(0,o.kt)("p",null,"Get the name of the cluster to update:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"aws eks list-clusters --region eu-west-1\n")),(0,o.kt)("p",null,"Update the control plane version by running the following command:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"aws eks update-cluster-version --region eu-west-1 --name <cluster-name> --kubernetes-version <version>\n")),(0,o.kt)("p",null,"The above command provides an id that can be use to check the status of the update:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"aws eks describe-update --region eu-west-1 --name <cluster-name> --update-id <id>\n")),(0,o.kt)("p",null,"The update is finished when status is ",(0,o.kt)("inlineCode",{parentName:"p"},"Successful"),". Previous updates have taken approximately ",(0,o.kt)("strong",{parentName:"p"},"45 minutes"),"."),(0,o.kt)("p",null,"In the ",(0,o.kt)("inlineCode",{parentName:"p"},"aws-eks/variables/<environment>.tfvars")," Terraform file that corresponds to the actual environment, update the ",(0,o.kt)("inlineCode",{parentName:"p"},"kubernetes_version")," in ",(0,o.kt)("inlineCode",{parentName:"p"},"eks_config")," and make a ",(0,o.kt)("inlineCode",{parentName:"p"},"terraform plan"),". No difference in the plan output is expected. Also perform a ",(0,o.kt)("inlineCode",{parentName:"p"},"terraform apply")," just to make sure state the state is updated (might not be needed)."),(0,o.kt)("h3",{id:"update-the-control-plane-using-terrafrom"},"Update the control plane using Terrafrom"),(0,o.kt)("p",null,"TBD"),(0,o.kt)("h3",{id:"update-the-addons"},"Update the addons"),(0,o.kt)("p",null,"The addons to be updated are ",(0,o.kt)("inlineCode",{parentName:"p"},"coredns")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"kube-proxy"),". Can be verified with:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"aws eks list-addons --region eu-west-1 --cluster-name <cluster-name>\n")),(0,o.kt)("p",null,"The current addon version can be found by:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"aws eks describe-addon --region eu-west-1  --cluster-name <cluster-name>  --addon-name <addon-name>\n")),(0,o.kt)("p",null,"The version to update to can be found by:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"aws eks describe-addon-versions --region eu-west-1 --kubernetes-version <version> --addon-name <addon-name>\n")),(0,o.kt)("p",null,"The addons are updated with:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"aws eks update-addon --region eu-west-1 --cluster-name <cluster-name> --addon-name <name> --addon-version <version> --resolve-conflicts OVERWRITE\n")),(0,o.kt)("p",null,"Check that the status of new addon vestion is ",(0,o.kt)("inlineCode",{parentName:"p"},"ACTIVE"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"aws eks describe-addon --region eu-west-1  --cluster-name <cluster-name>  --addon-name  <name>\n")),(0,o.kt)("p",null,"The corresponding Pods can be checked by:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get pods -n kube-system\n")),(0,o.kt)("p",null,"Also perform a health check:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"https://ingress-healthz.<environment>.<customer>.se/\n")),(0,o.kt)("h3",{id:"update-the-nodes"},"Update the nodes"),(0,o.kt)("p",null,"In the ",(0,o.kt)("inlineCode",{parentName:"p"},"aws-eks/variables/<environment>.tfvars")," Terraform file that corresponds to the actual environment, add a new node group in ",(0,o.kt)("inlineCode",{parentName:"p"},"eks_config"),". The example below shows a node upgrade from ",(0,o.kt)("inlineCode",{parentName:"p"},"1.20")," to ",(0,o.kt)("inlineCode",{parentName:"p"},"1.21")," where ",(0,o.kt)("inlineCode",{parentName:"p"},"standard2")," is the new node group. The value of ",(0,o.kt)("inlineCode",{parentName:"p"},"release_version")," must match an AMI version (preferrably the latest) for the actual Kubernetes version (can be found ",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/eks/latest/userguide/eks-linux-ami-versions.html"},"here"),"):"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-terraform"},'eks_config = {\n  kubernetes_version = "1.21"\n  cidr_block         = "10.100.64.0/18"\n  node_groups = [\n    {\n      name            = "standard"\n      release_version = "1.20.4-20210621"\n      min_size        = 3\n      max_size        = 4\n      instance_types  = ["t3.large"]\n    },\n    {\n      name            = "standard2"\n      release_version = "1.21.5-20220123"\n      min_size        = 3\n      max_size        = 4\n      instance_types  = ["t3.large"]\n    },\n  ]\n}\n')),(0,o.kt)("p",null,"When this change is applied, there will be a new set of nodes running the new version added to the cluster. The following command will show all nodes and their versions:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-terraform"},"kubectl get nodes\n")),(0,o.kt)("p",null,"Now it is time to drain the old nodes one by one with:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl drain <node-name> --ignore-daemonsets --delete-local-data\n")),(0,o.kt)("p",null,"When all nodes are drained, remove the old node group in ",(0,o.kt)("inlineCode",{parentName:"p"},"eks_config"),". From the example above:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-terraform"},'eks_config = {\n  kubernetes_version = "1.21"\n  cidr_block         = "10.100.64.0/18"\n  node_groups = [\n    {\n      name            = "standard2"\n      release_version = "1.21.5-20220123"\n      min_size        = 3\n      max_size        = 4\n      instance_types  = ["t3.large"]\n    },\n  ]\n}\n')),(0,o.kt)("p",null,"When applied, the old nodes are removed. The update is now complete."),(0,o.kt)("h3",{id:"command-examples"},"Command examples"),(0,o.kt)("p",null,"The following AWS CLI commands are an example of an update from 1.20 to 1.21:"),(0,o.kt)("p",null,"Control plane:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"aws eks list-clusters --region eu-west-1\naws eks update-cluster-version --region eu-west-1  --name qa-eks2  --kubernetes-version 1.21\naws eks describe-update --region eu-west-1 --name qa-eks2 --update-id 25b9f04f-0be3-40ca-bc37-aaf841070012\n")),(0,o.kt)("p",null,"Addons:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"aws eks describe-addon-versions --kubernetes-version 1.21 --addon-name coredns\naws eks update-addon --cluster-name qa-eks2 --addon-name coredns --addon-version v1.8.4-eksbuild.1 --resolve-conflicts OVERWRITE\naws eks describe-addon  --region eu-west-1  --cluster-name qa-eks2  --addon-name coredns\naws eks describe-addon-versions --kubernetes-version 1.21 --addon-name kube-proxy\naws eks update-addon --cluster-name qa-eks2 --addon-name kube-proxy --addon-version v1.21.2-eksbuild.2 --resolve-conflicts OVERWRITE\naws eks describe-addon --region eu-west-1  --cluster-name qa-eks2  --addon-name kube-proxy\n")),(0,o.kt)("h2",{id:"break-glass"},"Break glass"),(0,o.kt)("p",null,"We are very dependent on azad-proxy to work but if something happens with the\ningress, azad-proxy or the AAD we need to have ways of reaching the cluster:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"aws eks --region eu-west-1 update-kubeconfig --name dev-eks1 --alias dev-eks1 --role-arn arn:aws:iam::111111111111:role/xkf-eu-west-1-dev-eks-admin\n")),(0,o.kt)("h2",{id:"eks-resources"},"EKS resources"),(0,o.kt)("p",null,"To get a quick overview of what is happening in EKS you can look at its ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/awslabs/amazon-eks-ami/blob/master/CHANGELOG.md#changelog"},"changelog"),"."),(0,o.kt)("p",null,"When upgrading node groups you need to correlate with your Kubernetes release, you can find which node group is available to ",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/eks/latest/userguide/eks-linux-ami-versions.html"},"which node group"),"."),(0,o.kt)("p",null,"AWS general ",(0,o.kt)("a",{parentName:"p",href:"https://aws.amazon.com/security/security-bulletins/"},"security information")),(0,o.kt)("p",null,"Public ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/aws/containers-roadmap"},"containers roadmap"),"."))}c.isMDXComponent=!0}}]);