"use strict";(self.webpackChunkhome=self.webpackChunkhome||[]).push([[1451],{3905:function(e,t,n){n.d(t,{Zo:function(){return u},kt:function(){return h}});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),c=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},u=function(e){var t=c(e.components);return a.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},p=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),p=c(n),h=r,m=p["".concat(s,".").concat(h)]||p[h]||d[h]||o;return n?a.createElement(m,i(i({ref:t},u),{},{components:n})):a.createElement(m,i({ref:t},u))}));function h(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=p;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:r,i[1]=l;for(var c=2;c<o;c++)i[c]=n[c];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}p.displayName="MDXCreateElement"},4737:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return l},contentTitle:function(){return s},metadata:function(){return c},toc:function(){return u},default:function(){return p}});var a=n(7462),r=n(3366),o=(n(7294),n(3905)),i=["components"],l={id:"networking",title:"Networking"},s=void 0,c={unversionedId:"xks/operator-guide/networking",id:"xks/operator-guide/networking",isDocsHomePage:!1,title:"Networking",description:"There are a lot of moving parts when looking at networking and Kubernetes. There are both Kubernetes specific components such as kube-proxy and CoreDNS but also cloud specific components such as the",source:"@site/docs/xks/operator-guide/networking.md",sourceDirName:"xks/operator-guide",slug:"/xks/operator-guide/networking",permalink:"/docs/xks/operator-guide/networking",editUrl:"https://github.com/xenitab/xenitab.github.io/edit/main/docs/xks/operator-guide/networking.md",tags:[],version:"current",frontMatter:{id:"networking",title:"Networking"},sidebar:"docs",previous:{title:"Agents",permalink:"/docs/xks/operator-guide/agents"},next:{title:"Blast Radius",permalink:"/docs/xks/operator-guide/blast-radius"}},u=[{value:"Kubernetes",id:"kubernetes",children:[{value:"Node Local DNS",id:"node-local-dns",children:[{value:"Node Local DNS Configuration",id:"node-local-dns-configuration",children:[],level:4},{value:"Node local DNS networkpolicy",id:"node-local-dns-networkpolicy",children:[],level:4}],level:3}],level:2},{value:"Azure",id:"azure",children:[{value:"SNAT Exhaustion",id:"snat-exhaustion",children:[{value:"Links",id:"links",children:[],level:4}],level:3}],level:2},{value:"AWS",id:"aws",children:[],level:2}],d={toc:u};function p(e){var t=e.components,n=(0,r.Z)(e,i);return(0,o.kt)("wrapper",(0,a.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"There are a lot of moving parts when looking at networking and Kubernetes. There are both Kubernetes specific components such as kube-proxy and CoreDNS but also cloud specific components such as the\nunderlying VNET/VPC, load balancer, and NAT gateways. The page aims to describe the architecture and some of the limitations and problems which can be experienced when working with XKS to help with\ndebugging networking issues."),(0,o.kt)("h2",{id:"kubernetes"},"Kubernetes"),(0,o.kt)("p",null,"TBD"),(0,o.kt)("h3",{id:"node-local-dns"},"Node Local DNS"),(0,o.kt)("p",null,"To lower DNS query latency and a number of other ",(0,o.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/#motivation"},"reasons"),"\nwe are using ",(0,o.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/"},"NodeLocal DNS")," in XKS."),(0,o.kt)("p",null,"Node Local DNS is an application that runs on each node and creates a loopback interface on each node together with a number of iptables rules. The iptables rules intercepts all the DNS traffic from all pods that is sent to the clusters DNS server."),(0,o.kt)("h4",{id:"node-local-dns-configuration"},"Node Local DNS Configuration"),(0,o.kt)("p",null,"To configure Node Local DNS you need to provide two values.\nThe IP of the central DNS server in your cluster, you can find this by running: ",(0,o.kt)("inlineCode",{parentName:"p"},"kubectl get svc kube-dns -n kube-system -o jsonpath={.spec.clusterIP}"),".\nThe second value is a random IP that you know that nothing else in the cluster is ever going to use, in our case we used the example ip ",(0,o.kt)("inlineCode",{parentName:"p"},"169.254.20.10"),".\nThese values are defined for you in XKS but it's good to know about them and where to find them."),(0,o.kt)("p",null,"Here you can view the example ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml"},"configuration")," provided in the docs."),(0,o.kt)("p",null,"Node Local DNS is built on top of CoreDNS and is plugin based. Depending on your needs you can easily enable new features.\nBy default NodeLocal DNS don't log the DNS requests it gets but it can make it hard to debug."),(0,o.kt)("p",null,"In XKS we haven't enabled any debug logs ether but if you need to enable it all you need to do is to add ",(0,o.kt)("inlineCode",{parentName:"p"},"log")," as part of the plugins defined in your yaml."),(0,o.kt)("p",null,"For example:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-.yaml"},"data:\n  Corefile: |\n    .:53 {\n        errors\n        log\n        cache 30\n        reload\n        loop\n        bind 169.254.20.10 10.0.0.10\n        forward . __PILLAR__UPSTREAM__SERVERS__\n        prometheus :9253\n        }\n")),(0,o.kt)("p",null,"For you as a XKS administrator the biggest chance to change is in the ",(0,o.kt)("a",{parentName:"p",href:"https://coredns.io/plugins/cache/"},"cache plugin"),".\nInstead of me trying to rewrite the docs I recommend you to read it but we have changed the default value and at the time of writing we use the following config:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-.yaml"},"data:\n  Corefile: |\n    .:53 {\n        log\n        errors\n        cache {\n                success 9984 30\n                denial 9984 10\n                prefetch 20 60s 15%\n        }\n        reload\n        loop\n        bind 169.254.20.10 10.0.0.10\n        forward . /etc/resolv.conf\n        prometheus :9253\n        }\n")),(0,o.kt)("p",null,"The prefetch feature allows us to automatically get DNS entries that is in the cache and automatically update it before the DNS TTL ends.\nRemember that the cache TTL won't change the TTL of your cached DNS entries.\nIf the DNS entry have a TTL of 1 minute and the cache have a TTL of 5 minutes the DNS entry will disappear after 1 minute."),(0,o.kt)("p",null,"If you for example define a cache without setting success and denial but set the prefetch config the default TTL cache value will still be applied."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-.yaml"},"data:\n  Corefile: |\n    .:53 {\n        log\n        errors\n        cache {\n            prefetch 20 60s 15%\n        }\n        reload\n        loop\n        bind 169.254.20.10 10.0.0.10\n        forward . /etc/resolv.conf\n        prometheus :9253\n        }\n")),(0,o.kt)("h4",{id:"node-local-dns-networkpolicy"},"Node local DNS networkpolicy"),(0,o.kt)("p",null,"Sadly when using NodeLocal DNS together with Networkpolicy and the Calico CNI you need to write a networkpolicy that instead of using label selectors on a pod level you need to write a ruler\nthat will work on the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/dns/nodelocaldns/README.md#network-policy-and-dns-connectivity"},"node level"),"\nWhat it doesn't say in the docs is that you need to define the internal vnet IP as well."),(0,o.kt)("p",null,"These are the same values that was defined when doing the configuration.\nThe default values on XKS ",(0,o.kt)("inlineCode",{parentName:"p"},"AKS")," is ",(0,o.kt)("inlineCode",{parentName:"p"},"169.254.20.10")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"10.0.0.10")," and on ",(0,o.kt)("inlineCode",{parentName:"p"},"AWS")," it's ",(0,o.kt)("inlineCode",{parentName:"p"},"169.254.20.10")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"172.20.0.10"),"."),(0,o.kt)("p",null,"The needed networkpolicy exist by default in all the tenant namespaces and is called ",(0,o.kt)("inlineCode",{parentName:"p"},"default-deny")," and is managed by terraform."),(0,o.kt)("p",null,"To view the rule run:"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"kubectl get networkpolicies default-deny -n <tenant>")),(0,o.kt)("h2",{id:"azure"},"Azure"),(0,o.kt)("p",null,"XKS in Azure uses a single VNET with a single subnet per AKS cluster. The VNET and subnets are created by the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/XenitAB/terraform-modules/tree/main/modules/azure/core"},"core module"),".\nAdditionally each AKS cluster also creates a load balancer. The load balancer is used for both ingress and egress traffic."),(0,o.kt)("p",null,"When a Kubernetes service of type ",(0,o.kt)("inlineCode",{parentName:"p"},"LoadBalancer")," is created a new IP is attached to the load balancer. An Azure load balancer can have multiple IPs attached to it so unlike AWS it does not have to\ncreate a new load balancer."),(0,o.kt)("p",null,"During the creation of the AKS cluster a public IP prefix is attached to the load balancer for egress traffic. This ensures that all traffic egress with the same source IP, enabling the use of IP\nwhite listing in external sources. This does however mean that all outbound traffic will also go through the same load balancer as the incoming traffic. There is currently work underway to enable the\nuse of managed NAT gateways for egress traffic in AKS, but it is currently ",(0,o.kt)("a",{parentName:"p",href:"https://docs.microsoft.com/en-us/azure/aks/nat-gateway"},"in preview")," right now."),(0,o.kt)("h3",{id:"snat-exhaustion"},"SNAT Exhaustion"),(0,o.kt)("p",null,"Applications making large numbers of outgoing TCP or UDP connections to the same IP and port can cause an issue known as ",(0,o.kt)("em",{parentName:"p"},"SNAT port exhaustion"),". This is mostly due to the network architecture in Azure\nand AKS. All of the outgoing traffic from AKS goes through the load balancer, and for each outgoing request the load balancer needs to allocate an SNAT port to receive the response. Each Azure load\nbalancer will allocate 64000 SNAT ports. This may seem like a lot, but there is a caveat as AKS will limit the amount of SNAT ports per node. The amount of SNAT ports available per node depends on\nthe amount of nodes per cluster."),(0,o.kt)("table",null,(0,o.kt)("thead",{parentName:"table"},(0,o.kt)("tr",{parentName:"thead"},(0,o.kt)("th",{parentName:"tr",align:"center"},"Node Count"),(0,o.kt)("th",{parentName:"tr",align:"center"},"SNAT Ports per Node"))),(0,o.kt)("tbody",{parentName:"table"},(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"center"},"1-50"),(0,o.kt)("td",{parentName:"tr",align:"center"},"1024")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"center"},"51-100"),(0,o.kt)("td",{parentName:"tr",align:"center"},"512")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"center"},"101-200"),(0,o.kt)("td",{parentName:"tr",align:"center"},"256")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"center"},"201-400"),(0,o.kt)("td",{parentName:"tr",align:"center"},"128")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"center"},"401-800"),(0,o.kt)("td",{parentName:"tr",align:"center"},"64")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"center"},"801-1000"),(0,o.kt)("td",{parentName:"tr",align:"center"},"32")))),(0,o.kt)("p",null,"A symptom of exhausting the SNAT ports is that outgoing requests will just fail. This is of course not a good situation, and may be hard to debug as a failing request could be caused by many different\nfactors."),(0,o.kt)("h4",{id:"links"},"Links"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://docs.microsoft.com/en-us/azure/aks/load-balancer-standard#troubleshooting-snat"},"https://docs.microsoft.com/en-us/azure/aks/load-balancer-standard#troubleshooting-snat")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://docs.microsoft.com/en-us/azure/load-balancer/troubleshoot-outbound-connection"},"https://docs.microsoft.com/en-us/azure/load-balancer/troubleshoot-outbound-connection")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://www.danielstechblog.io/detecting-snat-port-exhaustion-on-azure-kubernetes-service/"},"https://www.danielstechblog.io/detecting-snat-port-exhaustion-on-azure-kubernetes-service/")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://medium.com/asos-techblog/an-aks-performance-journey-part-1-sizing-everything-up-ee6d2346ea99"},"https://medium.com/asos-techblog/an-aks-performance-journey-part-1-sizing-everything-up-ee6d2346ea99")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://medium.com/asos-techblog/an-aks-performance-journey-part-2-networking-it-out-e253f5bb4f69"},"https://medium.com/asos-techblog/an-aks-performance-journey-part-2-networking-it-out-e253f5bb4f69"))),(0,o.kt)("h2",{id:"aws"},"AWS"),(0,o.kt)("p",null,"TBD"))}p.isMDXComponent=!0}}]);